2022-12-20 04:15:55 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11838
2022-12-20 04:15:55 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11838
2022-12-20 04:15:55 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11838
2022-12-20 04:15:55 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11838
2022-12-20 04:15:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-20 04:15:56 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 0
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-20 04:15:56 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 1
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-20 04:15:56 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 3
2022-12-20 04:15:56 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-20 04:15:56 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 2
2022-12-20 04:16:04 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 3, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11838', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1024, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints//graph_text', 'restore_file': '../checkpoints/model_100k.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'rouge_avg', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_large', attention_dropout=0.0, attention_name='block_noglobal', azureml_logging=False, batch_size=2, batch_size_valid=2, best_checkpoint_metric='rouge_avg', bf16=False, block_attention=False, block_size=1024, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=True, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=True, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, custom_dict='../checkpoints/dict.txt', data='/home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, dual_graph_encoder=False, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_rouge=True, eval_rouge_args='{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', eval_rouge_detok='space', eval_rouge_detok_args='{}', eval_rouge_print_samples=False, eval_rouge_remove_bpe=None, fast_stat_sync=True, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, input_pattern='concat', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.0, layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0001], lr_scheduler='polynomial_decay', max_epoch=50, max_query_positions=50, max_source_positions=16384, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=4, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, pad_query=0, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, pooling_layers=4, power=1.0, profile=False, quantization_config_path=None, query_based=False, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1024, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='../checkpoints/model_100k.pt', restrict_position_embed=False, save_dir='checkpoints//graph_text', save_interval=1, save_interval_updates=0, scoring='bleu', seed=3, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, sliding_window=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='src', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='tgt', task='summarization', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, top_down=False, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, truncate_target=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_xformers=True, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'summarization', 'data': '/home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin', 'source_lang': 'src', 'target_lang': 'tgt', 'load_alignments': False, 'left_pad_source': False, 'left_pad_target': False, 'max_source_positions': 16384, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'truncate_target': True, 'query_lang': 'query', 'query_based': False, 'max_query_positions': 50, 'pad_query': 0, 'input_pattern': 'concat', 'block_size': 1024, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1024, 'eval_rouge': True, 'eval_rouge_args': '{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', 'eval_rouge_detok': 'space', 'eval_rouge_detok_args': '{}', 'eval_rouge_remove_bpe': None, 'eval_rouge_print_samples': False, 'custom_dict': '../checkpoints/dict.txt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.0, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-12-20 04:16:04 | INFO | fairseq.tasks.summarization | [src] dictionary: 50606 types
2022-12-20 04:16:04 | INFO | fairseq.tasks.summarization | [tgt] dictionary: 50606 types
2022-12-20 04:16:30 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(16386, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (9): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (10): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (11): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50606, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-12-20 04:16:30 | INFO | fairseq_cli.train | task: SummarizationTask
2022-12-20 04:16:30 | INFO | fairseq_cli.train | model: BARTModel
2022-12-20 04:16:30 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-12-20 04:16:30 | INFO | fairseq_cli.train | num. shared model params: 439,162,880 (num. trained: 439,162,880)
2022-12-20 04:16:30 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-12-20 04:16:30 | INFO | fairseq.data.data_utils | loaded 1,376 examples from: /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin/valid.src-tgt.src
2022-12-20 04:16:30 | INFO | fairseq.data.data_utils | loaded 1,376 examples from: /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin/valid.src-tgt.tgt
2022-12-20 04:16:30 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin valid src-tgt 1376 examples
2022-12-20 04:16:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-12-20 04:16:31 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2022-12-20 04:16:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-20 04:16:31 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-20 04:16:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-20 04:16:33 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-20 04:16:33 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-20 04:16:33 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-20 04:16:33 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-20 04:16:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-20 04:16:33 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2022-12-20 04:16:33 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2022-12-20 04:16:33 | INFO | fairseq.trainer | Preparing to load checkpoint ../checkpoints/model_100k.pt
2022-12-20 04:17:00 | INFO | fairseq.optim.adam | using FusedAdam
2022-12-20 04:17:00 | INFO | fairseq.trainer | Loaded checkpoint ../checkpoints/model_100k.pt (epoch 1 @ 0 updates)
2022-12-20 04:17:00 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-20 04:17:04 | INFO | fairseq.data.data_utils | loaded 24,773 examples from: /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin/train.src-tgt.src
2022-12-20 04:17:04 | INFO | fairseq.data.data_utils | loaded 24,773 examples from: /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin/train.src-tgt.tgt
2022-12-20 04:17:04 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/PLOS_fs-graph_text-bin train src-tgt 24773 examples
2022-12-20 04:17:04 | INFO | fairseq.trainer | begin training epoch 1
2022-12-20 04:17:04 | INFO | fairseq_cli.train | Start iterating over samples
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-20 04:17:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-20 04:17:56 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.
2022-12-20 04:18:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-20 04:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-20 04:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-20 04:18:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-20 04:18:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-20 04:18:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-12-20 04:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-12-20 04:19:59 | INFO | train_inner | {"epoch": 1, "update": 0.023, "loss": "3.734", "nll_loss": "3.734", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "13.31", "wps": "852.3", "ups": "0.11", "wpb": "7523.3", "bsz": "32", "num_updates": "10", "lr": "2e-06", "gnorm": "22.353", "clip": "100", "loss_scale": "0.5", "train_wall": "128", "gb_free": "62.9", "wall": "206"}
2022-12-20 04:21:09 | INFO | train_inner | {"epoch": 1, "update": 0.036, "loss": "3.633", "nll_loss": "3.633", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "12.4", "wps": "1079.8", "ups": "0.14", "wpb": "7556.6", "bsz": "32", "num_updates": "20", "lr": "4e-06", "gnorm": "18.393", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "276"}
2022-12-20 04:22:18 | INFO | train_inner | {"epoch": 1, "update": 0.049, "loss": "3.357", "nll_loss": "3.357", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.24", "wps": "1109.2", "ups": "0.14", "wpb": "7680.6", "bsz": "32", "num_updates": "30", "lr": "6e-06", "gnorm": "8.262", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.9", "wall": "345"}
2022-12-20 04:23:27 | INFO | train_inner | {"epoch": 1, "update": 0.062, "loss": "3.229", "nll_loss": "3.229", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.38", "wps": "1105.8", "ups": "0.15", "wpb": "7610.3", "bsz": "32", "num_updates": "40", "lr": "8e-06", "gnorm": "4.185", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "65", "wall": "414"}
2022-12-20 04:24:37 | INFO | train_inner | {"epoch": 1, "update": 0.075, "loss": "3.058", "nll_loss": "3.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.33", "wps": "1094.6", "ups": "0.14", "wpb": "7724.1", "bsz": "32", "num_updates": "50", "lr": "1e-05", "gnorm": "3.503", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "484"}
2022-12-20 04:25:48 | INFO | train_inner | {"epoch": 1, "update": 0.088, "loss": "3.01", "nll_loss": "3.01", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.05", "wps": "1047.5", "ups": "0.14", "wpb": "7393.1", "bsz": "32", "num_updates": "60", "lr": "1.2e-05", "gnorm": "2.907", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "555"}
2022-12-20 04:26:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-12-20 04:27:07 | INFO | train_inner | {"epoch": 1, "update": 0.102, "loss": "2.977", "nll_loss": "2.977", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.87", "wps": "985.3", "ups": "0.13", "wpb": "7789", "bsz": "32", "num_updates": "70", "lr": "1.4e-05", "gnorm": "2.944", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.9", "wall": "634"}
2022-12-20 04:28:19 | INFO | train_inner | {"epoch": 1, "update": 0.115, "loss": "2.958", "nll_loss": "2.958", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.77", "wps": "1048", "ups": "0.14", "wpb": "7551.4", "bsz": "32", "num_updates": "80", "lr": "1.6e-05", "gnorm": "2.285", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "65", "wall": "706"}
2022-12-20 04:29:30 | INFO | train_inner | {"epoch": 1, "update": 0.128, "loss": "2.931", "nll_loss": "2.931", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.62", "wps": "1041.8", "ups": "0.14", "wpb": "7415", "bsz": "32", "num_updates": "90", "lr": "1.8e-05", "gnorm": "2.213", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "65", "wall": "777"}
2022-12-20 04:30:41 | INFO | train_inner | {"epoch": 1, "update": 0.141, "loss": "2.927", "nll_loss": "2.927", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.6", "wps": "1083.6", "ups": "0.14", "wpb": "7623", "bsz": "32", "num_updates": "100", "lr": "2e-05", "gnorm": "2.845", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "848"}
2022-12-20 04:31:51 | INFO | train_inner | {"epoch": 1, "update": 0.154, "loss": "2.943", "nll_loss": "2.943", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.69", "wps": "1077.6", "ups": "0.14", "wpb": "7616.3", "bsz": "32", "num_updates": "110", "lr": "2.2e-05", "gnorm": "2.322", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "918"}
2022-12-20 04:33:03 | INFO | train_inner | {"epoch": 1, "update": 0.166, "loss": "2.916", "nll_loss": "2.916", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.55", "wps": "1054.6", "ups": "0.14", "wpb": "7544", "bsz": "32", "num_updates": "120", "lr": "2.4e-05", "gnorm": "2.27", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "990"}
2022-12-20 04:34:13 | INFO | train_inner | {"epoch": 1, "update": 0.179, "loss": "2.896", "nll_loss": "2.896", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.44", "wps": "1085.3", "ups": "0.14", "wpb": "7654.6", "bsz": "32", "num_updates": "130", "lr": "2.6e-05", "gnorm": "2.097", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "1060"}
2022-12-20 04:35:24 | INFO | train_inner | {"epoch": 1, "update": 0.192, "loss": "2.873", "nll_loss": "2.873", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.32", "wps": "1067", "ups": "0.14", "wpb": "7536.7", "bsz": "32", "num_updates": "140", "lr": "2.8e-05", "gnorm": "2.257", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "1131"}
2022-12-20 04:36:34 | INFO | train_inner | {"epoch": 1, "update": 0.205, "loss": "2.858", "nll_loss": "2.858", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.25", "wps": "1087", "ups": "0.14", "wpb": "7592.3", "bsz": "32", "num_updates": "150", "lr": "3e-05", "gnorm": "2.555", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "1201"}
2022-12-20 04:37:44 | INFO | train_inner | {"epoch": 1, "update": 0.218, "loss": "2.952", "nll_loss": "2.952", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.74", "wps": "1096.7", "ups": "0.14", "wpb": "7648", "bsz": "32", "num_updates": "160", "lr": "3.2e-05", "gnorm": "2.164", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "1271"}
2022-12-20 04:38:53 | INFO | train_inner | {"epoch": 1, "update": 0.231, "loss": "2.862", "nll_loss": "2.862", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.27", "wps": "1130.7", "ups": "0.14", "wpb": "7842.5", "bsz": "32", "num_updates": "170", "lr": "3.4e-05", "gnorm": "1.929", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63.6", "wall": "1340"}
2022-12-20 04:40:03 | INFO | train_inner | {"epoch": 1, "update": 0.244, "loss": "2.793", "nll_loss": "2.793", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.93", "wps": "1091.9", "ups": "0.14", "wpb": "7679.3", "bsz": "32", "num_updates": "180", "lr": "3.6e-05", "gnorm": "1.91", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.9", "wall": "1410"}
2022-12-20 04:41:15 | INFO | train_inner | {"epoch": 1, "update": 0.257, "loss": "2.881", "nll_loss": "2.881", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.37", "wps": "1052", "ups": "0.14", "wpb": "7572.8", "bsz": "32", "num_updates": "190", "lr": "3.8e-05", "gnorm": "2.014", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "62.2", "wall": "1482"}
2022-12-20 04:42:27 | INFO | train_inner | {"epoch": 1, "update": 0.27, "loss": "2.786", "nll_loss": "2.786", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.9", "wps": "1054.2", "ups": "0.14", "wpb": "7598.7", "bsz": "32", "num_updates": "200", "lr": "4e-05", "gnorm": "2.004", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "64.4", "wall": "1554"}
2022-12-20 04:43:37 | INFO | train_inner | {"epoch": 1, "update": 0.283, "loss": "2.924", "nll_loss": "2.924", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.59", "wps": "1073.3", "ups": "0.14", "wpb": "7510.2", "bsz": "32", "num_updates": "210", "lr": "4.2e-05", "gnorm": "2.814", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "65", "wall": "1624"}
2022-12-20 04:44:48 | INFO | train_inner | {"epoch": 1, "update": 0.295, "loss": "2.906", "nll_loss": "2.906", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.49", "wps": "1062.5", "ups": "0.14", "wpb": "7563.3", "bsz": "32", "num_updates": "220", "lr": "4.4e-05", "gnorm": "2.243", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.9", "wall": "1696"}
2022-12-20 04:45:59 | INFO | train_inner | {"epoch": 1, "update": 0.308, "loss": "2.809", "nll_loss": "2.809", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.01", "wps": "1074.9", "ups": "0.14", "wpb": "7585", "bsz": "32", "num_updates": "230", "lr": "4.6e-05", "gnorm": "2.127", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63", "wall": "1766"}
2022-12-20 04:47:11 | INFO | train_inner | {"epoch": 1, "update": 0.321, "loss": "2.856", "nll_loss": "2.856", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.24", "wps": "1077.1", "ups": "0.14", "wpb": "7726.2", "bsz": "31.9", "num_updates": "240", "lr": "4.8e-05", "gnorm": "2.475", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "1838"}
2022-12-20 04:48:20 | INFO | train_inner | {"epoch": 1, "update": 0.334, "loss": "2.874", "nll_loss": "2.874", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.33", "wps": "1106.7", "ups": "0.15", "wpb": "7618.4", "bsz": "32", "num_updates": "250", "lr": "5e-05", "gnorm": "2.401", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "1907"}
2022-12-20 04:49:29 | INFO | train_inner | {"epoch": 1, "update": 0.347, "loss": "2.83", "nll_loss": "2.83", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.11", "wps": "1101.1", "ups": "0.14", "wpb": "7596.6", "bsz": "32", "num_updates": "260", "lr": "5.2e-05", "gnorm": "1.872", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "1976"}
2022-12-20 04:50:40 | INFO | train_inner | {"epoch": 1, "update": 0.36, "loss": "2.837", "nll_loss": "2.837", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.15", "wps": "1067", "ups": "0.14", "wpb": "7563.3", "bsz": "32", "num_updates": "270", "lr": "5.4e-05", "gnorm": "2.74", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "2047"}
2022-12-20 04:51:52 | INFO | train_inner | {"epoch": 1, "update": 0.373, "loss": "2.894", "nll_loss": "2.894", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.43", "wps": "1066", "ups": "0.14", "wpb": "7690.5", "bsz": "32", "num_updates": "280", "lr": "5.6e-05", "gnorm": "1.93", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "63", "wall": "2119"}
2022-12-20 04:53:04 | INFO | train_inner | {"epoch": 1, "update": 0.386, "loss": "2.859", "nll_loss": "2.859", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.26", "wps": "1048.6", "ups": "0.14", "wpb": "7575.5", "bsz": "32", "num_updates": "290", "lr": "5.8e-05", "gnorm": "1.918", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "66.3", "wall": "2191"}
2022-12-20 04:54:14 | INFO | train_inner | {"epoch": 1, "update": 0.399, "loss": "2.837", "nll_loss": "2.837", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.15", "wps": "1102.7", "ups": "0.14", "wpb": "7729.2", "bsz": "32", "num_updates": "300", "lr": "6e-05", "gnorm": "1.721", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "64.3", "wall": "2261"}
2022-12-20 04:55:23 | INFO | train_inner | {"epoch": 1, "update": 0.412, "loss": "2.817", "nll_loss": "2.817", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.05", "wps": "1085.7", "ups": "0.14", "wpb": "7497.3", "bsz": "32", "num_updates": "310", "lr": "6.2e-05", "gnorm": "1.791", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "2330"}
2022-12-20 04:56:32 | INFO | train_inner | {"epoch": 1, "update": 0.425, "loss": "2.862", "nll_loss": "2.862", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.27", "wps": "1094.1", "ups": "0.14", "wpb": "7589.9", "bsz": "32", "num_updates": "320", "lr": "6.4e-05", "gnorm": "1.829", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "2400"}
2022-12-20 04:57:41 | INFO | train_inner | {"epoch": 1, "update": 0.437, "loss": "2.797", "nll_loss": "2.797", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.95", "wps": "1121.8", "ups": "0.15", "wpb": "7651.1", "bsz": "32", "num_updates": "330", "lr": "6.6e-05", "gnorm": "1.795", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "63.6", "wall": "2468"}
2022-12-20 04:58:49 | INFO | train_inner | {"epoch": 1, "update": 0.45, "loss": "2.853", "nll_loss": "2.853", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.23", "wps": "1119.3", "ups": "0.15", "wpb": "7655", "bsz": "32", "num_updates": "340", "lr": "6.8e-05", "gnorm": "1.739", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "2536"}
2022-12-20 04:59:56 | INFO | train_inner | {"epoch": 1, "update": 0.463, "loss": "2.837", "nll_loss": "2.837", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.14", "wps": "1147", "ups": "0.15", "wpb": "7726.4", "bsz": "32", "num_updates": "350", "lr": "7e-05", "gnorm": "1.685", "clip": "100", "loss_scale": "0.25", "train_wall": "67", "gb_free": "64.4", "wall": "2603"}
2022-12-20 05:01:08 | INFO | train_inner | {"epoch": 1, "update": 0.476, "loss": "2.929", "nll_loss": "2.929", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.62", "wps": "1054.9", "ups": "0.14", "wpb": "7516", "bsz": "32", "num_updates": "360", "lr": "7.2e-05", "gnorm": "1.747", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "2675"}
2022-12-20 05:02:17 | INFO | train_inner | {"epoch": 1, "update": 0.489, "loss": "2.891", "nll_loss": "2.891", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.42", "wps": "1097.5", "ups": "0.14", "wpb": "7636.9", "bsz": "32", "num_updates": "370", "lr": "7.4e-05", "gnorm": "1.838", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63.6", "wall": "2744"}
2022-12-20 05:03:25 | INFO | train_inner | {"epoch": 1, "update": 0.502, "loss": "2.802", "nll_loss": "2.802", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.97", "wps": "1124.3", "ups": "0.15", "wpb": "7647.7", "bsz": "32", "num_updates": "380", "lr": "7.6e-05", "gnorm": "1.861", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "2812"}
2022-12-20 05:04:36 | INFO | train_inner | {"epoch": 1, "update": 0.515, "loss": "2.858", "nll_loss": "2.858", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.25", "wps": "1082.2", "ups": "0.14", "wpb": "7655", "bsz": "32", "num_updates": "390", "lr": "7.8e-05", "gnorm": "1.768", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "2883"}
2022-12-20 05:05:45 | INFO | train_inner | {"epoch": 1, "update": 0.528, "loss": "2.848", "nll_loss": "2.848", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.2", "wps": "1120.5", "ups": "0.15", "wpb": "7708.4", "bsz": "32", "num_updates": "400", "lr": "8e-05", "gnorm": "1.953", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "2952"}
2022-12-20 05:06:52 | INFO | train_inner | {"epoch": 1, "update": 0.541, "loss": "2.851", "nll_loss": "2.851", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.22", "wps": "1101", "ups": "0.15", "wpb": "7408.5", "bsz": "32", "num_updates": "410", "lr": "8.2e-05", "gnorm": "1.844", "clip": "100", "loss_scale": "0.25", "train_wall": "67", "gb_free": "64.3", "wall": "3019"}
2022-12-20 05:08:02 | INFO | train_inner | {"epoch": 1, "update": 0.554, "loss": "2.804", "nll_loss": "2.804", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.99", "wps": "1079.9", "ups": "0.14", "wpb": "7578.1", "bsz": "32", "num_updates": "420", "lr": "8.4e-05", "gnorm": "1.705", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "3089"}
2022-12-20 05:09:14 | INFO | train_inner | {"epoch": 1, "update": 0.566, "loss": "2.833", "nll_loss": "2.833", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.12", "wps": "1060.7", "ups": "0.14", "wpb": "7650.6", "bsz": "32", "num_updates": "430", "lr": "8.6e-05", "gnorm": "1.766", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "65.6", "wall": "3161"}
2022-12-20 05:10:24 | INFO | train_inner | {"epoch": 1, "update": 0.579, "loss": "2.84", "nll_loss": "2.84", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.16", "wps": "1092.5", "ups": "0.14", "wpb": "7556.8", "bsz": "32", "num_updates": "440", "lr": "8.8e-05", "gnorm": "1.738", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "3231"}
2022-12-20 05:11:36 | INFO | train_inner | {"epoch": 1, "update": 0.592, "loss": "2.877", "nll_loss": "2.877", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.34", "wps": "1050.4", "ups": "0.14", "wpb": "7587.2", "bsz": "32", "num_updates": "450", "lr": "9e-05", "gnorm": "1.777", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "65.6", "wall": "3303"}
2022-12-20 05:12:46 | INFO | train_inner | {"epoch": 1, "update": 0.605, "loss": "2.867", "nll_loss": "2.867", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.29", "wps": "1099.5", "ups": "0.14", "wpb": "7680.1", "bsz": "32", "num_updates": "460", "lr": "9.2e-05", "gnorm": "1.959", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.9", "wall": "3373"}
2022-12-20 05:13:55 | INFO | train_inner | {"epoch": 1, "update": 0.618, "loss": "2.87", "nll_loss": "2.87", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.31", "wps": "1118.8", "ups": "0.14", "wpb": "7795.5", "bsz": "32", "num_updates": "470", "lr": "9.4e-05", "gnorm": "1.732", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "65", "wall": "3442"}
2022-12-20 05:15:06 | INFO | train_inner | {"epoch": 1, "update": 0.631, "loss": "2.853", "nll_loss": "2.853", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.23", "wps": "1099.4", "ups": "0.14", "wpb": "7723.1", "bsz": "32", "num_updates": "480", "lr": "9.6e-05", "gnorm": "1.804", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "65", "wall": "3513"}
2022-12-20 05:16:15 | INFO | train_inner | {"epoch": 1, "update": 0.644, "loss": "2.862", "nll_loss": "2.862", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.27", "wps": "1110.4", "ups": "0.14", "wpb": "7668", "bsz": "32", "num_updates": "490", "lr": "9.8e-05", "gnorm": "1.672", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "64.3", "wall": "3582"}
2022-12-20 05:17:26 | INFO | train_inner | {"epoch": 1, "update": 0.657, "loss": "2.928", "nll_loss": "2.928", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.61", "wps": "1090.9", "ups": "0.14", "wpb": "7791.9", "bsz": "32", "num_updates": "500", "lr": "0.0001", "gnorm": "1.737", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.9", "wall": "3653"}
2022-12-20 05:18:34 | INFO | train_inner | {"epoch": 1, "update": 0.67, "loss": "2.875", "nll_loss": "2.875", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.34", "wps": "1106.8", "ups": "0.15", "wpb": "7509.8", "bsz": "32", "num_updates": "510", "lr": "9.98947e-05", "gnorm": "1.679", "clip": "100", "loss_scale": "0.25", "train_wall": "67", "gb_free": "62.2", "wall": "3721"}
2022-12-20 05:19:44 | INFO | train_inner | {"epoch": 1, "update": 0.683, "loss": "2.905", "nll_loss": "2.905", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.49", "wps": "1092.2", "ups": "0.14", "wpb": "7646.7", "bsz": "32", "num_updates": "520", "lr": "9.97895e-05", "gnorm": "3.217", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63", "wall": "3791"}
2022-12-20 05:20:56 | INFO | train_inner | {"epoch": 1, "update": 0.695, "loss": "2.883", "nll_loss": "2.883", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.38", "wps": "1059", "ups": "0.14", "wpb": "7614", "bsz": "32", "num_updates": "530", "lr": "9.96842e-05", "gnorm": "1.683", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "63.6", "wall": "3863"}
2022-12-20 05:22:03 | INFO | train_inner | {"epoch": 1, "update": 0.708, "loss": "2.92", "nll_loss": "2.92", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.57", "wps": "1122.9", "ups": "0.15", "wpb": "7560", "bsz": "32", "num_updates": "540", "lr": "9.95789e-05", "gnorm": "1.702", "clip": "100", "loss_scale": "0.25", "train_wall": "67", "gb_free": "62.2", "wall": "3930"}
2022-12-20 05:23:14 | INFO | train_inner | {"epoch": 1, "update": 0.721, "loss": "2.867", "nll_loss": "2.867", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.29", "wps": "1068.6", "ups": "0.14", "wpb": "7581.7", "bsz": "32", "num_updates": "550", "lr": "9.94737e-05", "gnorm": "1.678", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "4001"}
2022-12-20 05:24:27 | INFO | train_inner | {"epoch": 1, "update": 0.734, "loss": "2.88", "nll_loss": "2.88", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.36", "wps": "1063", "ups": "0.14", "wpb": "7735.9", "bsz": "32", "num_updates": "560", "lr": "9.93684e-05", "gnorm": "1.642", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "64.3", "wall": "4074"}
2022-12-20 05:25:37 | INFO | train_inner | {"epoch": 1, "update": 0.747, "loss": "2.893", "nll_loss": "2.893", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.43", "wps": "1081.4", "ups": "0.14", "wpb": "7559.6", "bsz": "32", "num_updates": "570", "lr": "9.92632e-05", "gnorm": "1.694", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63", "wall": "4144"}
2022-12-20 05:26:49 | INFO | train_inner | {"epoch": 1, "update": 0.76, "loss": "2.836", "nll_loss": "2.836", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.14", "wps": "1053.6", "ups": "0.14", "wpb": "7587.2", "bsz": "32", "num_updates": "580", "lr": "9.91579e-05", "gnorm": "1.76", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "62.2", "wall": "4216"}
2022-12-20 05:27:59 | INFO | train_inner | {"epoch": 1, "update": 0.773, "loss": "2.841", "nll_loss": "2.841", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.17", "wps": "1094.3", "ups": "0.14", "wpb": "7656.4", "bsz": "32", "num_updates": "590", "lr": "9.90526e-05", "gnorm": "1.549", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.9", "wall": "4286"}
2022-12-20 05:29:05 | INFO | train_inner | {"epoch": 1, "update": 0.786, "loss": "2.878", "nll_loss": "2.878", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.35", "wps": "1136", "ups": "0.15", "wpb": "7542.4", "bsz": "32", "num_updates": "600", "lr": "9.89474e-05", "gnorm": "1.653", "clip": "100", "loss_scale": "0.25", "train_wall": "66", "gb_free": "66.3", "wall": "4352"}
2022-12-20 05:30:15 | INFO | train_inner | {"epoch": 1, "update": 0.799, "loss": "2.848", "nll_loss": "2.848", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.2", "wps": "1103.3", "ups": "0.14", "wpb": "7718.3", "bsz": "32", "num_updates": "610", "lr": "9.88421e-05", "gnorm": "1.655", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "4422"}
2022-12-20 05:31:27 | INFO | train_inner | {"epoch": 1, "update": 0.812, "loss": "2.932", "nll_loss": "2.932", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.63", "wps": "1051.7", "ups": "0.14", "wpb": "7584.1", "bsz": "32", "num_updates": "620", "lr": "9.87368e-05", "gnorm": "1.606", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "64.4", "wall": "4494"}
2022-12-20 05:32:36 | INFO | train_inner | {"epoch": 1, "update": 0.825, "loss": "2.906", "nll_loss": "2.906", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.5", "wps": "1087.4", "ups": "0.15", "wpb": "7485.2", "bsz": "32", "num_updates": "630", "lr": "9.86316e-05", "gnorm": "1.593", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.9", "wall": "4563"}
2022-12-20 05:33:48 | INFO | train_inner | {"epoch": 1, "update": 0.837, "loss": "2.894", "nll_loss": "2.894", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.43", "wps": "1059.8", "ups": "0.14", "wpb": "7595.1", "bsz": "32", "num_updates": "640", "lr": "9.85263e-05", "gnorm": "1.58", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "4635"}
2022-12-20 05:35:03 | INFO | train_inner | {"epoch": 1, "update": 0.85, "loss": "2.906", "nll_loss": "2.906", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.5", "wps": "1007.5", "ups": "0.13", "wpb": "7614", "bsz": "32", "num_updates": "650", "lr": "9.84211e-05", "gnorm": "1.563", "clip": "100", "loss_scale": "0.25", "train_wall": "75", "gb_free": "62.9", "wall": "4710"}
2022-12-20 05:36:13 | INFO | train_inner | {"epoch": 1, "update": 0.863, "loss": "2.81", "nll_loss": "2.81", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.01", "wps": "1080.8", "ups": "0.14", "wpb": "7584.4", "bsz": "32", "num_updates": "660", "lr": "9.83158e-05", "gnorm": "1.525", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "4781"}
2022-12-20 05:37:24 | INFO | train_inner | {"epoch": 1, "update": 0.876, "loss": "2.802", "nll_loss": "2.802", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.97", "wps": "1067.5", "ups": "0.14", "wpb": "7567.6", "bsz": "32", "num_updates": "670", "lr": "9.82105e-05", "gnorm": "1.548", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.9", "wall": "4851"}
2022-12-20 05:38:33 | INFO | train_inner | {"epoch": 1, "update": 0.889, "loss": "2.864", "nll_loss": "2.864", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.28", "wps": "1118.2", "ups": "0.15", "wpb": "7699.3", "bsz": "32", "num_updates": "680", "lr": "9.81053e-05", "gnorm": "1.535", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "4920"}
2022-12-20 05:39:45 | INFO | train_inner | {"epoch": 1, "update": 0.902, "loss": "2.8", "nll_loss": "2.8", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.97", "wps": "1081.9", "ups": "0.14", "wpb": "7802.4", "bsz": "32", "num_updates": "690", "lr": "9.8e-05", "gnorm": "1.554", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "63", "wall": "4992"}
2022-12-20 05:40:55 | INFO | train_inner | {"epoch": 1, "update": 0.915, "loss": "2.924", "nll_loss": "2.924", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.59", "wps": "1093.1", "ups": "0.14", "wpb": "7662.4", "bsz": "32", "num_updates": "700", "lr": "9.78947e-05", "gnorm": "1.627", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.9", "wall": "5063"}
2022-12-20 05:42:04 | INFO | train_inner | {"epoch": 1, "update": 0.928, "loss": "2.875", "nll_loss": "2.875", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.34", "wps": "1120.9", "ups": "0.15", "wpb": "7662.4", "bsz": "32", "num_updates": "710", "lr": "9.77895e-05", "gnorm": "1.636", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "63.7", "wall": "5131"}
2022-12-20 05:43:12 | INFO | train_inner | {"epoch": 1, "update": 0.941, "loss": "2.851", "nll_loss": "2.851", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.22", "wps": "1089.8", "ups": "0.15", "wpb": "7480.3", "bsz": "32", "num_updates": "720", "lr": "9.76842e-05", "gnorm": "1.636", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "5200"}
2022-12-20 05:44:21 | INFO | train_inner | {"epoch": 1, "update": 0.954, "loss": "2.862", "nll_loss": "2.862", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.27", "wps": "1115.3", "ups": "0.15", "wpb": "7689.3", "bsz": "32", "num_updates": "730", "lr": "9.75789e-05", "gnorm": "2.01", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.9", "wall": "5268"}
2022-12-20 05:45:33 | INFO | train_inner | {"epoch": 1, "update": 0.966, "loss": "2.821", "nll_loss": "2.821", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.07", "wps": "1073", "ups": "0.14", "wpb": "7658.3", "bsz": "32", "num_updates": "740", "lr": "9.74737e-05", "gnorm": "2.135", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "5340"}
2022-12-20 05:46:45 | INFO | train_inner | {"epoch": 1, "update": 0.979, "loss": "2.909", "nll_loss": "2.909", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.51", "wps": "1076.4", "ups": "0.14", "wpb": "7762.4", "bsz": "32", "num_updates": "750", "lr": "9.73684e-05", "gnorm": "2.244", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "62.2", "wall": "5412"}
2022-12-20 05:47:55 | INFO | train_inner | {"epoch": 1, "update": 0.992, "loss": "2.86", "nll_loss": "2.86", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.26", "wps": "1089.5", "ups": "0.14", "wpb": "7651.4", "bsz": "32", "num_updates": "760", "lr": "9.72632e-05", "gnorm": "3.967", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "5482"}
2022-12-20 05:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-20 05:49:20 | INFO | absl | Using default tokenizer.
2022-12-20 05:49:48 | INFO | absl | Using default tokenizer.
2022-12-20 05:50:11 | INFO | absl | Using default tokenizer.
2022-12-20 05:50:28 | INFO | absl | Using default tokenizer.
2022-12-20 05:50:50 | INFO | absl | Using default tokenizer.
2022-12-20 05:51:19 | INFO | absl | Using default tokenizer.
2022-12-20 05:51:45 | INFO | absl | Using default tokenizer.
2022-12-20 05:52:10 | INFO | absl | Using default tokenizer.
2022-12-20 05:52:43 | INFO | absl | Using default tokenizer.
2022-12-20 05:53:23 | INFO | absl | Using default tokenizer.
2022-12-20 05:53:52 | INFO | absl | Using default tokenizer.
2022-12-20 05:54:21 | INFO | absl | Using default tokenizer.
2022-12-20 05:54:50 | INFO | absl | Using default tokenizer.
2022-12-20 05:55:20 | INFO | absl | Using default tokenizer.
2022-12-20 05:55:41 | INFO | absl | Using default tokenizer.
2022-12-20 05:56:21 | INFO | absl | Using default tokenizer.
2022-12-20 05:56:46 | INFO | absl | Using default tokenizer.
2022-12-20 05:57:10 | INFO | absl | Using default tokenizer.
2022-12-20 05:57:42 | INFO | absl | Using default tokenizer.
2022-12-20 05:58:06 | INFO | absl | Using default tokenizer.
2022-12-20 05:58:35 | INFO | absl | Using default tokenizer.
2022-12-20 05:59:02 | INFO | absl | Using default tokenizer.
2022-12-20 05:59:31 | INFO | absl | Using default tokenizer.
2022-12-20 06:00:01 | INFO | absl | Using default tokenizer.
2022-12-20 06:00:30 | INFO | absl | Using default tokenizer.
2022-12-20 06:01:04 | INFO | absl | Using default tokenizer.
2022-12-20 06:01:31 | INFO | absl | Using default tokenizer.
2022-12-20 06:01:50 | INFO | absl | Using default tokenizer.
2022-12-20 06:02:19 | INFO | absl | Using default tokenizer.
2022-12-20 06:02:43 | INFO | absl | Using default tokenizer.
2022-12-20 06:03:11 | INFO | absl | Using default tokenizer.
2022-12-20 06:03:37 | INFO | absl | Using default tokenizer.
2022-12-20 06:04:11 | INFO | absl | Using default tokenizer.
2022-12-20 06:04:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:04:59 | INFO | absl | Using default tokenizer.
2022-12-20 06:05:25 | INFO | absl | Using default tokenizer.
2022-12-20 06:05:55 | INFO | absl | Using default tokenizer.
2022-12-20 06:06:24 | INFO | absl | Using default tokenizer.
2022-12-20 06:07:05 | INFO | absl | Using default tokenizer.
2022-12-20 06:07:25 | INFO | absl | Using default tokenizer.
2022-12-20 06:07:59 | INFO | absl | Using default tokenizer.
2022-12-20 06:08:24 | INFO | absl | Using default tokenizer.
2022-12-20 06:08:57 | INFO | absl | Using default tokenizer.
2022-12-20 06:09:18 | INFO | absl | Using default tokenizer.
2022-12-20 06:09:49 | INFO | absl | Using default tokenizer.
2022-12-20 06:10:30 | INFO | absl | Using default tokenizer.
2022-12-20 06:11:01 | INFO | absl | Using default tokenizer.
2022-12-20 06:11:31 | INFO | absl | Using default tokenizer.
2022-12-20 06:12:00 | INFO | absl | Using default tokenizer.
2022-12-20 06:12:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:13:01 | INFO | absl | Using default tokenizer.
2022-12-20 06:13:35 | INFO | absl | Using default tokenizer.
2022-12-20 06:14:01 | INFO | absl | Using default tokenizer.
2022-12-20 06:14:31 | INFO | absl | Using default tokenizer.
2022-12-20 06:14:59 | INFO | absl | Using default tokenizer.
2022-12-20 06:15:26 | INFO | absl | Using default tokenizer.
2022-12-20 06:15:48 | INFO | absl | Using default tokenizer.
2022-12-20 06:16:17 | INFO | absl | Using default tokenizer.
2022-12-20 06:16:55 | INFO | absl | Using default tokenizer.
2022-12-20 06:17:24 | INFO | absl | Using default tokenizer.
2022-12-20 06:17:51 | INFO | absl | Using default tokenizer.
2022-12-20 06:18:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:18:51 | INFO | absl | Using default tokenizer.
2022-12-20 06:19:26 | INFO | absl | Using default tokenizer.
2022-12-20 06:19:53 | INFO | absl | Using default tokenizer.
2022-12-20 06:20:20 | INFO | absl | Using default tokenizer.
2022-12-20 06:20:44 | INFO | absl | Using default tokenizer.
2022-12-20 06:21:13 | INFO | absl | Using default tokenizer.
2022-12-20 06:21:43 | INFO | absl | Using default tokenizer.
2022-12-20 06:22:11 | INFO | absl | Using default tokenizer.
2022-12-20 06:22:41 | INFO | absl | Using default tokenizer.
2022-12-20 06:23:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:23:46 | INFO | absl | Using default tokenizer.
2022-12-20 06:24:08 | INFO | absl | Using default tokenizer.
2022-12-20 06:24:37 | INFO | absl | Using default tokenizer.
2022-12-20 06:25:04 | INFO | absl | Using default tokenizer.
2022-12-20 06:25:34 | INFO | absl | Using default tokenizer.
2022-12-20 06:26:00 | INFO | absl | Using default tokenizer.
2022-12-20 06:26:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:26:56 | INFO | absl | Using default tokenizer.
2022-12-20 06:27:27 | INFO | absl | Using default tokenizer.
2022-12-20 06:27:53 | INFO | absl | Using default tokenizer.
2022-12-20 06:28:22 | INFO | absl | Using default tokenizer.
2022-12-20 06:28:48 | INFO | absl | Using default tokenizer.
2022-12-20 06:29:18 | INFO | absl | Using default tokenizer.
2022-12-20 06:29:42 | INFO | absl | Using default tokenizer.
2022-12-20 06:30:11 | INFO | absl | Using default tokenizer.
2022-12-20 06:30:35 | INFO | absl | Using default tokenizer.
2022-12-20 06:31:06 | INFO | absl | Using default tokenizer.
2022-12-20 06:31:32 | INFO | absl | Using default tokenizer.
2022-12-20 06:32:03 | INFO | absl | Using default tokenizer.
2022-12-20 06:32:38 | INFO | absl | Using default tokenizer.
2022-12-20 06:33:19 | INFO | absl | Using default tokenizer.
2022-12-20 06:33:55 | INFO | absl | Using default tokenizer.
2022-12-20 06:34:27 | INFO | absl | Using default tokenizer.
2022-12-20 06:34:54 | INFO | absl | Using default tokenizer.
2022-12-20 06:35:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:35:49 | INFO | absl | Using default tokenizer.
2022-12-20 06:36:15 | INFO | absl | Using default tokenizer.
2022-12-20 06:36:46 | INFO | absl | Using default tokenizer.
2022-12-20 06:37:27 | INFO | absl | Using default tokenizer.
2022-12-20 06:37:56 | INFO | absl | Using default tokenizer.
2022-12-20 06:38:26 | INFO | absl | Using default tokenizer.
2022-12-20 06:38:56 | INFO | absl | Using default tokenizer.
2022-12-20 06:39:23 | INFO | absl | Using default tokenizer.
2022-12-20 06:39:50 | INFO | absl | Using default tokenizer.
2022-12-20 06:40:22 | INFO | absl | Using default tokenizer.
2022-12-20 06:40:55 | INFO | absl | Using default tokenizer.
2022-12-20 06:41:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:41:46 | INFO | absl | Using default tokenizer.
2022-12-20 06:42:29 | INFO | absl | Using default tokenizer.
2022-12-20 06:42:56 | INFO | absl | Using default tokenizer.
2022-12-20 06:43:32 | INFO | absl | Using default tokenizer.
2022-12-20 06:44:00 | INFO | absl | Using default tokenizer.
2022-12-20 06:44:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:44:59 | INFO | absl | Using default tokenizer.
2022-12-20 06:45:27 | INFO | absl | Using default tokenizer.
2022-12-20 06:46:10 | INFO | absl | Using default tokenizer.
2022-12-20 06:46:38 | INFO | absl | Using default tokenizer.
2022-12-20 06:47:03 | INFO | absl | Using default tokenizer.
2022-12-20 06:47:37 | INFO | absl | Using default tokenizer.
2022-12-20 06:48:05 | INFO | absl | Using default tokenizer.
2022-12-20 06:48:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:49:03 | INFO | absl | Using default tokenizer.
2022-12-20 06:49:31 | INFO | absl | Using default tokenizer.
2022-12-20 06:50:00 | INFO | absl | Using default tokenizer.
2022-12-20 06:50:30 | INFO | absl | Using default tokenizer.
2022-12-20 06:50:52 | INFO | absl | Using default tokenizer.
2022-12-20 06:51:19 | INFO | absl | Using default tokenizer.
2022-12-20 06:51:45 | INFO | absl | Using default tokenizer.
2022-12-20 06:52:15 | INFO | absl | Using default tokenizer.
2022-12-20 06:52:59 | INFO | absl | Using default tokenizer.
2022-12-20 06:53:37 | INFO | absl | Using default tokenizer.
2022-12-20 06:54:04 | INFO | absl | Using default tokenizer.
2022-12-20 06:54:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:55:04 | INFO | absl | Using default tokenizer.
2022-12-20 06:55:35 | INFO | absl | Using default tokenizer.
2022-12-20 06:56:02 | INFO | absl | Using default tokenizer.
2022-12-20 06:56:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:57:01 | INFO | absl | Using default tokenizer.
2022-12-20 06:57:33 | INFO | absl | Using default tokenizer.
2022-12-20 06:58:05 | INFO | absl | Using default tokenizer.
2022-12-20 06:58:30 | INFO | absl | Using default tokenizer.
2022-12-20 06:58:53 | INFO | absl | Using default tokenizer.
2022-12-20 06:59:21 | INFO | absl | Using default tokenizer.
2022-12-20 06:59:48 | INFO | absl | Using default tokenizer.
2022-12-20 07:00:20 | INFO | absl | Using default tokenizer.
2022-12-20 07:00:47 | INFO | absl | Using default tokenizer.
2022-12-20 07:01:19 | INFO | absl | Using default tokenizer.
2022-12-20 07:01:52 | INFO | absl | Using default tokenizer.
2022-12-20 07:02:20 | INFO | absl | Using default tokenizer.
2022-12-20 07:02:55 | INFO | absl | Using default tokenizer.
2022-12-20 07:03:29 | INFO | absl | Using default tokenizer.
2022-12-20 07:03:59 | INFO | absl | Using default tokenizer.
2022-12-20 07:04:28 | INFO | absl | Using default tokenizer.
2022-12-20 07:04:56 | INFO | absl | Using default tokenizer.
2022-12-20 07:05:29 | INFO | absl | Using default tokenizer.
2022-12-20 07:06:00 | INFO | absl | Using default tokenizer.
2022-12-20 07:06:41 | INFO | absl | Using default tokenizer.
2022-12-20 07:07:04 | INFO | absl | Using default tokenizer.
2022-12-20 07:07:36 | INFO | absl | Using default tokenizer.
2022-12-20 07:08:16 | INFO | absl | Using default tokenizer.
2022-12-20 07:08:53 | INFO | absl | Using default tokenizer.
2022-12-20 07:09:30 | INFO | absl | Using default tokenizer.
2022-12-20 07:10:03 | INFO | absl | Using default tokenizer.
2022-12-20 07:10:31 | INFO | absl | Using default tokenizer.
2022-12-20 07:11:12 | INFO | absl | Using default tokenizer.
2022-12-20 07:11:43 | INFO | absl | Using default tokenizer.
2022-12-20 07:12:24 | INFO | absl | Using default tokenizer.
2022-12-20 07:12:59 | INFO | absl | Using default tokenizer.
2022-12-20 07:13:36 | INFO | absl | Using default tokenizer.
2022-12-20 07:14:09 | INFO | absl | Using default tokenizer.
2022-12-20 07:14:41 | INFO | valid | {"epoch": 1, "valid_loss": "2.791", "valid_nll_loss": "2.791", "valid_rouge1": 0.4918037802661674, "valid_rouge2": 0.18482278590991047, "valid_rougel": 0.2666913408940627, "valid_rouge_avg": 0.33831328308803893, "valid_ppl": "6.92", "valid_wps": "64", "valid_wpb": "1907.8", "valid_bsz": "8", "valid_num_updates": "766"}
2022-12-20 07:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 766 updates
2022-12-20 07:14:41 | INFO | fairseq.trainer | Saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 07:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 07:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints//graph_text/checkpoint_best.pt (epoch 1 @ 766 updates, score 0.33831328308803893) (writing took 41.923955561826006 seconds)
2022-12-20 07:15:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-20 07:15:23 | INFO | train | {"epoch": 1, "train_loss": "2.905", "train_nll_loss": "2.905", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "7.49", "train_wps": "549.4", "train_ups": "0.07", "train_wpb": "7615.2", "train_bsz": "32", "train_num_updates": "766", "train_lr": "9.72e-05", "train_gnorm": "2.587", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "5420", "train_gb_free": "63.7", "train_wall": "10730"}
2022-12-20 07:15:23 | INFO | fairseq.trainer | begin training epoch 2
2022-12-20 07:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-20 07:16:27 | INFO | train_inner | {"epoch": 2, "update": 1.005, "loss": "2.718", "nll_loss": "2.718", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.58", "wps": "13.5", "ups": "0", "wpb": "7191.8", "bsz": "29.4", "num_updates": "770", "lr": "9.71579e-05", "gnorm": "1.741", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63.7", "wall": "10795"}
2022-12-20 07:17:38 | INFO | train_inner | {"epoch": 2, "update": 1.018, "loss": "2.738", "nll_loss": "2.738", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.67", "wps": "1094.2", "ups": "0.14", "wpb": "7692.8", "bsz": "32", "num_updates": "780", "lr": "9.70526e-05", "gnorm": "1.637", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "65.6", "wall": "10865"}
2022-12-20 07:18:48 | INFO | train_inner | {"epoch": 2, "update": 1.031, "loss": "2.729", "nll_loss": "2.729", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.63", "wps": "1075.3", "ups": "0.14", "wpb": "7551.3", "bsz": "32", "num_updates": "790", "lr": "9.69474e-05", "gnorm": "1.883", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "62.2", "wall": "10935"}
2022-12-20 07:19:57 | INFO | train_inner | {"epoch": 2, "update": 1.044, "loss": "2.718", "nll_loss": "2.718", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.58", "wps": "1113.7", "ups": "0.14", "wpb": "7724.3", "bsz": "32", "num_updates": "800", "lr": "9.68421e-05", "gnorm": "1.833", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "66.3", "wall": "11004"}
2022-12-20 07:21:06 | INFO | train_inner | {"epoch": 2, "update": 1.057, "loss": "2.619", "nll_loss": "2.619", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.14", "wps": "1129.5", "ups": "0.14", "wpb": "7790.7", "bsz": "32", "num_updates": "810", "lr": "9.67368e-05", "gnorm": "2.964", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "11073"}
2022-12-20 07:22:14 | INFO | train_inner | {"epoch": 2, "update": 1.07, "loss": "2.848", "nll_loss": "2.848", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.2", "wps": "1131", "ups": "0.15", "wpb": "7674.9", "bsz": "32", "num_updates": "820", "lr": "9.66316e-05", "gnorm": "12.588", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.9", "wall": "11141"}
2022-12-20 07:23:23 | INFO | train_inner | {"epoch": 2, "update": 1.083, "loss": "2.658", "nll_loss": "2.658", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.31", "wps": "1109.2", "ups": "0.14", "wpb": "7655.7", "bsz": "32", "num_updates": "830", "lr": "9.65263e-05", "gnorm": "3.991", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "65.6", "wall": "11210"}
2022-12-20 07:24:34 | INFO | train_inner | {"epoch": 2, "update": 1.095, "loss": "2.693", "nll_loss": "2.693", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.47", "wps": "1053.1", "ups": "0.14", "wpb": "7476.8", "bsz": "32", "num_updates": "840", "lr": "9.64211e-05", "gnorm": "2.715", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "63", "wall": "11281"}
2022-12-20 07:25:45 | INFO | train_inner | {"epoch": 2, "update": 1.108, "loss": "2.645", "nll_loss": "2.645", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.25", "wps": "1091.5", "ups": "0.14", "wpb": "7669.1", "bsz": "32", "num_updates": "850", "lr": "9.63158e-05", "gnorm": "2.041", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "64.3", "wall": "11352"}
2022-12-20 07:26:54 | INFO | train_inner | {"epoch": 2, "update": 1.121, "loss": "2.672", "nll_loss": "2.672", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.38", "wps": "1090.6", "ups": "0.14", "wpb": "7541.2", "bsz": "32", "num_updates": "860", "lr": "9.62105e-05", "gnorm": "1.851", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.2", "wall": "11421"}
2022-12-20 07:28:03 | INFO | train_inner | {"epoch": 2, "update": 1.134, "loss": "2.633", "nll_loss": "2.633", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.2", "wps": "1082", "ups": "0.14", "wpb": "7516.1", "bsz": "32", "num_updates": "870", "lr": "9.61053e-05", "gnorm": "2.237", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.9", "wall": "11490"}
2022-12-20 07:29:15 | INFO | train_inner | {"epoch": 2, "update": 1.147, "loss": "2.668", "nll_loss": "2.668", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.35", "wps": "1047.6", "ups": "0.14", "wpb": "7506.1", "bsz": "32", "num_updates": "880", "lr": "9.6e-05", "gnorm": "3.114", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "65.6", "wall": "11562"}
2022-12-20 07:30:26 | INFO | train_inner | {"epoch": 2, "update": 1.16, "loss": "2.686", "nll_loss": "2.686", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.43", "wps": "1061", "ups": "0.14", "wpb": "7546", "bsz": "32", "num_updates": "890", "lr": "9.58947e-05", "gnorm": "2.572", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.9", "wall": "11633"}
2022-12-20 07:31:35 | INFO | train_inner | {"epoch": 2, "update": 1.173, "loss": "2.666", "nll_loss": "2.666", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.34", "wps": "1110.7", "ups": "0.14", "wpb": "7706.9", "bsz": "32", "num_updates": "900", "lr": "9.57895e-05", "gnorm": "2.738", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63", "wall": "11703"}
2022-12-20 07:32:44 | INFO | train_inner | {"epoch": 2, "update": 1.186, "loss": "2.723", "nll_loss": "2.723", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.6", "wps": "1106.3", "ups": "0.15", "wpb": "7541.9", "bsz": "32", "num_updates": "910", "lr": "9.56842e-05", "gnorm": "1.732", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "65.6", "wall": "11771"}
2022-12-20 07:33:53 | INFO | train_inner | {"epoch": 2, "update": 1.199, "loss": "2.725", "nll_loss": "2.725", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.61", "wps": "1087", "ups": "0.14", "wpb": "7570.1", "bsz": "32", "num_updates": "920", "lr": "9.55789e-05", "gnorm": "1.651", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "65", "wall": "11840"}
2022-12-20 07:35:03 | INFO | train_inner | {"epoch": 2, "update": 1.212, "loss": "2.622", "nll_loss": "2.622", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.16", "wps": "1106.9", "ups": "0.14", "wpb": "7731.3", "bsz": "32", "num_updates": "930", "lr": "9.54737e-05", "gnorm": "1.862", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.9", "wall": "11910"}
2022-12-20 07:36:13 | INFO | train_inner | {"epoch": 2, "update": 1.225, "loss": "2.697", "nll_loss": "2.697", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.49", "wps": "1094.2", "ups": "0.14", "wpb": "7590.4", "bsz": "32", "num_updates": "940", "lr": "9.53684e-05", "gnorm": "1.744", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63.6", "wall": "11980"}
2022-12-20 07:37:24 | INFO | train_inner | {"epoch": 2, "update": 1.237, "loss": "2.652", "nll_loss": "2.652", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.29", "wps": "1082.1", "ups": "0.14", "wpb": "7739.4", "bsz": "32", "num_updates": "950", "lr": "9.52632e-05", "gnorm": "1.642", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.9", "wall": "12051"}
2022-12-20 07:38:34 | INFO | train_inner | {"epoch": 2, "update": 1.25, "loss": "2.682", "nll_loss": "2.682", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.42", "wps": "1116.7", "ups": "0.14", "wpb": "7762.7", "bsz": "32", "num_updates": "960", "lr": "9.51579e-05", "gnorm": "1.625", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63", "wall": "12121"}
2022-12-20 07:39:42 | INFO | train_inner | {"epoch": 2, "update": 1.263, "loss": "2.702", "nll_loss": "2.702", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.51", "wps": "1123.9", "ups": "0.15", "wpb": "7714.9", "bsz": "32", "num_updates": "970", "lr": "9.50526e-05", "gnorm": "1.623", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "12189"}
2022-12-20 07:40:51 | INFO | train_inner | {"epoch": 2, "update": 1.276, "loss": "2.689", "nll_loss": "2.689", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.45", "wps": "1092.1", "ups": "0.15", "wpb": "7489.3", "bsz": "32", "num_updates": "980", "lr": "9.49474e-05", "gnorm": "2.546", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "63.6", "wall": "12258"}
2022-12-20 07:42:03 | INFO | train_inner | {"epoch": 2, "update": 1.289, "loss": "2.628", "nll_loss": "2.628", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.18", "wps": "1045.8", "ups": "0.14", "wpb": "7545.3", "bsz": "32", "num_updates": "990", "lr": "9.48421e-05", "gnorm": "1.583", "clip": "100", "loss_scale": "0.25", "train_wall": "72", "gb_free": "62.9", "wall": "12330"}
2022-12-20 07:43:14 | INFO | train_inner | {"epoch": 2, "update": 1.302, "loss": "2.686", "nll_loss": "2.686", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.44", "wps": "1099.5", "ups": "0.14", "wpb": "7828.2", "bsz": "32", "num_updates": "1000", "lr": "9.47368e-05", "gnorm": "1.606", "clip": "100", "loss_scale": "0.25", "train_wall": "71", "gb_free": "62.2", "wall": "12401"}
2022-12-20 07:44:23 | INFO | train_inner | {"epoch": 2, "update": 1.315, "loss": "2.643", "nll_loss": "2.643", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.25", "wps": "1094.5", "ups": "0.15", "wpb": "7526.7", "bsz": "32", "num_updates": "1010", "lr": "9.46316e-05", "gnorm": "1.945", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "12470"}
2022-12-20 07:45:33 | INFO | train_inner | {"epoch": 2, "update": 1.328, "loss": "2.594", "nll_loss": "2.594", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.04", "wps": "1073.1", "ups": "0.14", "wpb": "7471.6", "bsz": "32", "num_updates": "1020", "lr": "9.45263e-05", "gnorm": "1.788", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "63", "wall": "12540"}
2022-12-20 07:46:46 | INFO | train_inner | {"epoch": 2, "update": 1.341, "loss": "2.658", "nll_loss": "2.658", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.31", "wps": "1041.3", "ups": "0.14", "wpb": "7617.9", "bsz": "32", "num_updates": "1030", "lr": "9.44211e-05", "gnorm": "1.973", "clip": "100", "loss_scale": "0.25", "train_wall": "73", "gb_free": "64.3", "wall": "12613"}
2022-12-20 07:47:57 | INFO | train_inner | {"epoch": 2, "update": 1.354, "loss": "2.629", "nll_loss": "2.629", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.18", "wps": "1088.9", "ups": "0.14", "wpb": "7696.9", "bsz": "32", "num_updates": "1040", "lr": "9.43158e-05", "gnorm": "1.467", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "63.6", "wall": "12684"}
2022-12-20 07:49:05 | INFO | train_inner | {"epoch": 2, "update": 1.366, "loss": "2.668", "nll_loss": "2.668", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.35", "wps": "1107", "ups": "0.15", "wpb": "7605.1", "bsz": "32", "num_updates": "1050", "lr": "9.42105e-05", "gnorm": "2.381", "clip": "100", "loss_scale": "0.25", "train_wall": "68", "gb_free": "62.2", "wall": "12752"}
2022-12-20 07:50:13 | INFO | train_inner | {"epoch": 2, "update": 1.379, "loss": "2.678", "nll_loss": "2.678", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.4", "wps": "1122.5", "ups": "0.15", "wpb": "7555.9", "bsz": "32", "num_updates": "1060", "lr": "9.41053e-05", "gnorm": "1.508", "clip": "100", "loss_scale": "0.25", "train_wall": "67", "gb_free": "64.3", "wall": "12820"}
2022-12-20 07:51:23 | INFO | train_inner | {"epoch": 2, "update": 1.392, "loss": "2.675", "nll_loss": "2.675", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.38", "wps": "1088.3", "ups": "0.14", "wpb": "7658", "bsz": "32", "num_updates": "1070", "lr": "9.4e-05", "gnorm": "1.456", "clip": "100", "loss_scale": "0.25", "train_wall": "70", "gb_free": "64.3", "wall": "12890"}
2022-12-20 07:52:32 | INFO | train_inner | {"epoch": 2, "update": 1.405, "loss": "2.664", "nll_loss": "2.664", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.34", "wps": "1099.9", "ups": "0.14", "wpb": "7609.3", "bsz": "32", "num_updates": "1080", "lr": "9.38947e-05", "gnorm": "1.712", "clip": "100", "loss_scale": "0.25", "train_wall": "69", "gb_free": "62.9", "wall": "12959"}
2022-12-20 07:53:43 | INFO | train_inner | {"epoch": 2, "update": 1.418, "loss": "2.688", "nll_loss": "2.688", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.44", "wps": "1068.5", "ups": "0.14", "wpb": "7565.7", "bsz": "32", "num_updates": "1090", "lr": "9.37895e-05", "gnorm": "1.472", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "66.3", "wall": "13030"}
2022-12-20 07:54:55 | INFO | train_inner | {"epoch": 2, "update": 1.431, "loss": "2.652", "nll_loss": "2.652", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.29", "wps": "1073.5", "ups": "0.14", "wpb": "7709.3", "bsz": "32", "num_updates": "1100", "lr": "9.36842e-05", "gnorm": "1.567", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "13102"}
2022-12-20 07:56:05 | INFO | train_inner | {"epoch": 2, "update": 1.444, "loss": "2.718", "nll_loss": "2.718", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.58", "wps": "1076.4", "ups": "0.14", "wpb": "7589.8", "bsz": "32", "num_updates": "1110", "lr": "9.35789e-05", "gnorm": "1.501", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "13173"}
2022-12-20 07:57:20 | INFO | train_inner | {"epoch": 2, "update": 1.457, "loss": "2.643", "nll_loss": "2.643", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.25", "wps": "1059.6", "ups": "0.14", "wpb": "7844.6", "bsz": "32", "num_updates": "1120", "lr": "9.34737e-05", "gnorm": "2.613", "clip": "100", "loss_scale": "0.5", "train_wall": "74", "gb_free": "64.3", "wall": "13247"}
2022-12-20 07:58:28 | INFO | train_inner | {"epoch": 2, "update": 1.47, "loss": "2.653", "nll_loss": "2.653", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.29", "wps": "1115.4", "ups": "0.15", "wpb": "7683.5", "bsz": "32", "num_updates": "1130", "lr": "9.33684e-05", "gnorm": "1.47", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "64.3", "wall": "13316"}
2022-12-20 07:59:37 | INFO | train_inner | {"epoch": 2, "update": 1.483, "loss": "2.652", "nll_loss": "2.652", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.29", "wps": "1100.6", "ups": "0.14", "wpb": "7596.7", "bsz": "32", "num_updates": "1140", "lr": "9.32632e-05", "gnorm": "1.445", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "66.4", "wall": "13385"}
2022-12-20 08:00:47 | INFO | train_inner | {"epoch": 2, "update": 1.495, "loss": "2.731", "nll_loss": "2.731", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.64", "wps": "1080.9", "ups": "0.14", "wpb": "7547.3", "bsz": "32", "num_updates": "1150", "lr": "9.31579e-05", "gnorm": "1.55", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "13454"}
2022-12-20 08:01:58 | INFO | train_inner | {"epoch": 2, "update": 1.508, "loss": "2.701", "nll_loss": "2.701", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.5", "wps": "1056.1", "ups": "0.14", "wpb": "7476.3", "bsz": "32", "num_updates": "1160", "lr": "9.30526e-05", "gnorm": "1.464", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "13525"}
2022-12-20 08:03:09 | INFO | train_inner | {"epoch": 2, "update": 1.521, "loss": "2.678", "nll_loss": "2.678", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.4", "wps": "1074.8", "ups": "0.14", "wpb": "7567.3", "bsz": "32", "num_updates": "1170", "lr": "9.29474e-05", "gnorm": "1.576", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "13596"}
2022-12-20 08:04:23 | INFO | train_inner | {"epoch": 2, "update": 1.534, "loss": "2.701", "nll_loss": "2.701", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.5", "wps": "1017.5", "ups": "0.13", "wpb": "7563.6", "bsz": "32", "num_updates": "1180", "lr": "9.28421e-05", "gnorm": "1.61", "clip": "100", "loss_scale": "0.5", "train_wall": "74", "gb_free": "63", "wall": "13670"}
2022-12-20 08:05:34 | INFO | train_inner | {"epoch": 2, "update": 1.547, "loss": "2.72", "nll_loss": "2.72", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.59", "wps": "1065.2", "ups": "0.14", "wpb": "7591.5", "bsz": "32", "num_updates": "1190", "lr": "9.27368e-05", "gnorm": "1.501", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.7", "wall": "13741"}
2022-12-20 08:06:45 | INFO | train_inner | {"epoch": 2, "update": 1.56, "loss": "2.701", "nll_loss": "2.701", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.5", "wps": "1072.1", "ups": "0.14", "wpb": "7587.3", "bsz": "32", "num_updates": "1200", "lr": "9.26316e-05", "gnorm": "1.441", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "13812"}
2022-12-20 08:07:56 | INFO | train_inner | {"epoch": 2, "update": 1.573, "loss": "2.685", "nll_loss": "2.685", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.43", "wps": "1097.1", "ups": "0.14", "wpb": "7747.1", "bsz": "32", "num_updates": "1210", "lr": "9.25263e-05", "gnorm": "1.462", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "67", "wall": "13883"}
2022-12-20 08:09:06 | INFO | train_inner | {"epoch": 2, "update": 1.586, "loss": "2.674", "nll_loss": "2.674", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.38", "wps": "1095", "ups": "0.14", "wpb": "7701.9", "bsz": "32", "num_updates": "1220", "lr": "9.24211e-05", "gnorm": "1.433", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "13953"}
2022-12-20 08:10:17 | INFO | train_inner | {"epoch": 2, "update": 1.599, "loss": "2.672", "nll_loss": "2.672", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.37", "wps": "1096.6", "ups": "0.14", "wpb": "7783.9", "bsz": "32", "num_updates": "1230", "lr": "9.23158e-05", "gnorm": "1.373", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "14024"}
2022-12-20 08:11:28 | INFO | train_inner | {"epoch": 2, "update": 1.612, "loss": "2.692", "nll_loss": "2.692", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.46", "wps": "1068.9", "ups": "0.14", "wpb": "7590.7", "bsz": "32", "num_updates": "1240", "lr": "9.22105e-05", "gnorm": "1.401", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "14095"}
2022-12-20 08:12:41 | INFO | train_inner | {"epoch": 2, "update": 1.625, "loss": "2.615", "nll_loss": "2.615", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.12", "wps": "1076.8", "ups": "0.14", "wpb": "7819.2", "bsz": "32", "num_updates": "1250", "lr": "9.21053e-05", "gnorm": "1.381", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "62.2", "wall": "14168"}
2022-12-20 08:13:50 | INFO | train_inner | {"epoch": 2, "update": 1.637, "loss": "2.623", "nll_loss": "2.623", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.16", "wps": "1112.5", "ups": "0.14", "wpb": "7765.1", "bsz": "32", "num_updates": "1260", "lr": "9.2e-05", "gnorm": "1.437", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "63", "wall": "14238"}
2022-12-20 08:15:02 | INFO | train_inner | {"epoch": 2, "update": 1.65, "loss": "2.668", "nll_loss": "2.668", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.36", "wps": "1069.9", "ups": "0.14", "wpb": "7641.5", "bsz": "32", "num_updates": "1270", "lr": "9.18947e-05", "gnorm": "1.419", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "14309"}
2022-12-20 08:16:11 | INFO | train_inner | {"epoch": 2, "update": 1.663, "loss": "2.604", "nll_loss": "2.604", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.08", "wps": "1091.1", "ups": "0.14", "wpb": "7569.4", "bsz": "32", "num_updates": "1280", "lr": "9.17895e-05", "gnorm": "1.397", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "14378"}
2022-12-20 08:17:22 | INFO | train_inner | {"epoch": 2, "update": 1.676, "loss": "2.673", "nll_loss": "2.673", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.38", "wps": "1062.9", "ups": "0.14", "wpb": "7473.7", "bsz": "32", "num_updates": "1290", "lr": "9.16842e-05", "gnorm": "1.503", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63", "wall": "14449"}
2022-12-20 08:18:30 | INFO | train_inner | {"epoch": 2, "update": 1.689, "loss": "2.601", "nll_loss": "2.601", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.06", "wps": "1116.6", "ups": "0.15", "wpb": "7675.8", "bsz": "32", "num_updates": "1300", "lr": "9.15789e-05", "gnorm": "1.673", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "65", "wall": "14517"}
2022-12-20 08:19:41 | INFO | train_inner | {"epoch": 2, "update": 1.702, "loss": "2.615", "nll_loss": "2.615", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.13", "wps": "1069.7", "ups": "0.14", "wpb": "7577.1", "bsz": "32", "num_updates": "1310", "lr": "9.14737e-05", "gnorm": "1.48", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "14588"}
2022-12-20 08:20:54 | INFO | train_inner | {"epoch": 2, "update": 1.715, "loss": "2.65", "nll_loss": "2.65", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.28", "wps": "1060.6", "ups": "0.14", "wpb": "7681.4", "bsz": "31.9", "num_updates": "1320", "lr": "9.13684e-05", "gnorm": "1.41", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "64.3", "wall": "14661"}
2022-12-20 08:22:05 | INFO | train_inner | {"epoch": 2, "update": 1.728, "loss": "2.696", "nll_loss": "2.696", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.48", "wps": "1068.3", "ups": "0.14", "wpb": "7572.3", "bsz": "32", "num_updates": "1330", "lr": "9.12632e-05", "gnorm": "1.506", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63", "wall": "14732"}
2022-12-20 08:23:14 | INFO | train_inner | {"epoch": 2, "update": 1.741, "loss": "2.622", "nll_loss": "2.622", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.16", "wps": "1107.5", "ups": "0.14", "wpb": "7722.9", "bsz": "32", "num_updates": "1340", "lr": "9.11579e-05", "gnorm": "1.408", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "64.3", "wall": "14801"}
2022-12-20 08:24:25 | INFO | train_inner | {"epoch": 2, "update": 1.754, "loss": "2.642", "nll_loss": "2.642", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.24", "wps": "1069.9", "ups": "0.14", "wpb": "7561.2", "bsz": "32", "num_updates": "1350", "lr": "9.10526e-05", "gnorm": "1.381", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63", "wall": "14872"}
2022-12-20 08:25:32 | INFO | train_inner | {"epoch": 2, "update": 1.766, "loss": "2.656", "nll_loss": "2.656", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.3", "wps": "1125.3", "ups": "0.15", "wpb": "7493", "bsz": "32", "num_updates": "1360", "lr": "9.09474e-05", "gnorm": "1.407", "clip": "100", "loss_scale": "0.5", "train_wall": "66", "gb_free": "63.6", "wall": "14939"}
2022-12-20 08:26:42 | INFO | train_inner | {"epoch": 2, "update": 1.779, "loss": "2.648", "nll_loss": "2.648", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.27", "wps": "1088.8", "ups": "0.14", "wpb": "7655", "bsz": "32", "num_updates": "1370", "lr": "9.08421e-05", "gnorm": "1.397", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "15009"}
2022-12-20 08:27:53 | INFO | train_inner | {"epoch": 2, "update": 1.792, "loss": "2.662", "nll_loss": "2.662", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.33", "wps": "1065.7", "ups": "0.14", "wpb": "7523.4", "bsz": "32", "num_updates": "1380", "lr": "9.07368e-05", "gnorm": "1.394", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "15080"}
2022-12-20 08:29:05 | INFO | train_inner | {"epoch": 2, "update": 1.805, "loss": "2.631", "nll_loss": "2.631", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.2", "wps": "1066.5", "ups": "0.14", "wpb": "7673.9", "bsz": "32", "num_updates": "1390", "lr": "9.06316e-05", "gnorm": "1.679", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63", "wall": "15152"}
2022-12-20 08:30:15 | INFO | train_inner | {"epoch": 2, "update": 1.818, "loss": "2.683", "nll_loss": "2.683", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.42", "wps": "1094.9", "ups": "0.14", "wpb": "7677.4", "bsz": "32", "num_updates": "1400", "lr": "9.05263e-05", "gnorm": "1.419", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "15222"}
2022-12-20 08:31:25 | INFO | train_inner | {"epoch": 2, "update": 1.831, "loss": "2.598", "nll_loss": "2.598", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.05", "wps": "1083.9", "ups": "0.14", "wpb": "7659.9", "bsz": "32", "num_updates": "1410", "lr": "9.04211e-05", "gnorm": "1.382", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "15293"}
2022-12-20 08:32:36 | INFO | train_inner | {"epoch": 2, "update": 1.844, "loss": "2.634", "nll_loss": "2.634", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.21", "wps": "1065.6", "ups": "0.14", "wpb": "7529.4", "bsz": "32", "num_updates": "1420", "lr": "9.03158e-05", "gnorm": "1.487", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "65.6", "wall": "15363"}
2022-12-20 08:33:46 | INFO | train_inner | {"epoch": 2, "update": 1.857, "loss": "2.722", "nll_loss": "2.722", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.6", "wps": "1099.8", "ups": "0.14", "wpb": "7679.8", "bsz": "32", "num_updates": "1430", "lr": "9.02105e-05", "gnorm": "1.506", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "66.3", "wall": "15433"}
2022-12-20 08:34:56 | INFO | train_inner | {"epoch": 2, "update": 1.87, "loss": "2.625", "nll_loss": "2.625", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.17", "wps": "1074.6", "ups": "0.14", "wpb": "7541.2", "bsz": "32", "num_updates": "1440", "lr": "9.01053e-05", "gnorm": "1.5", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "15503"}
2022-12-20 08:36:05 | INFO | train_inner | {"epoch": 2, "update": 1.883, "loss": "2.631", "nll_loss": "2.631", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.19", "wps": "1112", "ups": "0.14", "wpb": "7679.5", "bsz": "32", "num_updates": "1450", "lr": "9e-05", "gnorm": "1.429", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "15572"}
2022-12-20 08:37:14 | INFO | train_inner | {"epoch": 2, "update": 1.895, "loss": "2.643", "nll_loss": "2.643", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.25", "wps": "1119", "ups": "0.15", "wpb": "7632.5", "bsz": "32", "num_updates": "1460", "lr": "8.98947e-05", "gnorm": "1.451", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "15641"}
2022-12-20 08:38:25 | INFO | train_inner | {"epoch": 2, "update": 1.908, "loss": "2.603", "nll_loss": "2.603", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.08", "wps": "1039.8", "ups": "0.14", "wpb": "7446.8", "bsz": "32", "num_updates": "1470", "lr": "8.97895e-05", "gnorm": "1.488", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "15712"}
2022-12-20 08:39:35 | INFO | train_inner | {"epoch": 2, "update": 1.921, "loss": "2.651", "nll_loss": "2.651", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.28", "wps": "1088.2", "ups": "0.14", "wpb": "7643.2", "bsz": "32", "num_updates": "1480", "lr": "8.96842e-05", "gnorm": "1.405", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "15783"}
2022-12-20 08:40:47 | INFO | train_inner | {"epoch": 2, "update": 1.934, "loss": "2.607", "nll_loss": "2.607", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.09", "wps": "1067.2", "ups": "0.14", "wpb": "7601.3", "bsz": "32", "num_updates": "1490", "lr": "8.95789e-05", "gnorm": "1.656", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.6", "wall": "15854"}
2022-12-20 08:41:56 | INFO | train_inner | {"epoch": 2, "update": 1.947, "loss": "2.615", "nll_loss": "2.615", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.13", "wps": "1108", "ups": "0.14", "wpb": "7670.1", "bsz": "32", "num_updates": "1500", "lr": "8.94737e-05", "gnorm": "1.36", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "63", "wall": "15923"}
2022-12-20 08:43:10 | INFO | train_inner | {"epoch": 2, "update": 1.96, "loss": "2.679", "nll_loss": "2.679", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.4", "wps": "1036.3", "ups": "0.14", "wpb": "7626.2", "bsz": "32", "num_updates": "1510", "lr": "8.93684e-05", "gnorm": "1.405", "clip": "100", "loss_scale": "0.5", "train_wall": "73", "gb_free": "63", "wall": "15997"}
2022-12-20 08:44:18 | INFO | train_inner | {"epoch": 2, "update": 1.973, "loss": "2.654", "nll_loss": "2.654", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.29", "wps": "1092.5", "ups": "0.15", "wpb": "7481.8", "bsz": "32", "num_updates": "1520", "lr": "8.92632e-05", "gnorm": "1.422", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "63.6", "wall": "16065"}
2022-12-20 08:45:25 | INFO | train_inner | {"epoch": 2, "update": 1.986, "loss": "2.645", "nll_loss": "2.645", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.25", "wps": "1144.6", "ups": "0.15", "wpb": "7703.5", "bsz": "32", "num_updates": "1530", "lr": "8.91579e-05", "gnorm": "1.403", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "65", "wall": "16132"}
2022-12-20 08:46:40 | INFO | train_inner | {"epoch": 2, "update": 1.999, "loss": "2.615", "nll_loss": "2.615", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.13", "wps": "1035.3", "ups": "0.13", "wpb": "7766.3", "bsz": "32", "num_updates": "1540", "lr": "8.90526e-05", "gnorm": "1.427", "clip": "100", "loss_scale": "0.5", "train_wall": "75", "gb_free": "63", "wall": "16208"}
2022-12-20 08:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-20 08:47:26 | INFO | absl | Using default tokenizer.
2022-12-20 08:47:53 | INFO | absl | Using default tokenizer.
2022-12-20 08:48:13 | INFO | absl | Using default tokenizer.
2022-12-20 08:48:33 | INFO | absl | Using default tokenizer.
2022-12-20 08:48:55 | INFO | absl | Using default tokenizer.
2022-12-20 08:49:22 | INFO | absl | Using default tokenizer.
2022-12-20 08:49:52 | INFO | absl | Using default tokenizer.
2022-12-20 08:50:19 | INFO | absl | Using default tokenizer.
2022-12-20 08:50:50 | INFO | absl | Using default tokenizer.
2022-12-20 08:51:18 | INFO | absl | Using default tokenizer.
2022-12-20 08:51:45 | INFO | absl | Using default tokenizer.
2022-12-20 08:52:13 | INFO | absl | Using default tokenizer.
2022-12-20 08:52:43 | INFO | absl | Using default tokenizer.
2022-12-20 08:53:21 | INFO | absl | Using default tokenizer.
2022-12-20 08:53:48 | INFO | absl | Using default tokenizer.
2022-12-20 08:54:14 | INFO | absl | Using default tokenizer.
2022-12-20 08:54:41 | INFO | absl | Using default tokenizer.
2022-12-20 08:55:09 | INFO | absl | Using default tokenizer.
2022-12-20 08:55:37 | INFO | absl | Using default tokenizer.
2022-12-20 08:55:59 | INFO | absl | Using default tokenizer.
2022-12-20 08:56:38 | INFO | absl | Using default tokenizer.
2022-12-20 08:57:21 | INFO | absl | Using default tokenizer.
2022-12-20 08:57:49 | INFO | absl | Using default tokenizer.
2022-12-20 08:58:16 | INFO | absl | Using default tokenizer.
2022-12-20 08:58:46 | INFO | absl | Using default tokenizer.
2022-12-20 08:59:11 | INFO | absl | Using default tokenizer.
2022-12-20 08:59:37 | INFO | absl | Using default tokenizer.
2022-12-20 09:00:09 | INFO | absl | Using default tokenizer.
2022-12-20 09:00:36 | INFO | absl | Using default tokenizer.
2022-12-20 09:01:06 | INFO | absl | Using default tokenizer.
2022-12-20 09:01:30 | INFO | absl | Using default tokenizer.
2022-12-20 09:02:01 | INFO | absl | Using default tokenizer.
2022-12-20 09:02:24 | INFO | absl | Using default tokenizer.
2022-12-20 09:02:54 | INFO | absl | Using default tokenizer.
2022-12-20 09:03:21 | INFO | absl | Using default tokenizer.
2022-12-20 09:03:53 | INFO | absl | Using default tokenizer.
2022-12-20 09:04:27 | INFO | absl | Using default tokenizer.
2022-12-20 09:04:55 | INFO | absl | Using default tokenizer.
2022-12-20 09:05:36 | INFO | absl | Using default tokenizer.
2022-12-20 09:05:59 | INFO | absl | Using default tokenizer.
2022-12-20 09:06:25 | INFO | absl | Using default tokenizer.
2022-12-20 09:06:53 | INFO | absl | Using default tokenizer.
2022-12-20 09:07:18 | INFO | absl | Using default tokenizer.
2022-12-20 09:07:41 | INFO | absl | Using default tokenizer.
2022-12-20 09:08:17 | INFO | absl | Using default tokenizer.
2022-12-20 09:08:47 | INFO | absl | Using default tokenizer.
2022-12-20 09:09:12 | INFO | absl | Using default tokenizer.
2022-12-20 09:09:48 | INFO | absl | Using default tokenizer.
2022-12-20 09:10:25 | INFO | absl | Using default tokenizer.
2022-12-20 09:10:57 | INFO | absl | Using default tokenizer.
2022-12-20 09:11:21 | INFO | absl | Using default tokenizer.
2022-12-20 09:11:47 | INFO | absl | Using default tokenizer.
2022-12-20 09:12:16 | INFO | absl | Using default tokenizer.
2022-12-20 09:12:42 | INFO | absl | Using default tokenizer.
2022-12-20 09:13:12 | INFO | absl | Using default tokenizer.
2022-12-20 09:13:43 | INFO | absl | Using default tokenizer.
2022-12-20 09:14:16 | INFO | absl | Using default tokenizer.
2022-12-20 09:14:46 | INFO | absl | Using default tokenizer.
2022-12-20 09:15:18 | INFO | absl | Using default tokenizer.
2022-12-20 09:15:41 | INFO | absl | Using default tokenizer.
2022-12-20 09:16:14 | INFO | absl | Using default tokenizer.
2022-12-20 09:16:45 | INFO | absl | Using default tokenizer.
2022-12-20 09:17:16 | INFO | absl | Using default tokenizer.
2022-12-20 09:17:43 | INFO | absl | Using default tokenizer.
2022-12-20 09:18:11 | INFO | absl | Using default tokenizer.
2022-12-20 09:18:42 | INFO | absl | Using default tokenizer.
2022-12-20 09:19:03 | INFO | absl | Using default tokenizer.
2022-12-20 09:19:31 | INFO | absl | Using default tokenizer.
2022-12-20 09:19:59 | INFO | absl | Using default tokenizer.
2022-12-20 09:20:38 | INFO | absl | Using default tokenizer.
2022-12-20 09:21:04 | INFO | absl | Using default tokenizer.
2022-12-20 09:21:29 | INFO | absl | Using default tokenizer.
2022-12-20 09:22:00 | INFO | absl | Using default tokenizer.
2022-12-20 09:22:34 | INFO | absl | Using default tokenizer.
2022-12-20 09:23:06 | INFO | absl | Using default tokenizer.
2022-12-20 09:23:32 | INFO | absl | Using default tokenizer.
2022-12-20 09:24:06 | INFO | absl | Using default tokenizer.
2022-12-20 09:24:36 | INFO | absl | Using default tokenizer.
2022-12-20 09:25:01 | INFO | absl | Using default tokenizer.
2022-12-20 09:25:30 | INFO | absl | Using default tokenizer.
2022-12-20 09:26:04 | INFO | absl | Using default tokenizer.
2022-12-20 09:26:36 | INFO | absl | Using default tokenizer.
2022-12-20 09:27:09 | INFO | absl | Using default tokenizer.
2022-12-20 09:27:35 | INFO | absl | Using default tokenizer.
2022-12-20 09:28:03 | INFO | absl | Using default tokenizer.
2022-12-20 09:28:29 | INFO | absl | Using default tokenizer.
2022-12-20 09:28:55 | INFO | absl | Using default tokenizer.
2022-12-20 09:29:23 | INFO | absl | Using default tokenizer.
2022-12-20 09:29:54 | INFO | absl | Using default tokenizer.
2022-12-20 09:30:20 | INFO | absl | Using default tokenizer.
2022-12-20 09:30:52 | INFO | absl | Using default tokenizer.
2022-12-20 09:31:28 | INFO | absl | Using default tokenizer.
2022-12-20 09:31:54 | INFO | absl | Using default tokenizer.
2022-12-20 09:32:17 | INFO | absl | Using default tokenizer.
2022-12-20 09:32:52 | INFO | absl | Using default tokenizer.
2022-12-20 09:33:21 | INFO | absl | Using default tokenizer.
2022-12-20 09:33:48 | INFO | absl | Using default tokenizer.
2022-12-20 09:34:16 | INFO | absl | Using default tokenizer.
2022-12-20 09:34:46 | INFO | absl | Using default tokenizer.
2022-12-20 09:35:13 | INFO | absl | Using default tokenizer.
2022-12-20 09:35:50 | INFO | absl | Using default tokenizer.
2022-12-20 09:36:30 | INFO | absl | Using default tokenizer.
2022-12-20 09:37:08 | INFO | absl | Using default tokenizer.
2022-12-20 09:37:40 | INFO | absl | Using default tokenizer.
2022-12-20 09:38:15 | INFO | absl | Using default tokenizer.
2022-12-20 09:38:48 | INFO | absl | Using default tokenizer.
2022-12-20 09:39:20 | INFO | absl | Using default tokenizer.
2022-12-20 09:40:02 | INFO | absl | Using default tokenizer.
2022-12-20 09:40:28 | INFO | absl | Using default tokenizer.
2022-12-20 09:41:00 | INFO | absl | Using default tokenizer.
2022-12-20 09:41:30 | INFO | absl | Using default tokenizer.
2022-12-20 09:41:58 | INFO | absl | Using default tokenizer.
2022-12-20 09:42:35 | INFO | absl | Using default tokenizer.
2022-12-20 09:43:06 | INFO | absl | Using default tokenizer.
2022-12-20 09:43:37 | INFO | absl | Using default tokenizer.
2022-12-20 09:44:04 | INFO | absl | Using default tokenizer.
2022-12-20 09:44:29 | INFO | absl | Using default tokenizer.
2022-12-20 09:44:57 | INFO | absl | Using default tokenizer.
2022-12-20 09:45:26 | INFO | absl | Using default tokenizer.
2022-12-20 09:45:52 | INFO | absl | Using default tokenizer.
2022-12-20 09:46:20 | INFO | absl | Using default tokenizer.
2022-12-20 09:46:52 | INFO | absl | Using default tokenizer.
2022-12-20 09:47:23 | INFO | absl | Using default tokenizer.
2022-12-20 09:47:53 | INFO | absl | Using default tokenizer.
2022-12-20 09:48:28 | INFO | absl | Using default tokenizer.
2022-12-20 09:49:03 | INFO | absl | Using default tokenizer.
2022-12-20 09:49:45 | INFO | absl | Using default tokenizer.
2022-12-20 09:50:09 | INFO | absl | Using default tokenizer.
2022-12-20 09:50:37 | INFO | absl | Using default tokenizer.
2022-12-20 09:51:04 | INFO | absl | Using default tokenizer.
2022-12-20 09:51:43 | INFO | absl | Using default tokenizer.
2022-12-20 09:52:24 | INFO | absl | Using default tokenizer.
2022-12-20 09:52:58 | INFO | absl | Using default tokenizer.
2022-12-20 09:53:34 | INFO | absl | Using default tokenizer.
2022-12-20 09:54:05 | INFO | absl | Using default tokenizer.
2022-12-20 09:54:33 | INFO | absl | Using default tokenizer.
2022-12-20 09:55:02 | INFO | absl | Using default tokenizer.
2022-12-20 09:55:30 | INFO | absl | Using default tokenizer.
2022-12-20 09:56:04 | INFO | absl | Using default tokenizer.
2022-12-20 09:56:39 | INFO | absl | Using default tokenizer.
2022-12-20 09:57:10 | INFO | absl | Using default tokenizer.
2022-12-20 09:57:35 | INFO | absl | Using default tokenizer.
2022-12-20 09:58:00 | INFO | absl | Using default tokenizer.
2022-12-20 09:58:25 | INFO | absl | Using default tokenizer.
2022-12-20 09:58:58 | INFO | absl | Using default tokenizer.
2022-12-20 09:59:23 | INFO | absl | Using default tokenizer.
2022-12-20 09:59:49 | INFO | absl | Using default tokenizer.
2022-12-20 10:00:19 | INFO | absl | Using default tokenizer.
2022-12-20 10:00:49 | INFO | absl | Using default tokenizer.
2022-12-20 10:01:24 | INFO | absl | Using default tokenizer.
2022-12-20 10:01:54 | INFO | absl | Using default tokenizer.
2022-12-20 10:02:32 | INFO | absl | Using default tokenizer.
2022-12-20 10:03:03 | INFO | absl | Using default tokenizer.
2022-12-20 10:03:36 | INFO | absl | Using default tokenizer.
2022-12-20 10:04:19 | INFO | absl | Using default tokenizer.
2022-12-20 10:04:50 | INFO | absl | Using default tokenizer.
2022-12-20 10:05:23 | INFO | absl | Using default tokenizer.
2022-12-20 10:06:08 | INFO | absl | Using default tokenizer.
2022-12-20 10:06:40 | INFO | absl | Using default tokenizer.
2022-12-20 10:07:06 | INFO | absl | Using default tokenizer.
2022-12-20 10:07:37 | INFO | absl | Using default tokenizer.
2022-12-20 10:08:09 | INFO | absl | Using default tokenizer.
2022-12-20 10:08:45 | INFO | absl | Using default tokenizer.
2022-12-20 10:09:18 | INFO | absl | Using default tokenizer.
2022-12-20 10:09:51 | INFO | absl | Using default tokenizer.
2022-12-20 10:10:22 | INFO | absl | Using default tokenizer.
2022-12-20 10:10:58 | INFO | absl | Using default tokenizer.
2022-12-20 10:11:30 | INFO | absl | Using default tokenizer.
2022-12-20 10:12:08 | INFO | absl | Using default tokenizer.
2022-12-20 10:12:42 | INFO | absl | Using default tokenizer.
2022-12-20 10:13:27 | INFO | absl | Using default tokenizer.
2022-12-20 10:14:00 | INFO | absl | Using default tokenizer.
2022-12-20 10:14:40 | INFO | valid | {"epoch": 2, "valid_loss": "2.718", "valid_nll_loss": "2.718", "valid_rouge1": 0.4977840797787323, "valid_rouge2": 0.19118486235932058, "valid_rougel": 0.2724753409776423, "valid_rouge_avg": 0.3444844710690267, "valid_ppl": "6.58", "valid_wps": "62.6", "valid_wpb": "1907.8", "valid_bsz": "8", "valid_num_updates": "1541", "valid_best_rouge_avg": 0.3444844710690267}
2022-12-20 10:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1541 updates
2022-12-20 10:14:40 | INFO | fairseq.trainer | Saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 10:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 10:16:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints//graph_text/checkpoint_best.pt (epoch 2 @ 1541 updates, score 0.3444844710690267) (writing took 107.99822622910142 seconds)
2022-12-20 10:16:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-20 10:16:28 | INFO | train | {"epoch": 2, "train_loss": "2.665", "train_nll_loss": "2.665", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "6.34", "train_wps": "543.4", "train_ups": "0.07", "train_wpb": "7618.9", "train_bsz": "32", "train_num_updates": "1541", "train_lr": "8.90421e-05", "train_gnorm": "1.851", "train_clip": "100", "train_loss_scale": "0.5", "train_train_wall": "5415", "train_gb_free": "67.1", "train_wall": "21595"}
2022-12-20 10:16:28 | INFO | fairseq.trainer | begin training epoch 3
2022-12-20 10:16:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-20 10:18:02 | INFO | train_inner | {"epoch": 3, "update": 2.012, "loss": "2.417", "nll_loss": "2.417", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.34", "wps": "12.9", "ups": "0", "wpb": "7074.9", "bsz": "29.4", "num_updates": "1550", "lr": "8.89474e-05", "gnorm": "1.608", "clip": "100", "loss_scale": "0.5", "train_wall": "63", "gb_free": "63", "wall": "21689"}
2022-12-20 10:19:10 | INFO | train_inner | {"epoch": 3, "update": 2.025, "loss": "2.373", "nll_loss": "2.373", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.18", "wps": "1119.2", "ups": "0.15", "wpb": "7606.9", "bsz": "32", "num_updates": "1560", "lr": "8.88421e-05", "gnorm": "1.413", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.9", "wall": "21757"}
2022-12-20 10:20:22 | INFO | train_inner | {"epoch": 3, "update": 2.037, "loss": "2.311", "nll_loss": "2.311", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.96", "wps": "1072.7", "ups": "0.14", "wpb": "7707.8", "bsz": "32", "num_updates": "1570", "lr": "8.87368e-05", "gnorm": "1.33", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "21829"}
2022-12-20 10:21:31 | INFO | train_inner | {"epoch": 3, "update": 2.05, "loss": "2.311", "nll_loss": "2.311", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.96", "wps": "1102.3", "ups": "0.14", "wpb": "7657.4", "bsz": "32", "num_updates": "1580", "lr": "8.86316e-05", "gnorm": "1.379", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "21898"}
2022-12-20 10:22:40 | INFO | train_inner | {"epoch": 3, "update": 2.063, "loss": "2.378", "nll_loss": "2.378", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.2", "wps": "1078.1", "ups": "0.14", "wpb": "7437.8", "bsz": "32", "num_updates": "1590", "lr": "8.85263e-05", "gnorm": "1.396", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65", "wall": "21967"}
2022-12-20 10:23:49 | INFO | train_inner | {"epoch": 3, "update": 2.076, "loss": "2.356", "nll_loss": "2.356", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.12", "wps": "1110.7", "ups": "0.15", "wpb": "7643.3", "bsz": "32", "num_updates": "1600", "lr": "8.84211e-05", "gnorm": "1.37", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "22036"}
2022-12-20 10:24:58 | INFO | train_inner | {"epoch": 3, "update": 2.089, "loss": "2.405", "nll_loss": "2.405", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.3", "wps": "1118.1", "ups": "0.14", "wpb": "7744.5", "bsz": "32", "num_updates": "1610", "lr": "8.83158e-05", "gnorm": "1.36", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "64.3", "wall": "22106"}
2022-12-20 10:26:08 | INFO | train_inner | {"epoch": 3, "update": 2.102, "loss": "2.353", "nll_loss": "2.353", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.11", "wps": "1095.6", "ups": "0.14", "wpb": "7598.3", "bsz": "32", "num_updates": "1620", "lr": "8.82105e-05", "gnorm": "1.43", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "22175"}
2022-12-20 10:27:16 | INFO | train_inner | {"epoch": 3, "update": 2.115, "loss": "2.404", "nll_loss": "2.404", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1095.3", "ups": "0.15", "wpb": "7465.5", "bsz": "32", "num_updates": "1630", "lr": "8.81053e-05", "gnorm": "1.564", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "22243"}
2022-12-20 10:28:24 | INFO | train_inner | {"epoch": 3, "update": 2.128, "loss": "2.326", "nll_loss": "2.326", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.02", "wps": "1115.4", "ups": "0.15", "wpb": "7632.5", "bsz": "32", "num_updates": "1640", "lr": "8.8e-05", "gnorm": "1.363", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "22312"}
2022-12-20 10:29:34 | INFO | train_inner | {"epoch": 3, "update": 2.141, "loss": "2.415", "nll_loss": "2.415", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.33", "wps": "1085.8", "ups": "0.14", "wpb": "7534.2", "bsz": "32", "num_updates": "1650", "lr": "8.78947e-05", "gnorm": "1.383", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "22381"}
2022-12-20 10:30:43 | INFO | train_inner | {"epoch": 3, "update": 2.154, "loss": "2.329", "nll_loss": "2.329", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.02", "wps": "1126.6", "ups": "0.15", "wpb": "7757", "bsz": "32", "num_updates": "1660", "lr": "8.77895e-05", "gnorm": "1.403", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.9", "wall": "22450"}
2022-12-20 10:31:52 | INFO | train_inner | {"epoch": 3, "update": 2.166, "loss": "2.311", "nll_loss": "2.311", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.96", "wps": "1097.3", "ups": "0.15", "wpb": "7566.1", "bsz": "32", "num_updates": "1670", "lr": "8.76842e-05", "gnorm": "1.4", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "22519"}
2022-12-20 10:33:02 | INFO | train_inner | {"epoch": 3, "update": 2.179, "loss": "2.329", "nll_loss": "2.329", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.02", "wps": "1107.5", "ups": "0.14", "wpb": "7728.1", "bsz": "32", "num_updates": "1680", "lr": "8.75789e-05", "gnorm": "1.383", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "64.4", "wall": "22589"}
2022-12-20 10:34:12 | INFO | train_inner | {"epoch": 3, "update": 2.192, "loss": "2.337", "nll_loss": "2.337", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.05", "wps": "1078.7", "ups": "0.14", "wpb": "7578.7", "bsz": "32", "num_updates": "1690", "lr": "8.74737e-05", "gnorm": "1.386", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63", "wall": "22659"}
2022-12-20 10:35:22 | INFO | train_inner | {"epoch": 3, "update": 2.205, "loss": "2.404", "nll_loss": "2.404", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1080.7", "ups": "0.14", "wpb": "7633.9", "bsz": "31.9", "num_updates": "1700", "lr": "8.73684e-05", "gnorm": "1.593", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "22730"}
2022-12-20 10:36:32 | INFO | train_inner | {"epoch": 3, "update": 2.218, "loss": "2.404", "nll_loss": "2.404", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1110.2", "ups": "0.14", "wpb": "7742", "bsz": "32", "num_updates": "1710", "lr": "8.72632e-05", "gnorm": "1.389", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.9", "wall": "22799"}
2022-12-20 10:37:45 | INFO | train_inner | {"epoch": 3, "update": 2.231, "loss": "2.387", "nll_loss": "2.387", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.23", "wps": "1038.1", "ups": "0.14", "wpb": "7519.7", "bsz": "32", "num_updates": "1720", "lr": "8.71579e-05", "gnorm": "1.441", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "65", "wall": "22872"}
2022-12-20 10:38:52 | INFO | train_inner | {"epoch": 3, "update": 2.244, "loss": "2.409", "nll_loss": "2.409", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.31", "wps": "1105.9", "ups": "0.15", "wpb": "7407.7", "bsz": "32", "num_updates": "1730", "lr": "8.70526e-05", "gnorm": "1.425", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "62.9", "wall": "22939"}
2022-12-20 10:40:04 | INFO | train_inner | {"epoch": 3, "update": 2.257, "loss": "2.406", "nll_loss": "2.406", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.3", "wps": "1041.2", "ups": "0.14", "wpb": "7494.6", "bsz": "32", "num_updates": "1740", "lr": "8.69474e-05", "gnorm": "1.398", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63.6", "wall": "23011"}
2022-12-20 10:41:09 | INFO | train_inner | {"epoch": 3, "update": 2.27, "loss": "2.364", "nll_loss": "2.364", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.15", "wps": "1160.5", "ups": "0.15", "wpb": "7541.2", "bsz": "32", "num_updates": "1750", "lr": "8.68421e-05", "gnorm": "1.4", "clip": "100", "loss_scale": "0.5", "train_wall": "65", "gb_free": "62.2", "wall": "23076"}
2022-12-20 10:42:20 | INFO | train_inner | {"epoch": 3, "update": 2.283, "loss": "2.436", "nll_loss": "2.436", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.41", "wps": "1079.5", "ups": "0.14", "wpb": "7689.3", "bsz": "32", "num_updates": "1760", "lr": "8.67368e-05", "gnorm": "1.387", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "23147"}
2022-12-20 10:43:34 | INFO | train_inner | {"epoch": 3, "update": 2.295, "loss": "2.416", "nll_loss": "2.416", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.34", "wps": "1035.5", "ups": "0.14", "wpb": "7666.9", "bsz": "32", "num_updates": "1770", "lr": "8.66316e-05", "gnorm": "1.365", "clip": "100", "loss_scale": "0.5", "train_wall": "74", "gb_free": "62.2", "wall": "23221"}
2022-12-20 10:44:47 | INFO | train_inner | {"epoch": 3, "update": 2.308, "loss": "2.441", "nll_loss": "2.441", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.43", "wps": "1074.4", "ups": "0.14", "wpb": "7809", "bsz": "32", "num_updates": "1780", "lr": "8.65263e-05", "gnorm": "1.357", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "62.9", "wall": "23294"}
2022-12-20 10:45:56 | INFO | train_inner | {"epoch": 3, "update": 2.321, "loss": "2.431", "nll_loss": "2.431", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.39", "wps": "1087.1", "ups": "0.14", "wpb": "7582.3", "bsz": "32", "num_updates": "1790", "lr": "8.64211e-05", "gnorm": "1.381", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65", "wall": "23363"}
2022-12-20 10:47:07 | INFO | train_inner | {"epoch": 3, "update": 2.334, "loss": "2.382", "nll_loss": "2.382", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.21", "wps": "1085", "ups": "0.14", "wpb": "7710.1", "bsz": "32", "num_updates": "1800", "lr": "8.63158e-05", "gnorm": "1.385", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "23435"}
2022-12-20 10:48:19 | INFO | train_inner | {"epoch": 3, "update": 2.347, "loss": "2.498", "nll_loss": "2.498", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.65", "wps": "1092.4", "ups": "0.14", "wpb": "7861.1", "bsz": "32", "num_updates": "1810", "lr": "8.62105e-05", "gnorm": "4.546", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63.6", "wall": "23507"}
2022-12-20 10:49:29 | INFO | train_inner | {"epoch": 3, "update": 2.36, "loss": "2.377", "nll_loss": "2.377", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.19", "wps": "1078.6", "ups": "0.14", "wpb": "7499.4", "bsz": "32", "num_updates": "1820", "lr": "8.61053e-05", "gnorm": "1.473", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65.6", "wall": "23576"}
2022-12-20 10:50:40 | INFO | train_inner | {"epoch": 3, "update": 2.373, "loss": "2.442", "nll_loss": "2.442", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.44", "wps": "1088.4", "ups": "0.14", "wpb": "7771.2", "bsz": "32", "num_updates": "1830", "lr": "8.6e-05", "gnorm": "1.479", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.6", "wall": "23647"}
2022-12-20 10:51:53 | INFO | train_inner | {"epoch": 3, "update": 2.386, "loss": "2.454", "nll_loss": "2.454", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.48", "wps": "1039.8", "ups": "0.14", "wpb": "7552.4", "bsz": "32", "num_updates": "1840", "lr": "8.58947e-05", "gnorm": "1.404", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "64.3", "wall": "23720"}
2022-12-20 10:53:01 | INFO | train_inner | {"epoch": 3, "update": 2.399, "loss": "2.395", "nll_loss": "2.395", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.26", "wps": "1137", "ups": "0.15", "wpb": "7706.9", "bsz": "32", "num_updates": "1850", "lr": "8.57895e-05", "gnorm": "1.392", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "62.9", "wall": "23788"}
2022-12-20 10:54:12 | INFO | train_inner | {"epoch": 3, "update": 2.412, "loss": "2.334", "nll_loss": "2.334", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.04", "wps": "1069.5", "ups": "0.14", "wpb": "7654", "bsz": "32", "num_updates": "1860", "lr": "8.56842e-05", "gnorm": "1.413", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "65", "wall": "23860"}
2022-12-20 10:55:24 | INFO | train_inner | {"epoch": 3, "update": 2.425, "loss": "2.436", "nll_loss": "2.436", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.41", "wps": "1045.2", "ups": "0.14", "wpb": "7483.8", "bsz": "32", "num_updates": "1870", "lr": "8.55789e-05", "gnorm": "1.404", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.6", "wall": "23931"}
2022-12-20 10:56:35 | INFO | train_inner | {"epoch": 3, "update": 2.437, "loss": "2.401", "nll_loss": "2.401", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.28", "wps": "1064", "ups": "0.14", "wpb": "7597.7", "bsz": "32", "num_updates": "1880", "lr": "8.54737e-05", "gnorm": "1.506", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "24003"}
2022-12-20 10:57:46 | INFO | train_inner | {"epoch": 3, "update": 2.45, "loss": "2.392", "nll_loss": "2.392", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.25", "wps": "1096.7", "ups": "0.14", "wpb": "7744.5", "bsz": "32", "num_updates": "1890", "lr": "8.53684e-05", "gnorm": "1.328", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "24073"}
2022-12-20 10:58:58 | INFO | train_inner | {"epoch": 3, "update": 2.463, "loss": "2.403", "nll_loss": "2.403", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1053.2", "ups": "0.14", "wpb": "7586.2", "bsz": "32", "num_updates": "1900", "lr": "8.52632e-05", "gnorm": "1.377", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63.6", "wall": "24145"}
2022-12-20 11:00:10 | INFO | train_inner | {"epoch": 3, "update": 2.476, "loss": "2.372", "nll_loss": "2.372", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.18", "wps": "1083", "ups": "0.14", "wpb": "7760.5", "bsz": "32", "num_updates": "1910", "lr": "8.51579e-05", "gnorm": "1.307", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "24217"}
2022-12-20 11:01:21 | INFO | train_inner | {"epoch": 3, "update": 2.489, "loss": "2.41", "nll_loss": "2.41", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.31", "wps": "1054.3", "ups": "0.14", "wpb": "7489.2", "bsz": "32", "num_updates": "1920", "lr": "8.50526e-05", "gnorm": "1.484", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "24288"}
2022-12-20 11:02:32 | INFO | train_inner | {"epoch": 3, "update": 2.502, "loss": "2.386", "nll_loss": "2.386", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.23", "wps": "1048.5", "ups": "0.14", "wpb": "7447.9", "bsz": "32", "num_updates": "1930", "lr": "8.49474e-05", "gnorm": "1.342", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "65.6", "wall": "24359"}
2022-12-20 11:03:40 | INFO | train_inner | {"epoch": 3, "update": 2.515, "loss": "2.36", "nll_loss": "2.36", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.13", "wps": "1103.2", "ups": "0.15", "wpb": "7558.5", "bsz": "32", "num_updates": "1940", "lr": "8.48421e-05", "gnorm": "1.425", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "24428"}
2022-12-20 11:04:51 | INFO | train_inner | {"epoch": 3, "update": 2.528, "loss": "2.34", "nll_loss": "2.34", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.06", "wps": "1083.8", "ups": "0.14", "wpb": "7682.9", "bsz": "32", "num_updates": "1950", "lr": "8.47368e-05", "gnorm": "1.452", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "24498"}
2022-12-20 11:06:01 | INFO | train_inner | {"epoch": 3, "update": 2.541, "loss": "2.414", "nll_loss": "2.414", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.33", "wps": "1092.5", "ups": "0.14", "wpb": "7614.6", "bsz": "32", "num_updates": "1960", "lr": "8.46316e-05", "gnorm": "1.344", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "63.6", "wall": "24568"}
2022-12-20 11:07:09 | INFO | train_inner | {"epoch": 3, "update": 2.554, "loss": "2.458", "nll_loss": "2.458", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.49", "wps": "1135", "ups": "0.15", "wpb": "7741.3", "bsz": "32", "num_updates": "1970", "lr": "8.45263e-05", "gnorm": "1.341", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "24636"}
2022-12-20 11:08:22 | INFO | train_inner | {"epoch": 3, "update": 2.566, "loss": "2.455", "nll_loss": "2.455", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.48", "wps": "1038.9", "ups": "0.14", "wpb": "7522.3", "bsz": "32", "num_updates": "1980", "lr": "8.44211e-05", "gnorm": "1.356", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63.7", "wall": "24709"}
2022-12-20 11:09:35 | INFO | train_inner | {"epoch": 3, "update": 2.579, "loss": "2.445", "nll_loss": "2.445", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.45", "wps": "1021.8", "ups": "0.14", "wpb": "7486.8", "bsz": "32", "num_updates": "1990", "lr": "8.43158e-05", "gnorm": "1.401", "clip": "100", "loss_scale": "0.5", "train_wall": "73", "gb_free": "62.9", "wall": "24782"}
2022-12-20 11:10:45 | INFO | train_inner | {"epoch": 3, "update": 2.592, "loss": "2.414", "nll_loss": "2.414", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.33", "wps": "1075", "ups": "0.14", "wpb": "7465.2", "bsz": "32", "num_updates": "2000", "lr": "8.42105e-05", "gnorm": "1.353", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "63.6", "wall": "24852"}
2022-12-20 11:11:56 | INFO | train_inner | {"epoch": 3, "update": 2.605, "loss": "2.482", "nll_loss": "2.482", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.59", "wps": "1056.6", "ups": "0.14", "wpb": "7537.3", "bsz": "32", "num_updates": "2010", "lr": "8.41053e-05", "gnorm": "1.379", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "24923"}
2022-12-20 11:13:07 | INFO | train_inner | {"epoch": 3, "update": 2.618, "loss": "2.439", "nll_loss": "2.439", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.42", "wps": "1077.4", "ups": "0.14", "wpb": "7609", "bsz": "32", "num_updates": "2020", "lr": "8.4e-05", "gnorm": "1.388", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "24994"}
2022-12-20 11:14:17 | INFO | train_inner | {"epoch": 3, "update": 2.631, "loss": "2.382", "nll_loss": "2.382", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.21", "wps": "1095", "ups": "0.14", "wpb": "7660.7", "bsz": "32", "num_updates": "2030", "lr": "8.38947e-05", "gnorm": "2.028", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "25064"}
2022-12-20 11:15:28 | INFO | train_inner | {"epoch": 3, "update": 2.644, "loss": "2.402", "nll_loss": "2.402", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1087.1", "ups": "0.14", "wpb": "7769.9", "bsz": "32", "num_updates": "2040", "lr": "8.37895e-05", "gnorm": "1.414", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "25135"}
2022-12-20 11:16:37 | INFO | train_inner | {"epoch": 3, "update": 2.657, "loss": "2.368", "nll_loss": "2.368", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.16", "wps": "1091.3", "ups": "0.15", "wpb": "7505.9", "bsz": "32", "num_updates": "2050", "lr": "8.36842e-05", "gnorm": "1.326", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "64.3", "wall": "25204"}
2022-12-20 11:17:47 | INFO | train_inner | {"epoch": 3, "update": 2.67, "loss": "2.4", "nll_loss": "2.4", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.28", "wps": "1111.1", "ups": "0.14", "wpb": "7791.7", "bsz": "32", "num_updates": "2060", "lr": "8.35789e-05", "gnorm": "1.335", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.9", "wall": "25274"}
2022-12-20 11:18:56 | INFO | train_inner | {"epoch": 3, "update": 2.683, "loss": "2.412", "nll_loss": "2.412", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.32", "wps": "1081", "ups": "0.14", "wpb": "7487.5", "bsz": "32", "num_updates": "2070", "lr": "8.34737e-05", "gnorm": "1.405", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "25343"}
2022-12-20 11:20:07 | INFO | train_inner | {"epoch": 3, "update": 2.695, "loss": "2.47", "nll_loss": "2.47", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.54", "wps": "1085.6", "ups": "0.14", "wpb": "7618.1", "bsz": "32", "num_updates": "2080", "lr": "8.33684e-05", "gnorm": "1.409", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "25414"}
2022-12-20 11:21:19 | INFO | train_inner | {"epoch": 3, "update": 2.708, "loss": "2.49", "nll_loss": "2.49", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.62", "wps": "1072", "ups": "0.14", "wpb": "7726", "bsz": "32", "num_updates": "2090", "lr": "8.32632e-05", "gnorm": "1.385", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "62.2", "wall": "25486"}
2022-12-20 11:22:26 | INFO | train_inner | {"epoch": 3, "update": 2.721, "loss": "2.456", "nll_loss": "2.456", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.49", "wps": "1126.2", "ups": "0.15", "wpb": "7543.8", "bsz": "32", "num_updates": "2100", "lr": "8.31579e-05", "gnorm": "1.934", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "65.1", "wall": "25553"}
2022-12-20 11:23:37 | INFO | train_inner | {"epoch": 3, "update": 2.734, "loss": "2.479", "nll_loss": "2.479", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.57", "wps": "1048.6", "ups": "0.14", "wpb": "7501.7", "bsz": "32", "num_updates": "2110", "lr": "8.30526e-05", "gnorm": "2.229", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "63.6", "wall": "25624"}
2022-12-20 11:24:45 | INFO | train_inner | {"epoch": 3, "update": 2.747, "loss": "2.447", "nll_loss": "2.447", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.45", "wps": "1110.2", "ups": "0.15", "wpb": "7550.8", "bsz": "32", "num_updates": "2120", "lr": "8.29474e-05", "gnorm": "3.209", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "64.3", "wall": "25692"}
2022-12-20 11:25:56 | INFO | train_inner | {"epoch": 3, "update": 2.76, "loss": "2.457", "nll_loss": "2.457", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.49", "wps": "1067.6", "ups": "0.14", "wpb": "7597.1", "bsz": "32", "num_updates": "2130", "lr": "8.28421e-05", "gnorm": "2.556", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "65", "wall": "25763"}
2022-12-20 11:27:07 | INFO | train_inner | {"epoch": 3, "update": 2.773, "loss": "2.427", "nll_loss": "2.427", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.38", "wps": "1072.4", "ups": "0.14", "wpb": "7603.6", "bsz": "32", "num_updates": "2140", "lr": "8.27368e-05", "gnorm": "2.252", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "62.2", "wall": "25834"}
2022-12-20 11:28:17 | INFO | train_inner | {"epoch": 3, "update": 2.786, "loss": "2.432", "nll_loss": "2.432", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.4", "wps": "1073.2", "ups": "0.14", "wpb": "7492.7", "bsz": "32", "num_updates": "2150", "lr": "8.26316e-05", "gnorm": "2.146", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "64.3", "wall": "25904"}
2022-12-20 11:29:27 | INFO | train_inner | {"epoch": 3, "update": 2.799, "loss": "2.42", "nll_loss": "2.42", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.35", "wps": "1094.9", "ups": "0.14", "wpb": "7678.5", "bsz": "32", "num_updates": "2160", "lr": "8.25263e-05", "gnorm": "2.115", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.2", "wall": "25974"}
2022-12-20 11:30:37 | INFO | train_inner | {"epoch": 3, "update": 2.812, "loss": "2.432", "nll_loss": "2.432", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.4", "wps": "1106.4", "ups": "0.14", "wpb": "7762", "bsz": "32", "num_updates": "2170", "lr": "8.24211e-05", "gnorm": "2.956", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.2", "wall": "26045"}
2022-12-20 11:31:46 | INFO | train_inner | {"epoch": 3, "update": 2.825, "loss": "2.46", "nll_loss": "2.46", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.5", "wps": "1111.2", "ups": "0.15", "wpb": "7624.7", "bsz": "32", "num_updates": "2180", "lr": "8.23158e-05", "gnorm": "2.434", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "62.9", "wall": "26113"}
2022-12-20 11:32:57 | INFO | train_inner | {"epoch": 3, "update": 2.837, "loss": "2.459", "nll_loss": "2.459", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.5", "wps": "1079.3", "ups": "0.14", "wpb": "7692.8", "bsz": "32", "num_updates": "2190", "lr": "8.22105e-05", "gnorm": "1.475", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "65", "wall": "26185"}
2022-12-20 11:34:07 | INFO | train_inner | {"epoch": 3, "update": 2.85, "loss": "2.484", "nll_loss": "2.484", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.6", "wps": "1101.5", "ups": "0.14", "wpb": "7648.3", "bsz": "32", "num_updates": "2200", "lr": "8.21053e-05", "gnorm": "1.996", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "63.6", "wall": "26254"}
2022-12-20 11:35:19 | INFO | train_inner | {"epoch": 3, "update": 2.863, "loss": "2.468", "nll_loss": "2.468", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.53", "wps": "1071.9", "ups": "0.14", "wpb": "7687.1", "bsz": "32", "num_updates": "2210", "lr": "8.2e-05", "gnorm": "2.358", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "63.6", "wall": "26326"}
2022-12-20 11:36:26 | INFO | train_inner | {"epoch": 3, "update": 2.876, "loss": "2.379", "nll_loss": "2.379", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.2", "wps": "1139.5", "ups": "0.15", "wpb": "7653.6", "bsz": "32", "num_updates": "2220", "lr": "8.18947e-05", "gnorm": "3.023", "clip": "100", "loss_scale": "1", "train_wall": "67", "gb_free": "63.6", "wall": "26393"}
2022-12-20 11:37:33 | INFO | train_inner | {"epoch": 3, "update": 2.889, "loss": "2.48", "nll_loss": "2.48", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.58", "wps": "1152.9", "ups": "0.15", "wpb": "7736.5", "bsz": "32", "num_updates": "2230", "lr": "8.17895e-05", "gnorm": "5.57", "clip": "100", "loss_scale": "1", "train_wall": "67", "gb_free": "63.6", "wall": "26460"}
2022-12-20 11:38:40 | INFO | train_inner | {"epoch": 3, "update": 2.902, "loss": "2.541", "nll_loss": "2.541", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.82", "wps": "1159.3", "ups": "0.15", "wpb": "7723.8", "bsz": "32", "num_updates": "2240", "lr": "8.16842e-05", "gnorm": "12.04", "clip": "100", "loss_scale": "1", "train_wall": "66", "gb_free": "64.3", "wall": "26527"}
2022-12-20 11:39:51 | INFO | train_inner | {"epoch": 3, "update": 2.915, "loss": "2.46", "nll_loss": "2.46", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.5", "wps": "1078", "ups": "0.14", "wpb": "7713", "bsz": "32", "num_updates": "2250", "lr": "8.15789e-05", "gnorm": "6.961", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "64.3", "wall": "26598"}
2022-12-20 11:41:04 | INFO | train_inner | {"epoch": 3, "update": 2.928, "loss": "2.365", "nll_loss": "2.365", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.15", "wps": "1028.9", "ups": "0.14", "wpb": "7498.8", "bsz": "32", "num_updates": "2260", "lr": "8.14737e-05", "gnorm": "2.359", "clip": "100", "loss_scale": "1", "train_wall": "73", "gb_free": "64.3", "wall": "26671"}
2022-12-20 11:42:13 | INFO | train_inner | {"epoch": 3, "update": 2.941, "loss": "2.448", "nll_loss": "2.448", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.46", "wps": "1104.3", "ups": "0.14", "wpb": "7637.8", "bsz": "32", "num_updates": "2270", "lr": "8.13684e-05", "gnorm": "1.709", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "62.9", "wall": "26740"}
2022-12-20 11:43:25 | INFO | train_inner | {"epoch": 3, "update": 2.954, "loss": "2.444", "nll_loss": "2.444", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.44", "wps": "1083.7", "ups": "0.14", "wpb": "7781.1", "bsz": "32", "num_updates": "2280", "lr": "8.12632e-05", "gnorm": "2.432", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "65.6", "wall": "26812"}
2022-12-20 11:44:35 | INFO | train_inner | {"epoch": 3, "update": 2.966, "loss": "2.415", "nll_loss": "2.415", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.33", "wps": "1096.8", "ups": "0.14", "wpb": "7665.7", "bsz": "32", "num_updates": "2290", "lr": "8.11579e-05", "gnorm": "2.847", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.9", "wall": "26882"}
2022-12-20 11:45:44 | INFO | train_inner | {"epoch": 3, "update": 2.979, "loss": "2.414", "nll_loss": "2.414", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.33", "wps": "1127.1", "ups": "0.15", "wpb": "7753.6", "bsz": "32", "num_updates": "2300", "lr": "8.10526e-05", "gnorm": "4.545", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "63.6", "wall": "26951"}
2022-12-20 11:46:55 | INFO | train_inner | {"epoch": 3, "update": 2.992, "loss": "2.379", "nll_loss": "2.379", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.2", "wps": "1079.6", "ups": "0.14", "wpb": "7647.1", "bsz": "32", "num_updates": "2310", "lr": "8.09474e-05", "gnorm": "2.681", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "64.3", "wall": "27022"}
2022-12-20 11:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-20 11:48:22 | INFO | absl | Using default tokenizer.
2022-12-20 11:48:42 | INFO | absl | Using default tokenizer.
2022-12-20 11:49:02 | INFO | absl | Using default tokenizer.
2022-12-20 11:49:20 | INFO | absl | Using default tokenizer.
2022-12-20 11:49:40 | INFO | absl | Using default tokenizer.
2022-12-20 11:50:10 | INFO | absl | Using default tokenizer.
2022-12-20 11:50:44 | INFO | absl | Using default tokenizer.
2022-12-20 11:51:06 | INFO | absl | Using default tokenizer.
2022-12-20 11:51:36 | INFO | absl | Using default tokenizer.
2022-12-20 11:52:02 | INFO | absl | Using default tokenizer.
2022-12-20 11:52:25 | INFO | absl | Using default tokenizer.
2022-12-20 11:52:58 | INFO | absl | Using default tokenizer.
2022-12-20 11:53:25 | INFO | absl | Using default tokenizer.
2022-12-20 11:53:58 | INFO | absl | Using default tokenizer.
2022-12-20 11:54:20 | INFO | absl | Using default tokenizer.
2022-12-20 11:54:44 | INFO | absl | Using default tokenizer.
2022-12-20 11:55:07 | INFO | absl | Using default tokenizer.
2022-12-20 11:55:32 | INFO | absl | Using default tokenizer.
2022-12-20 11:56:05 | INFO | absl | Using default tokenizer.
2022-12-20 11:56:25 | INFO | absl | Using default tokenizer.
2022-12-20 11:56:56 | INFO | absl | Using default tokenizer.
2022-12-20 11:57:18 | INFO | absl | Using default tokenizer.
2022-12-20 11:57:56 | INFO | absl | Using default tokenizer.
2022-12-20 11:58:27 | INFO | absl | Using default tokenizer.
2022-12-20 11:58:53 | INFO | absl | Using default tokenizer.
2022-12-20 11:59:16 | INFO | absl | Using default tokenizer.
2022-12-20 11:59:39 | INFO | absl | Using default tokenizer.
2022-12-20 12:00:10 | INFO | absl | Using default tokenizer.
2022-12-20 12:00:34 | INFO | absl | Using default tokenizer.
2022-12-20 12:00:59 | INFO | absl | Using default tokenizer.
2022-12-20 12:01:25 | INFO | absl | Using default tokenizer.
2022-12-20 12:01:50 | INFO | absl | Using default tokenizer.
2022-12-20 12:02:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:02:39 | INFO | absl | Using default tokenizer.
2022-12-20 12:03:04 | INFO | absl | Using default tokenizer.
2022-12-20 12:03:31 | INFO | absl | Using default tokenizer.
2022-12-20 12:03:54 | INFO | absl | Using default tokenizer.
2022-12-20 12:04:20 | INFO | absl | Using default tokenizer.
2022-12-20 12:04:59 | INFO | absl | Using default tokenizer.
2022-12-20 12:05:28 | INFO | absl | Using default tokenizer.
2022-12-20 12:05:55 | INFO | absl | Using default tokenizer.
2022-12-20 12:06:19 | INFO | absl | Using default tokenizer.
2022-12-20 12:06:41 | INFO | absl | Using default tokenizer.
2022-12-20 12:07:05 | INFO | absl | Using default tokenizer.
2022-12-20 12:07:35 | INFO | absl | Using default tokenizer.
2022-12-20 12:08:03 | INFO | absl | Using default tokenizer.
2022-12-20 12:08:24 | INFO | absl | Using default tokenizer.
2022-12-20 12:08:53 | INFO | absl | Using default tokenizer.
2022-12-20 12:09:26 | INFO | absl | Using default tokenizer.
2022-12-20 12:09:51 | INFO | absl | Using default tokenizer.
2022-12-20 12:10:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:10:38 | INFO | absl | Using default tokenizer.
2022-12-20 12:11:01 | INFO | absl | Using default tokenizer.
2022-12-20 12:11:30 | INFO | absl | Using default tokenizer.
2022-12-20 12:11:59 | INFO | absl | Using default tokenizer.
2022-12-20 12:12:28 | INFO | absl | Using default tokenizer.
2022-12-20 12:12:52 | INFO | absl | Using default tokenizer.
2022-12-20 12:13:28 | INFO | absl | Using default tokenizer.
2022-12-20 12:13:59 | INFO | absl | Using default tokenizer.
2022-12-20 12:14:23 | INFO | absl | Using default tokenizer.
2022-12-20 12:14:53 | INFO | absl | Using default tokenizer.
2022-12-20 12:15:26 | INFO | absl | Using default tokenizer.
2022-12-20 12:15:56 | INFO | absl | Using default tokenizer.
2022-12-20 12:16:20 | INFO | absl | Using default tokenizer.
2022-12-20 12:16:49 | INFO | absl | Using default tokenizer.
2022-12-20 12:17:15 | INFO | absl | Using default tokenizer.
2022-12-20 12:17:38 | INFO | absl | Using default tokenizer.
2022-12-20 12:18:07 | INFO | absl | Using default tokenizer.
2022-12-20 12:18:36 | INFO | absl | Using default tokenizer.
2022-12-20 12:19:01 | INFO | absl | Using default tokenizer.
2022-12-20 12:19:26 | INFO | absl | Using default tokenizer.
2022-12-20 12:19:54 | INFO | absl | Using default tokenizer.
2022-12-20 12:20:20 | INFO | absl | Using default tokenizer.
2022-12-20 12:20:43 | INFO | absl | Using default tokenizer.
2022-12-20 12:21:11 | INFO | absl | Using default tokenizer.
2022-12-20 12:21:36 | INFO | absl | Using default tokenizer.
2022-12-20 12:22:01 | INFO | absl | Using default tokenizer.
2022-12-20 12:22:29 | INFO | absl | Using default tokenizer.
2022-12-20 12:22:51 | INFO | absl | Using default tokenizer.
2022-12-20 12:23:24 | INFO | absl | Using default tokenizer.
2022-12-20 12:23:50 | INFO | absl | Using default tokenizer.
2022-12-20 12:24:23 | INFO | absl | Using default tokenizer.
2022-12-20 12:24:47 | INFO | absl | Using default tokenizer.
2022-12-20 12:25:19 | INFO | absl | Using default tokenizer.
2022-12-20 12:25:50 | INFO | absl | Using default tokenizer.
2022-12-20 12:26:12 | INFO | absl | Using default tokenizer.
2022-12-20 12:26:36 | INFO | absl | Using default tokenizer.
2022-12-20 12:27:08 | INFO | absl | Using default tokenizer.
2022-12-20 12:27:39 | INFO | absl | Using default tokenizer.
2022-12-20 12:28:05 | INFO | absl | Using default tokenizer.
2022-12-20 12:28:34 | INFO | absl | Using default tokenizer.
2022-12-20 12:29:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:29:36 | INFO | absl | Using default tokenizer.
2022-12-20 12:30:06 | INFO | absl | Using default tokenizer.
2022-12-20 12:30:31 | INFO | absl | Using default tokenizer.
2022-12-20 12:31:01 | INFO | absl | Using default tokenizer.
2022-12-20 12:31:28 | INFO | absl | Using default tokenizer.
2022-12-20 12:31:57 | INFO | absl | Using default tokenizer.
2022-12-20 12:32:24 | INFO | absl | Using default tokenizer.
2022-12-20 12:32:53 | INFO | absl | Using default tokenizer.
2022-12-20 12:33:28 | INFO | absl | Using default tokenizer.
2022-12-20 12:33:56 | INFO | absl | Using default tokenizer.
2022-12-20 12:34:32 | INFO | absl | Using default tokenizer.
2022-12-20 12:35:00 | INFO | absl | Using default tokenizer.
2022-12-20 12:35:33 | INFO | absl | Using default tokenizer.
2022-12-20 12:36:08 | INFO | absl | Using default tokenizer.
2022-12-20 12:36:41 | INFO | absl | Using default tokenizer.
2022-12-20 12:37:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:37:40 | INFO | absl | Using default tokenizer.
2022-12-20 12:38:09 | INFO | absl | Using default tokenizer.
2022-12-20 12:38:43 | INFO | absl | Using default tokenizer.
2022-12-20 12:39:12 | INFO | absl | Using default tokenizer.
2022-12-20 12:39:40 | INFO | absl | Using default tokenizer.
2022-12-20 12:40:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:40:37 | INFO | absl | Using default tokenizer.
2022-12-20 12:41:09 | INFO | absl | Using default tokenizer.
2022-12-20 12:41:37 | INFO | absl | Using default tokenizer.
2022-12-20 12:42:09 | INFO | absl | Using default tokenizer.
2022-12-20 12:42:40 | INFO | absl | Using default tokenizer.
2022-12-20 12:43:06 | INFO | absl | Using default tokenizer.
2022-12-20 12:43:33 | INFO | absl | Using default tokenizer.
2022-12-20 12:43:58 | INFO | absl | Using default tokenizer.
2022-12-20 12:44:24 | INFO | absl | Using default tokenizer.
2022-12-20 12:44:59 | INFO | absl | Using default tokenizer.
2022-12-20 12:45:25 | INFO | absl | Using default tokenizer.
2022-12-20 12:45:54 | INFO | absl | Using default tokenizer.
2022-12-20 12:46:31 | INFO | absl | Using default tokenizer.
2022-12-20 12:47:00 | INFO | absl | Using default tokenizer.
2022-12-20 12:47:34 | INFO | absl | Using default tokenizer.
2022-12-20 12:48:02 | INFO | absl | Using default tokenizer.
2022-12-20 12:48:38 | INFO | absl | Using default tokenizer.
2022-12-20 12:49:09 | INFO | absl | Using default tokenizer.
2022-12-20 12:49:43 | INFO | absl | Using default tokenizer.
2022-12-20 12:50:12 | INFO | absl | Using default tokenizer.
2022-12-20 12:50:42 | INFO | absl | Using default tokenizer.
2022-12-20 12:51:13 | INFO | absl | Using default tokenizer.
2022-12-20 12:51:45 | INFO | absl | Using default tokenizer.
2022-12-20 12:52:13 | INFO | absl | Using default tokenizer.
2022-12-20 12:52:45 | INFO | absl | Using default tokenizer.
2022-12-20 12:53:12 | INFO | absl | Using default tokenizer.
2022-12-20 12:53:39 | INFO | absl | Using default tokenizer.
2022-12-20 12:54:14 | INFO | absl | Using default tokenizer.
2022-12-20 12:54:42 | INFO | absl | Using default tokenizer.
2022-12-20 12:55:07 | INFO | absl | Using default tokenizer.
2022-12-20 12:55:37 | INFO | absl | Using default tokenizer.
2022-12-20 12:56:04 | INFO | absl | Using default tokenizer.
2022-12-20 12:56:36 | INFO | absl | Using default tokenizer.
2022-12-20 12:57:04 | INFO | absl | Using default tokenizer.
2022-12-20 12:57:33 | INFO | absl | Using default tokenizer.
2022-12-20 12:58:03 | INFO | absl | Using default tokenizer.
2022-12-20 12:58:35 | INFO | absl | Using default tokenizer.
2022-12-20 12:59:15 | INFO | absl | Using default tokenizer.
2022-12-20 12:59:48 | INFO | absl | Using default tokenizer.
2022-12-20 13:00:20 | INFO | absl | Using default tokenizer.
2022-12-20 13:00:59 | INFO | absl | Using default tokenizer.
2022-12-20 13:01:32 | INFO | absl | Using default tokenizer.
2022-12-20 13:02:04 | INFO | absl | Using default tokenizer.
2022-12-20 13:02:35 | INFO | absl | Using default tokenizer.
2022-12-20 13:03:03 | INFO | absl | Using default tokenizer.
2022-12-20 13:03:26 | INFO | absl | Using default tokenizer.
2022-12-20 13:03:56 | INFO | absl | Using default tokenizer.
2022-12-20 13:04:24 | INFO | absl | Using default tokenizer.
2022-12-20 13:04:56 | INFO | absl | Using default tokenizer.
2022-12-20 13:05:27 | INFO | absl | Using default tokenizer.
2022-12-20 13:05:56 | INFO | absl | Using default tokenizer.
2022-12-20 13:06:25 | INFO | absl | Using default tokenizer.
2022-12-20 13:07:05 | INFO | absl | Using default tokenizer.
2022-12-20 13:07:37 | INFO | absl | Using default tokenizer.
2022-12-20 13:08:13 | INFO | absl | Using default tokenizer.
2022-12-20 13:08:45 | INFO | absl | Using default tokenizer.
2022-12-20 13:09:30 | INFO | absl | Using default tokenizer.
2022-12-20 13:10:02 | INFO | absl | Using default tokenizer.
2022-12-20 13:10:43 | INFO | valid | {"epoch": 3, "valid_loss": "2.731", "valid_nll_loss": "2.731", "valid_rouge1": 0.5034512536410294, "valid_rouge2": 0.19427975101286826, "valid_rougel": 0.2769212682314047, "valid_rouge_avg": 0.348865502326949, "valid_ppl": "6.64", "valid_wps": "66.2", "valid_wpb": "1907.8", "valid_bsz": "8", "valid_num_updates": "2316", "valid_best_rouge_avg": 0.348865502326949}
2022-12-20 13:10:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2316 updates
2022-12-20 13:10:43 | INFO | fairseq.trainer | Saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 13:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints//graph_text/checkpoint_best.pt
2022-12-20 13:11:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints//graph_text/checkpoint_best.pt (epoch 3 @ 2316 updates, score 0.348865502326949) (writing took 56.13114207005128 seconds)
2022-12-20 13:11:40 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-20 13:11:40 | INFO | train | {"epoch": 3, "train_loss": "2.411", "train_nll_loss": "2.411", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "5.32", "train_wps": "561.7", "train_ups": "0.07", "train_wpb": "7618.9", "train_bsz": "32", "train_num_updates": "2316", "train_lr": "8.08842e-05", "train_gnorm": "2.044", "train_clip": "100", "train_loss_scale": "1", "train_train_wall": "5411", "train_gb_free": "63.7", "train_wall": "32107"}
2022-12-20 13:11:40 | INFO | fairseq.trainer | begin training epoch 4
2022-12-20 13:11:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-20 13:12:37 | INFO | train_inner | {"epoch": 4, "update": 3.005, "loss": "2.339", "nll_loss": "2.339", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.06", "wps": "13.6", "ups": "0", "wpb": "6999.2", "bsz": "29.4", "num_updates": "2320", "lr": "8.08421e-05", "gnorm": "7.113", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "62.9", "wall": "32164"}
2022-12-20 13:13:48 | INFO | train_inner | {"epoch": 4, "update": 3.018, "loss": "2.15", "nll_loss": "2.15", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.44", "wps": "1060.8", "ups": "0.14", "wpb": "7464.6", "bsz": "32", "num_updates": "2330", "lr": "8.07368e-05", "gnorm": "3.194", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "63.6", "wall": "32235"}
2022-12-20 13:14:57 | INFO | train_inner | {"epoch": 4, "update": 3.031, "loss": "2.188", "nll_loss": "2.188", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.56", "wps": "1061.1", "ups": "0.14", "wpb": "7377.4", "bsz": "32", "num_updates": "2340", "lr": "8.06316e-05", "gnorm": "2.134", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "62.2", "wall": "32304"}
2022-12-20 13:16:08 | INFO | train_inner | {"epoch": 4, "update": 3.044, "loss": "2.138", "nll_loss": "2.138", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.4", "wps": "1094.5", "ups": "0.14", "wpb": "7707.8", "bsz": "32", "num_updates": "2350", "lr": "8.05263e-05", "gnorm": "2.121", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.2", "wall": "32375"}
2022-12-20 13:17:16 | INFO | train_inner | {"epoch": 4, "update": 3.057, "loss": "2.2", "nll_loss": "2.2", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.6", "wps": "1093.5", "ups": "0.15", "wpb": "7505.7", "bsz": "32", "num_updates": "2360", "lr": "8.04211e-05", "gnorm": "2.486", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "62.2", "wall": "32443"}
2022-12-20 13:18:26 | INFO | train_inner | {"epoch": 4, "update": 3.07, "loss": "2.192", "nll_loss": "2.192", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.57", "wps": "1088.2", "ups": "0.14", "wpb": "7620.9", "bsz": "32", "num_updates": "2370", "lr": "8.03158e-05", "gnorm": "1.971", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "65", "wall": "32514"}
2022-12-20 13:19:35 | INFO | train_inner | {"epoch": 4, "update": 3.083, "loss": "2.212", "nll_loss": "2.212", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.63", "wps": "1088.1", "ups": "0.15", "wpb": "7433.8", "bsz": "32", "num_updates": "2380", "lr": "8.02105e-05", "gnorm": "1.861", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "65", "wall": "32582"}
2022-12-20 13:20:42 | INFO | train_inner | {"epoch": 4, "update": 3.095, "loss": "2.147", "nll_loss": "2.147", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.43", "wps": "1152.4", "ups": "0.15", "wpb": "7774.5", "bsz": "32", "num_updates": "2390", "lr": "8.01053e-05", "gnorm": "1.725", "clip": "100", "loss_scale": "1", "train_wall": "67", "gb_free": "62.2", "wall": "32649"}
2022-12-20 13:21:51 | INFO | train_inner | {"epoch": 4, "update": 3.108, "loss": "2.207", "nll_loss": "2.207", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.62", "wps": "1083.6", "ups": "0.15", "wpb": "7410.2", "bsz": "32", "num_updates": "2400", "lr": "8e-05", "gnorm": "1.55", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "62.2", "wall": "32718"}
2022-12-20 13:23:01 | INFO | train_inner | {"epoch": 4, "update": 3.121, "loss": "2.255", "nll_loss": "2.255", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.77", "wps": "1099.2", "ups": "0.14", "wpb": "7676.6", "bsz": "32", "num_updates": "2410", "lr": "7.98947e-05", "gnorm": "1.889", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "63.6", "wall": "32788"}
2022-12-20 13:24:12 | INFO | train_inner | {"epoch": 4, "update": 3.134, "loss": "2.198", "nll_loss": "2.198", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.59", "wps": "1082.1", "ups": "0.14", "wpb": "7716", "bsz": "32", "num_updates": "2420", "lr": "7.97895e-05", "gnorm": "1.939", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "62.2", "wall": "32859"}
2022-12-20 13:25:19 | INFO | train_inner | {"epoch": 4, "update": 3.147, "loss": "2.164", "nll_loss": "2.164", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.48", "wps": "1146.4", "ups": "0.15", "wpb": "7653.8", "bsz": "32", "num_updates": "2430", "lr": "7.96842e-05", "gnorm": "1.626", "clip": "100", "loss_scale": "1", "train_wall": "66", "gb_free": "65", "wall": "32926"}
2022-12-20 13:26:29 | INFO | train_inner | {"epoch": 4, "update": 3.16, "loss": "2.21", "nll_loss": "2.21", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.63", "wps": "1061.3", "ups": "0.14", "wpb": "7442.9", "bsz": "32", "num_updates": "2440", "lr": "7.95789e-05", "gnorm": "1.623", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.2", "wall": "32996"}
2022-12-20 13:27:40 | INFO | train_inner | {"epoch": 4, "update": 3.173, "loss": "2.164", "nll_loss": "2.164", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.48", "wps": "1072.7", "ups": "0.14", "wpb": "7652", "bsz": "32", "num_updates": "2450", "lr": "7.94737e-05", "gnorm": "1.445", "clip": "100", "loss_scale": "1", "train_wall": "71", "gb_free": "65", "wall": "33067"}
2022-12-20 13:28:49 | INFO | train_inner | {"epoch": 4, "update": 3.186, "loss": "2.194", "nll_loss": "2.194", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.58", "wps": "1089.7", "ups": "0.15", "wpb": "7513.7", "bsz": "32", "num_updates": "2460", "lr": "7.93684e-05", "gnorm": "1.592", "clip": "100", "loss_scale": "1", "train_wall": "69", "gb_free": "62.2", "wall": "33136"}
2022-12-20 13:29:59 | INFO | train_inner | {"epoch": 4, "update": 3.199, "loss": "2.188", "nll_loss": "2.188", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.56", "wps": "1116.2", "ups": "0.14", "wpb": "7821.5", "bsz": "32", "num_updates": "2470", "lr": "7.92632e-05", "gnorm": "2.643", "clip": "100", "loss_scale": "1", "train_wall": "70", "gb_free": "62.9", "wall": "33206"}
2022-12-20 13:31:08 | INFO | train_inner | {"epoch": 4, "update": 3.212, "loss": "2.162", "nll_loss": "2.162", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.47", "wps": "1104.9", "ups": "0.15", "wpb": "7607.3", "bsz": "32", "num_updates": "2480", "lr": "7.91579e-05", "gnorm": "2.902", "clip": "100", "loss_scale": "1", "train_wall": "68", "gb_free": "65.6", "wall": "33275"}
2022-12-20 13:31:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-12-20 13:32:26 | INFO | train_inner | {"epoch": 4, "update": 3.226, "loss": "2.173", "nll_loss": "2.173", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.51", "wps": "981.5", "ups": "0.13", "wpb": "7638.3", "bsz": "32", "num_updates": "2490", "lr": "7.90526e-05", "gnorm": "2.025", "clip": "100", "loss_scale": "0.5", "train_wall": "77", "gb_free": "62.9", "wall": "33353"}
2022-12-20 13:33:35 | INFO | train_inner | {"epoch": 4, "update": 3.239, "loss": "2.131", "nll_loss": "2.131", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.38", "wps": "1091.6", "ups": "0.14", "wpb": "7579.2", "bsz": "32", "num_updates": "2500", "lr": "7.89474e-05", "gnorm": "1.918", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "63.6", "wall": "33423"}
2022-12-20 13:34:47 | INFO | train_inner | {"epoch": 4, "update": 3.252, "loss": "2.188", "nll_loss": "2.188", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.56", "wps": "1085.3", "ups": "0.14", "wpb": "7789.5", "bsz": "32", "num_updates": "2510", "lr": "7.88421e-05", "gnorm": "2.212", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "33494"}
2022-12-20 13:35:58 | INFO | train_inner | {"epoch": 4, "update": 3.265, "loss": "2.178", "nll_loss": "2.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.53", "wps": "1086.3", "ups": "0.14", "wpb": "7691", "bsz": "32", "num_updates": "2520", "lr": "7.87368e-05", "gnorm": "2.935", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "33565"}
2022-12-20 13:37:12 | INFO | train_inner | {"epoch": 4, "update": 3.277, "loss": "2.178", "nll_loss": "2.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.53", "wps": "1053.4", "ups": "0.14", "wpb": "7774.2", "bsz": "32", "num_updates": "2530", "lr": "7.86316e-05", "gnorm": "2.067", "clip": "100", "loss_scale": "0.5", "train_wall": "74", "gb_free": "62.9", "wall": "33639"}
2022-12-20 13:38:19 | INFO | train_inner | {"epoch": 4, "update": 3.29, "loss": "2.187", "nll_loss": "2.187", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.55", "wps": "1132.6", "ups": "0.15", "wpb": "7648.7", "bsz": "32", "num_updates": "2540", "lr": "7.85263e-05", "gnorm": "1.951", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "65", "wall": "33707"}
2022-12-20 13:39:30 | INFO | train_inner | {"epoch": 4, "update": 3.303, "loss": "2.233", "nll_loss": "2.233", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.7", "wps": "1076.4", "ups": "0.14", "wpb": "7590.8", "bsz": "32", "num_updates": "2550", "lr": "7.84211e-05", "gnorm": "1.815", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "33777"}
2022-12-20 13:40:36 | INFO | train_inner | {"epoch": 4, "update": 3.316, "loss": "2.225", "nll_loss": "2.225", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.68", "wps": "1147.8", "ups": "0.15", "wpb": "7620.7", "bsz": "32", "num_updates": "2560", "lr": "7.83158e-05", "gnorm": "2.972", "clip": "100", "loss_scale": "0.5", "train_wall": "66", "gb_free": "63.6", "wall": "33844"}
2022-12-20 13:41:47 | INFO | train_inner | {"epoch": 4, "update": 3.329, "loss": "2.223", "nll_loss": "2.223", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.67", "wps": "1083.6", "ups": "0.14", "wpb": "7628.4", "bsz": "32", "num_updates": "2570", "lr": "7.82105e-05", "gnorm": "6.991", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "33914"}
2022-12-20 13:42:55 | INFO | train_inner | {"epoch": 4, "update": 3.342, "loss": "2.211", "nll_loss": "2.211", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.63", "wps": "1095.7", "ups": "0.15", "wpb": "7463.8", "bsz": "32", "num_updates": "2580", "lr": "7.81053e-05", "gnorm": "2.61", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "63.6", "wall": "33982"}
2022-12-20 13:44:06 | INFO | train_inner | {"epoch": 4, "update": 3.355, "loss": "2.174", "nll_loss": "2.174", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.51", "wps": "1075.5", "ups": "0.14", "wpb": "7648.2", "bsz": "32", "num_updates": "2590", "lr": "7.8e-05", "gnorm": "2.227", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63", "wall": "34053"}
2022-12-20 13:45:18 | INFO | train_inner | {"epoch": 4, "update": 3.368, "loss": "2.225", "nll_loss": "2.225", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.68", "wps": "1062.4", "ups": "0.14", "wpb": "7610.9", "bsz": "32", "num_updates": "2600", "lr": "7.78947e-05", "gnorm": "1.362", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.9", "wall": "34125"}
2022-12-20 13:46:27 | INFO | train_inner | {"epoch": 4, "update": 3.381, "loss": "2.232", "nll_loss": "2.232", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.7", "wps": "1103.3", "ups": "0.14", "wpb": "7623.8", "bsz": "32", "num_updates": "2610", "lr": "7.77895e-05", "gnorm": "1.399", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65", "wall": "34194"}
2022-12-20 13:47:38 | INFO | train_inner | {"epoch": 4, "update": 3.394, "loss": "2.127", "nll_loss": "2.127", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.37", "wps": "1088.4", "ups": "0.14", "wpb": "7740.9", "bsz": "32", "num_updates": "2620", "lr": "7.76842e-05", "gnorm": "1.424", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "34265"}
2022-12-20 13:48:47 | INFO | train_inner | {"epoch": 4, "update": 3.406, "loss": "2.235", "nll_loss": "2.235", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.71", "wps": "1112.4", "ups": "0.15", "wpb": "7637.9", "bsz": "31.9", "num_updates": "2630", "lr": "7.75789e-05", "gnorm": "1.412", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "63.6", "wall": "34334"}
2022-12-20 13:49:56 | INFO | train_inner | {"epoch": 4, "update": 3.419, "loss": "2.205", "nll_loss": "2.205", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.61", "wps": "1095.5", "ups": "0.15", "wpb": "7534.7", "bsz": "32", "num_updates": "2640", "lr": "7.74737e-05", "gnorm": "1.465", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "64.3", "wall": "34403"}
2022-12-20 13:51:04 | INFO | train_inner | {"epoch": 4, "update": 3.432, "loss": "2.268", "nll_loss": "2.268", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.82", "wps": "1088.7", "ups": "0.15", "wpb": "7487.2", "bsz": "32", "num_updates": "2650", "lr": "7.73684e-05", "gnorm": "1.797", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "62.2", "wall": "34471"}
2022-12-20 13:52:15 | INFO | train_inner | {"epoch": 4, "update": 3.445, "loss": "2.22", "nll_loss": "2.22", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.66", "wps": "1086.2", "ups": "0.14", "wpb": "7646.2", "bsz": "32", "num_updates": "2660", "lr": "7.72632e-05", "gnorm": "1.478", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.9", "wall": "34542"}
2022-12-20 13:53:24 | INFO | train_inner | {"epoch": 4, "update": 3.458, "loss": "2.17", "nll_loss": "2.17", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.5", "wps": "1104", "ups": "0.14", "wpb": "7666.5", "bsz": "32", "num_updates": "2670", "lr": "7.71579e-05", "gnorm": "1.365", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "34611"}
2022-12-20 13:54:34 | INFO | train_inner | {"epoch": 4, "update": 3.471, "loss": "2.206", "nll_loss": "2.206", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.61", "wps": "1091.2", "ups": "0.14", "wpb": "7642.3", "bsz": "32", "num_updates": "2680", "lr": "7.70526e-05", "gnorm": "1.374", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "65", "wall": "34681"}
2022-12-20 13:55:46 | INFO | train_inner | {"epoch": 4, "update": 3.484, "loss": "2.185", "nll_loss": "2.185", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.55", "wps": "1087.6", "ups": "0.14", "wpb": "7751.1", "bsz": "32", "num_updates": "2690", "lr": "7.69474e-05", "gnorm": "1.341", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "62.2", "wall": "34753"}
2022-12-20 13:56:56 | INFO | train_inner | {"epoch": 4, "update": 3.497, "loss": "2.267", "nll_loss": "2.267", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.81", "wps": "1074.3", "ups": "0.14", "wpb": "7549.5", "bsz": "32", "num_updates": "2700", "lr": "7.68421e-05", "gnorm": "1.411", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "34823"}
2022-12-20 13:58:07 | INFO | train_inner | {"epoch": 4, "update": 3.51, "loss": "2.179", "nll_loss": "2.179", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.53", "wps": "1096.8", "ups": "0.14", "wpb": "7772.5", "bsz": "32", "num_updates": "2710", "lr": "7.67368e-05", "gnorm": "1.336", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "34894"}
2022-12-20 13:59:14 | INFO | train_inner | {"epoch": 4, "update": 3.523, "loss": "2.198", "nll_loss": "2.198", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.59", "wps": "1149.3", "ups": "0.15", "wpb": "7733.3", "bsz": "32", "num_updates": "2720", "lr": "7.66316e-05", "gnorm": "1.305", "clip": "100", "loss_scale": "0.5", "train_wall": "67", "gb_free": "63.7", "wall": "34961"}
2022-12-20 14:00:24 | INFO | train_inner | {"epoch": 4, "update": 3.535, "loss": "2.206", "nll_loss": "2.206", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.61", "wps": "1086.4", "ups": "0.14", "wpb": "7569", "bsz": "32", "num_updates": "2730", "lr": "7.65263e-05", "gnorm": "1.354", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65.1", "wall": "35031"}
2022-12-20 14:01:35 | INFO | train_inner | {"epoch": 4, "update": 3.548, "loss": "2.222", "nll_loss": "2.222", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.67", "wps": "1079.8", "ups": "0.14", "wpb": "7692.4", "bsz": "32", "num_updates": "2740", "lr": "7.64211e-05", "gnorm": "1.321", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "65", "wall": "35102"}
2022-12-20 14:02:45 | INFO | train_inner | {"epoch": 4, "update": 3.561, "loss": "2.155", "nll_loss": "2.155", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.45", "wps": "1092.3", "ups": "0.14", "wpb": "7612.4", "bsz": "32", "num_updates": "2750", "lr": "7.63158e-05", "gnorm": "3.199", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.9", "wall": "35172"}
2022-12-20 14:03:54 | INFO | train_inner | {"epoch": 4, "update": 3.574, "loss": "2.215", "nll_loss": "2.215", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.64", "wps": "1093.4", "ups": "0.14", "wpb": "7580.7", "bsz": "32", "num_updates": "2760", "lr": "7.62105e-05", "gnorm": "1.336", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "35241"}
2022-12-20 14:05:04 | INFO | train_inner | {"epoch": 4, "update": 3.587, "loss": "2.169", "nll_loss": "2.169", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.5", "wps": "1087", "ups": "0.14", "wpb": "7573", "bsz": "32", "num_updates": "2770", "lr": "7.61053e-05", "gnorm": "1.36", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "35311"}
2022-12-20 14:06:13 | INFO | train_inner | {"epoch": 4, "update": 3.6, "loss": "2.241", "nll_loss": "2.241", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.73", "wps": "1099.2", "ups": "0.15", "wpb": "7557.8", "bsz": "32", "num_updates": "2780", "lr": "7.6e-05", "gnorm": "1.355", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "64.3", "wall": "35380"}
2022-12-20 14:07:24 | INFO | train_inner | {"epoch": 4, "update": 3.613, "loss": "2.203", "nll_loss": "2.203", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.6", "wps": "1054.8", "ups": "0.14", "wpb": "7518.8", "bsz": "32", "num_updates": "2790", "lr": "7.58947e-05", "gnorm": "1.337", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "66.3", "wall": "35451"}
2022-12-20 14:08:37 | INFO | train_inner | {"epoch": 4, "update": 3.626, "loss": "2.145", "nll_loss": "2.145", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.42", "wps": "1057.1", "ups": "0.14", "wpb": "7744.4", "bsz": "32", "num_updates": "2800", "lr": "7.57895e-05", "gnorm": "1.329", "clip": "100", "loss_scale": "0.5", "train_wall": "73", "gb_free": "62.2", "wall": "35524"}
2022-12-20 14:09:49 | INFO | train_inner | {"epoch": 4, "update": 3.639, "loss": "2.194", "nll_loss": "2.194", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.58", "wps": "1071", "ups": "0.14", "wpb": "7663.1", "bsz": "32", "num_updates": "2810", "lr": "7.56842e-05", "gnorm": "1.347", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.6", "wall": "35596"}
2022-12-20 14:10:58 | INFO | train_inner | {"epoch": 4, "update": 3.652, "loss": "2.232", "nll_loss": "2.232", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.7", "wps": "1091.6", "ups": "0.14", "wpb": "7592.9", "bsz": "32", "num_updates": "2820", "lr": "7.55789e-05", "gnorm": "1.354", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65.6", "wall": "35665"}
2022-12-20 14:12:10 | INFO | train_inner | {"epoch": 4, "update": 3.665, "loss": "2.173", "nll_loss": "2.173", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.51", "wps": "1079.5", "ups": "0.14", "wpb": "7748.3", "bsz": "32", "num_updates": "2830", "lr": "7.54737e-05", "gnorm": "1.338", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63.6", "wall": "35737"}
2022-12-20 14:13:16 | INFO | train_inner | {"epoch": 4, "update": 3.677, "loss": "2.255", "nll_loss": "2.255", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.77", "wps": "1128.7", "ups": "0.15", "wpb": "7493.1", "bsz": "32", "num_updates": "2840", "lr": "7.53684e-05", "gnorm": "1.41", "clip": "100", "loss_scale": "0.5", "train_wall": "66", "gb_free": "63", "wall": "35804"}
2022-12-20 14:14:27 | INFO | train_inner | {"epoch": 4, "update": 3.69, "loss": "2.175", "nll_loss": "2.175", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.51", "wps": "1084.5", "ups": "0.14", "wpb": "7624", "bsz": "32", "num_updates": "2850", "lr": "7.52632e-05", "gnorm": "1.339", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "35874"}
2022-12-20 14:15:38 | INFO | train_inner | {"epoch": 4, "update": 3.703, "loss": "2.172", "nll_loss": "2.172", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.51", "wps": "1066.7", "ups": "0.14", "wpb": "7552.1", "bsz": "32", "num_updates": "2860", "lr": "7.51579e-05", "gnorm": "1.346", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "64.3", "wall": "35945"}
2022-12-20 14:16:47 | INFO | train_inner | {"epoch": 4, "update": 3.716, "loss": "2.191", "nll_loss": "2.191", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.57", "wps": "1122", "ups": "0.14", "wpb": "7754.8", "bsz": "32", "num_updates": "2870", "lr": "7.50526e-05", "gnorm": "1.348", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "36014"}
2022-12-20 14:17:57 | INFO | train_inner | {"epoch": 4, "update": 3.729, "loss": "2.259", "nll_loss": "2.259", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.79", "wps": "1088.9", "ups": "0.14", "wpb": "7674.5", "bsz": "32", "num_updates": "2880", "lr": "7.49474e-05", "gnorm": "1.364", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "36084"}
2022-12-20 14:19:08 | INFO | train_inner | {"epoch": 4, "update": 3.742, "loss": "2.235", "nll_loss": "2.235", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.71", "wps": "1082.1", "ups": "0.14", "wpb": "7666.3", "bsz": "32", "num_updates": "2890", "lr": "7.48421e-05", "gnorm": "1.369", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "66.3", "wall": "36155"}
2022-12-20 14:20:18 | INFO | train_inner | {"epoch": 4, "update": 3.755, "loss": "2.207", "nll_loss": "2.207", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.62", "wps": "1093.6", "ups": "0.14", "wpb": "7630.2", "bsz": "32", "num_updates": "2900", "lr": "7.47368e-05", "gnorm": "1.367", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "64.3", "wall": "36225"}
2022-12-20 14:21:32 | INFO | train_inner | {"epoch": 4, "update": 3.768, "loss": "2.256", "nll_loss": "2.256", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.78", "wps": "1040.4", "ups": "0.13", "wpb": "7710.7", "bsz": "32", "num_updates": "2910", "lr": "7.46316e-05", "gnorm": "1.349", "clip": "100", "loss_scale": "0.5", "train_wall": "74", "gb_free": "62.2", "wall": "36299"}
2022-12-20 14:22:44 | INFO | train_inner | {"epoch": 4, "update": 3.781, "loss": "2.216", "nll_loss": "2.216", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.64", "wps": "1069", "ups": "0.14", "wpb": "7653.9", "bsz": "32", "num_updates": "2920", "lr": "7.45263e-05", "gnorm": "1.312", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "36371"}
2022-12-20 14:23:54 | INFO | train_inner | {"epoch": 4, "update": 3.794, "loss": "2.196", "nll_loss": "2.196", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.58", "wps": "1102.9", "ups": "0.14", "wpb": "7737.7", "bsz": "32", "num_updates": "2930", "lr": "7.44211e-05", "gnorm": "1.318", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "63.6", "wall": "36441"}
2022-12-20 14:25:03 | INFO | train_inner | {"epoch": 4, "update": 3.806, "loss": "2.189", "nll_loss": "2.189", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.56", "wps": "1128.7", "ups": "0.15", "wpb": "7781.1", "bsz": "32", "num_updates": "2940", "lr": "7.43158e-05", "gnorm": "1.344", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "65", "wall": "36510"}
2022-12-20 14:26:18 | INFO | train_inner | {"epoch": 4, "update": 3.819, "loss": "2.195", "nll_loss": "2.195", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.58", "wps": "1013.5", "ups": "0.13", "wpb": "7595.8", "bsz": "32", "num_updates": "2950", "lr": "7.42105e-05", "gnorm": "1.382", "clip": "100", "loss_scale": "0.5", "train_wall": "75", "gb_free": "62.2", "wall": "36585"}
2022-12-20 14:27:27 | INFO | train_inner | {"epoch": 4, "update": 3.832, "loss": "2.262", "nll_loss": "2.262", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.8", "wps": "1085.7", "ups": "0.14", "wpb": "7539.2", "bsz": "32", "num_updates": "2960", "lr": "7.41053e-05", "gnorm": "1.388", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "36654"}
2022-12-20 14:28:37 | INFO | train_inner | {"epoch": 4, "update": 3.845, "loss": "2.236", "nll_loss": "2.236", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.71", "wps": "1106", "ups": "0.14", "wpb": "7708.2", "bsz": "32", "num_updates": "2970", "lr": "7.4e-05", "gnorm": "1.371", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.9", "wall": "36724"}
2022-12-20 14:29:47 | INFO | train_inner | {"epoch": 4, "update": 3.858, "loss": "2.209", "nll_loss": "2.209", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.62", "wps": "1082.3", "ups": "0.14", "wpb": "7582.6", "bsz": "32", "num_updates": "2980", "lr": "7.38947e-05", "gnorm": "1.375", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "36794"}
2022-12-20 14:30:56 | INFO | train_inner | {"epoch": 4, "update": 3.871, "loss": "2.197", "nll_loss": "2.197", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.59", "wps": "1099.1", "ups": "0.14", "wpb": "7588.9", "bsz": "32", "num_updates": "2990", "lr": "7.37895e-05", "gnorm": "1.348", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "64.3", "wall": "36863"}
2022-12-20 14:32:07 | INFO | train_inner | {"epoch": 4, "update": 3.884, "loss": "2.24", "nll_loss": "2.24", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.72", "wps": "1086.7", "ups": "0.14", "wpb": "7739.3", "bsz": "32", "num_updates": "3000", "lr": "7.36842e-05", "gnorm": "1.342", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "63", "wall": "36934"}
2022-12-20 14:33:17 | INFO | train_inner | {"epoch": 4, "update": 3.897, "loss": "2.216", "nll_loss": "2.216", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.65", "wps": "1084.1", "ups": "0.14", "wpb": "7545.2", "bsz": "32", "num_updates": "3010", "lr": "7.35789e-05", "gnorm": "1.353", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.2", "wall": "37004"}
2022-12-20 14:34:29 | INFO | train_inner | {"epoch": 4, "update": 3.91, "loss": "2.277", "nll_loss": "2.277", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.85", "wps": "1045.9", "ups": "0.14", "wpb": "7542.6", "bsz": "32", "num_updates": "3020", "lr": "7.34737e-05", "gnorm": "1.355", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "62.2", "wall": "37076"}
2022-12-20 14:35:39 | INFO | train_inner | {"epoch": 4, "update": 3.923, "loss": "2.191", "nll_loss": "2.191", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.57", "wps": "1103.6", "ups": "0.14", "wpb": "7690.7", "bsz": "32", "num_updates": "3030", "lr": "7.33684e-05", "gnorm": "1.458", "clip": "100", "loss_scale": "0.5", "train_wall": "69", "gb_free": "62.9", "wall": "37146"}
2022-12-20 14:36:47 | INFO | train_inner | {"epoch": 4, "update": 3.935, "loss": "2.184", "nll_loss": "2.184", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.54", "wps": "1124.1", "ups": "0.15", "wpb": "7674.5", "bsz": "32", "num_updates": "3040", "lr": "7.32632e-05", "gnorm": "1.298", "clip": "100", "loss_scale": "0.5", "train_wall": "68", "gb_free": "65", "wall": "37214"}
2022-12-20 14:38:00 | INFO | train_inner | {"epoch": 4, "update": 3.948, "loss": "2.203", "nll_loss": "2.203", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.6", "wps": "1053", "ups": "0.14", "wpb": "7669.6", "bsz": "32", "num_updates": "3050", "lr": "7.31579e-05", "gnorm": "1.338", "clip": "100", "loss_scale": "0.5", "train_wall": "73", "gb_free": "62.2", "wall": "37287"}
2022-12-20 14:39:12 | INFO | train_inner | {"epoch": 4, "update": 3.961, "loss": "2.263", "nll_loss": "2.263", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.8", "wps": "1042.8", "ups": "0.14", "wpb": "7518.3", "bsz": "32", "num_updates": "3060", "lr": "7.30526e-05", "gnorm": "1.396", "clip": "100", "loss_scale": "0.5", "train_wall": "72", "gb_free": "63.6", "wall": "37359"}
2022-12-20 14:40:23 | INFO | train_inner | {"epoch": 4, "update": 3.974, "loss": "2.18", "nll_loss": "2.18", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.53", "wps": "1079.8", "ups": "0.14", "wpb": "7660.8", "bsz": "32", "num_updates": "3070", "lr": "7.29474e-05", "gnorm": "1.377", "clip": "100", "loss_scale": "0.5", "train_wall": "71", "gb_free": "64.3", "wall": "37430"}
2022-12-20 14:41:34 | INFO | train_inner | {"epoch": 4, "update": 3.987, "loss": "2.193", "nll_loss": "2.193", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.57", "wps": "1085.2", "ups": "0.14", "wpb": "7665.7", "bsz": "32", "num_updates": "3080", "lr": "7.28421e-05", "gnorm": "1.446", "clip": "100", "loss_scale": "0.5", "train_wall": "70", "gb_free": "62.2", "wall": "37501"}
2022-12-20 14:42:39 | INFO | train_inner | {"epoch": 4, "update": 4.0, "loss": "2.273", "nll_loss": "2.273", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.83", "wps": "1066", "ups": "0.15", "wpb": "6985.9", "bsz": "29.4", "num_updates": "3090", "lr": "7.27368e-05", "gnorm": "1.562", "clip": "100", "loss_scale": "0.5", "train_wall": "65", "gb_free": "63.7", "wall": "37566"}
2022-12-20 14:42:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-20 14:43:24 | INFO | absl | Using default tokenizer.
2022-12-20 14:43:47 | INFO | absl | Using default tokenizer.
2022-12-20 14:44:05 | INFO | absl | Using default tokenizer.
2022-12-20 14:44:27 | INFO | absl | Using default tokenizer.
2022-12-20 14:44:49 | INFO | absl | Using default tokenizer.
2022-12-20 14:45:19 | INFO | absl | Using default tokenizer.
2022-12-20 14:45:52 | INFO | absl | Using default tokenizer.
2022-12-20 14:46:14 | INFO | absl | Using default tokenizer.
2022-12-20 14:46:46 | INFO | absl | Using default tokenizer.
2022-12-20 14:47:09 | INFO | absl | Using default tokenizer.
2022-12-20 14:47:31 | INFO | absl | Using default tokenizer.
2022-12-20 14:48:00 | INFO | absl | Using default tokenizer.
2022-12-20 14:48:28 | INFO | absl | Using default tokenizer.
2022-12-20 14:49:03 | INFO | absl | Using default tokenizer.
2022-12-20 14:49:25 | INFO | absl | Using default tokenizer.
2022-12-20 14:49:49 | INFO | absl | Using default tokenizer.
2022-12-20 14:50:28 | INFO | absl | Using default tokenizer.
2022-12-20 14:50:52 | INFO | absl | Using default tokenizer.
2022-12-20 14:51:16 | INFO | absl | Using default tokenizer.
2022-12-20 14:51:36 | INFO | absl | Using default tokenizer.
2022-12-20 14:52:10 | INFO | absl | Using default tokenizer.
2022-12-20 14:52:33 | INFO | absl | Using default tokenizer.
2022-12-20 14:53:00 | INFO | absl | Using default tokenizer.
2022-12-20 14:53:19 | INFO | absl | Using default tokenizer.
2022-12-20 14:53:43 | INFO | absl | Using default tokenizer.
2022-12-20 14:54:09 | INFO | absl | Using default tokenizer.
2022-12-20 14:54:34 | INFO | absl | Using default tokenizer.
2022-12-20 14:55:11 | INFO | absl | Using default tokenizer.
2022-12-20 14:55:38 | INFO | absl | Using default tokenizer.
2022-12-20 14:56:04 | INFO | absl | Using default tokenizer.
2022-12-20 14:56:28 | INFO | absl | Using default tokenizer.
2022-12-20 14:56:51 | INFO | absl | Using default tokenizer.
2022-12-20 14:57:15 | INFO | absl | Using default tokenizer.
2022-12-20 14:57:36 | INFO | absl | Using default tokenizer.
2022-12-20 14:57:59 | INFO | absl | Using default tokenizer.
2022-12-20 14:58:31 | INFO | absl | Using default tokenizer.
2022-12-20 14:59:05 | INFO | absl | Using default tokenizer.
2022-12-20 14:59:33 | INFO | absl | Using default tokenizer.
2022-12-20 14:59:57 | INFO | absl | Using default tokenizer.
2022-12-20 15:00:26 | INFO | absl | Using default tokenizer.
2022-12-20 15:00:53 | INFO | absl | Using default tokenizer.
2022-12-20 15:01:17 | INFO | absl | Using default tokenizer.
2022-12-20 15:01:41 | INFO | absl | Using default tokenizer.
2022-12-20 15:02:03 | INFO | absl | Using default tokenizer.
2022-12-20 15:02:33 | INFO | absl | Using default tokenizer.
2022-12-20 15:03:01 | INFO | absl | Using default tokenizer.
2022-12-20 15:03:26 | INFO | absl | Using default tokenizer.
2022-12-20 15:03:58 | INFO | absl | Using default tokenizer.
2022-12-20 15:04:28 | INFO | absl | Using default tokenizer.
2022-12-20 15:04:55 | INFO | absl | Using default tokenizer.
2022-12-20 15:05:16 | INFO | absl | Using default tokenizer.
2022-12-20 15:05:41 | INFO | absl | Using default tokenizer.
2022-12-20 15:06:04 | INFO | absl | Using default tokenizer.
2022-12-20 15:06:27 | INFO | absl | Using default tokenizer.
2022-12-20 15:06:49 | INFO | absl | Using default tokenizer.
2022-12-20 15:07:17 | INFO | absl | Using default tokenizer.
2022-12-20 15:07:40 | INFO | absl | Using default tokenizer.
2022-12-20 15:08:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:08:37 | INFO | absl | Using default tokenizer.
2022-12-20 15:09:03 | INFO | absl | Using default tokenizer.
2022-12-20 15:09:28 | INFO | absl | Using default tokenizer.
2022-12-20 15:09:49 | INFO | absl | Using default tokenizer.
2022-12-20 15:10:15 | INFO | absl | Using default tokenizer.
2022-12-20 15:10:41 | INFO | absl | Using default tokenizer.
2022-12-20 15:11:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:11:36 | INFO | absl | Using default tokenizer.
2022-12-20 15:11:56 | INFO | absl | Using default tokenizer.
2022-12-20 15:12:23 | INFO | absl | Using default tokenizer.
2022-12-20 15:12:50 | INFO | absl | Using default tokenizer.
2022-12-20 15:13:16 | INFO | absl | Using default tokenizer.
2022-12-20 15:13:43 | INFO | absl | Using default tokenizer.
2022-12-20 15:14:09 | INFO | absl | Using default tokenizer.
2022-12-20 15:14:35 | INFO | absl | Using default tokenizer.
2022-12-20 15:14:58 | INFO | absl | Using default tokenizer.
2022-12-20 15:15:28 | INFO | absl | Using default tokenizer.
2022-12-20 15:15:55 | INFO | absl | Using default tokenizer.
2022-12-20 15:16:18 | INFO | absl | Using default tokenizer.
2022-12-20 15:16:45 | INFO | absl | Using default tokenizer.
2022-12-20 15:17:04 | INFO | absl | Using default tokenizer.
2022-12-20 15:17:27 | INFO | absl | Using default tokenizer.
2022-12-20 15:17:57 | INFO | absl | Using default tokenizer.
2022-12-20 15:18:23 | INFO | absl | Using default tokenizer.
2022-12-20 15:18:44 | INFO | absl | Using default tokenizer.
2022-12-20 15:19:06 | INFO | absl | Using default tokenizer.
2022-12-20 15:19:37 | INFO | absl | Using default tokenizer.
2022-12-20 15:19:59 | INFO | absl | Using default tokenizer.
2022-12-20 15:20:22 | INFO | absl | Using default tokenizer.
2022-12-20 15:20:51 | INFO | absl | Using default tokenizer.
2022-12-20 15:21:20 | INFO | absl | Using default tokenizer.
2022-12-20 15:21:44 | INFO | absl | Using default tokenizer.
2022-12-20 15:22:13 | INFO | absl | Using default tokenizer.
2022-12-20 15:22:38 | INFO | absl | Using default tokenizer.
2022-12-20 15:23:04 | INFO | absl | Using default tokenizer.
2022-12-20 15:23:29 | INFO | absl | Using default tokenizer.
2022-12-20 15:23:54 | INFO | absl | Using default tokenizer.
2022-12-20 15:24:19 | INFO | absl | Using default tokenizer.
2022-12-20 15:24:42 | INFO | absl | Using default tokenizer.
2022-12-20 15:25:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:25:35 | INFO | absl | Using default tokenizer.
2022-12-20 15:26:02 | INFO | absl | Using default tokenizer.
2022-12-20 15:26:37 | INFO | absl | Using default tokenizer.
2022-12-20 15:27:05 | INFO | absl | Using default tokenizer.
2022-12-20 15:27:32 | INFO | absl | Using default tokenizer.
2022-12-20 15:28:00 | INFO | absl | Using default tokenizer.
2022-12-20 15:28:27 | INFO | absl | Using default tokenizer.
2022-12-20 15:28:55 | INFO | absl | Using default tokenizer.
2022-12-20 15:29:24 | INFO | absl | Using default tokenizer.
2022-12-20 15:29:53 | INFO | absl | Using default tokenizer.
2022-12-20 15:30:17 | INFO | absl | Using default tokenizer.
2022-12-20 15:30:44 | INFO | absl | Using default tokenizer.
2022-12-20 15:31:09 | INFO | absl | Using default tokenizer.
2022-12-20 15:31:31 | INFO | absl | Using default tokenizer.
2022-12-20 15:32:00 | INFO | absl | Using default tokenizer.
2022-12-20 15:32:29 | INFO | absl | Using default tokenizer.
2022-12-20 15:32:50 | INFO | absl | Using default tokenizer.
2022-12-20 15:33:20 | INFO | absl | Using default tokenizer.
2022-12-20 15:33:46 | INFO | absl | Using default tokenizer.
2022-12-20 15:34:15 | INFO | absl | Using default tokenizer.
2022-12-20 15:34:42 | INFO | absl | Using default tokenizer.
2022-12-20 15:35:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:35:36 | INFO | absl | Using default tokenizer.
2022-12-20 15:36:05 | INFO | absl | Using default tokenizer.
2022-12-20 15:36:28 | INFO | absl | Using default tokenizer.
2022-12-20 15:36:55 | INFO | absl | Using default tokenizer.
2022-12-20 15:37:19 | INFO | absl | Using default tokenizer.
2022-12-20 15:37:44 | INFO | absl | Using default tokenizer.
2022-12-20 15:38:10 | INFO | absl | Using default tokenizer.
2022-12-20 15:38:30 | INFO | absl | Using default tokenizer.
2022-12-20 15:39:00 | INFO | absl | Using default tokenizer.
2022-12-20 15:39:28 | INFO | absl | Using default tokenizer.
2022-12-20 15:39:53 | INFO | absl | Using default tokenizer.
2022-12-20 15:40:20 | INFO | absl | Using default tokenizer.
2022-12-20 15:40:50 | INFO | absl | Using default tokenizer.
2022-12-20 15:41:17 | INFO | absl | Using default tokenizer.
2022-12-20 15:41:48 | INFO | absl | Using default tokenizer.
2022-12-20 15:42:15 | INFO | absl | Using default tokenizer.
2022-12-20 15:42:44 | INFO | absl | Using default tokenizer.
2022-12-20 15:43:10 | INFO | absl | Using default tokenizer.
2022-12-20 15:43:40 | INFO | absl | Using default tokenizer.
2022-12-20 15:44:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:44:38 | INFO | absl | Using default tokenizer.
2022-12-20 15:45:07 | INFO | absl | Using default tokenizer.
2022-12-20 15:45:30 | INFO | absl | Using default tokenizer.
2022-12-20 15:45:53 | INFO | absl | Using default tokenizer.
2022-12-20 15:46:19 | INFO | absl | Using default tokenizer.
2022-12-20 15:46:43 | INFO | absl | Using default tokenizer.
2022-12-20 15:47:08 | INFO | absl | Using default tokenizer.
2022-12-20 15:47:35 | INFO | absl | Using default tokenizer.
2022-12-20 15:48:06 | INFO | absl | Using default tokenizer.
2022-12-20 15:48:37 | INFO | absl | Using default tokenizer.
2022-12-20 15:49:05 | INFO | absl | Using default tokenizer.
2022-12-20 15:49:40 | INFO | absl | Using default tokenizer.
2022-12-20 15:50:05 | INFO | absl | Using default tokenizer.
2022-12-20 15:50:35 | INFO | absl | Using default tokenizer.
2022-12-20 15:51:05 | INFO | absl | Using default tokenizer.
2022-12-20 15:51:32 | INFO | absl | Using default tokenizer.
2022-12-20 15:51:59 | INFO | absl | Using default tokenizer.
2022-12-20 15:52:31 | INFO | absl | Using default tokenizer.
2022-12-20 15:53:02 | INFO | absl | Using default tokenizer.
2022-12-20 15:53:24 | INFO | absl | Using default tokenizer.
2022-12-20 15:53:54 | INFO | absl | Using default tokenizer.
2022-12-20 15:54:20 | INFO | absl | Using default tokenizer.
2022-12-20 15:54:57 | INFO | absl | Using default tokenizer.
2022-12-20 15:55:26 | INFO | absl | Using default tokenizer.
2022-12-20 15:55:52 | INFO | absl | Using default tokenizer.
2022-12-20 15:56:23 | INFO | absl | Using default tokenizer.
2022-12-20 15:56:57 | INFO | absl | Using default tokenizer.
2022-12-20 15:57:27 | INFO | absl | Using default tokenizer.
2022-12-20 15:58:00 | INFO | absl | Using default tokenizer.
2022-12-20 15:58:32 | INFO | absl | Using default tokenizer.
2022-12-20 15:59:18 | INFO | absl | Using default tokenizer.
2022-12-20 15:59:47 | INFO | absl | Using default tokenizer.
2022-12-20 16:00:16 | INFO | valid | {"epoch": 4, "valid_loss": "2.738", "valid_nll_loss": "2.738", "valid_rouge1": 0.4991537215639261, "valid_rouge2": 0.1883761021859088, "valid_rougel": 0.27201496560885463, "valid_rouge_avg": 0.3437649118749175, "valid_ppl": "6.67", "valid_wps": "71.1", "valid_wpb": "1907.8", "valid_bsz": "8", "valid_num_updates": "3090", "valid_best_rouge_avg": 0.348865502326949}
2022-12-20 16:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3090 updates
2022-12-20 16:00:16 | INFO | fairseq.trainer | Saving checkpoint to checkpoints//graph_text/checkpoint_last.pt
