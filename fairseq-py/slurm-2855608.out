2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10034
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10034
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10034
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10034
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | initialized host bessemer-node033.shef.ac.uk as rank 0
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | initialized host bessemer-node033.shef.ac.uk as rank 3
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | initialized host bessemer-node033.shef.ac.uk as rank 1
2022-12-30 10:20:42 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-30 10:20:42 | INFO | fairseq.distributed.utils | initialized host bessemer-node033.shef.ac.uk as rank 2
2022-12-30 10:20:50 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 3, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10034', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1024, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints//dual_encode', 'restore_file': '../checkpoints/model_100k.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'rouge_avg', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_large', attention_dropout=0.0, attention_name='block_noglobal', azureml_logging=False, batch_size=2, batch_size_valid=2, best_checkpoint_metric='rouge_avg', bf16=False, block_attention=False, block_size=1024, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=True, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=True, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, custom_dict='../checkpoints/dict.txt', data='/home/acp20tg/bart_ls/resources/eLife_fs-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, dual_graph_encoder=True, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_rouge=True, eval_rouge_args='{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', eval_rouge_detok='space', eval_rouge_detok_args='{}', eval_rouge_print_samples=False, eval_rouge_remove_bpe=None, fast_stat_sync=True, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, input_pattern='concat', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.0, layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0001], lr_scheduler='polynomial_decay', max_epoch=50, max_query_positions=50, max_source_positions=16384, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=4, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, pad_query=0, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, pooling_layers=4, power=1.0, profile=False, quantization_config_path=None, query_based=False, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1024, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='../checkpoints/model_100k.pt', restrict_position_embed=False, save_dir='checkpoints//dual_encode', save_interval=1, save_interval_updates=0, scoring='bleu', seed=3, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, sliding_window=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='src', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='tgt', task='summarization', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, top_down=False, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, truncate_target=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_xformers=True, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'summarization', 'data': '/home/acp20tg/bart_ls/resources/eLife_fs-bin', 'source_lang': 'src', 'target_lang': 'tgt', 'load_alignments': False, 'left_pad_source': False, 'left_pad_target': False, 'max_source_positions': 16384, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'truncate_target': True, 'query_lang': 'query', 'query_based': False, 'max_query_positions': 50, 'pad_query': 0, 'input_pattern': 'concat', 'block_size': 1024, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1024, 'eval_rouge': True, 'eval_rouge_args': '{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', 'eval_rouge_detok': 'space', 'eval_rouge_detok_args': '{}', 'eval_rouge_remove_bpe': None, 'eval_rouge_print_samples': False, 'custom_dict': '../checkpoints/dict.txt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.0, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-12-30 10:20:50 | INFO | fairseq.tasks.summarization | [src] dictionary: 50606 types
2022-12-30 10:20:50 | INFO | fairseq.tasks.summarization | [tgt] dictionary: 50606 types
Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2022-12-30 10:22:08 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(16386, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (9): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (10): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (11): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50606, bias=False)
  )
  (classification_heads): ModuleDict()
  (graph_cross_attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
  )
)
2022-12-30 10:22:08 | INFO | fairseq_cli.train | task: SummarizationTask
2022-12-30 10:22:08 | INFO | fairseq_cli.train | model: BARTModel
2022-12-30 10:22:08 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-12-30 10:22:08 | INFO | fairseq_cli.train | num. shared model params: 443,361,280 (num. trained: 443,361,280)
2022-12-30 10:22:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-12-30 10:22:08 | INFO | fairseq.data.data_utils | loaded 241 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-bin/valid.src-tgt.src
2022-12-30 10:22:08 | INFO | fairseq.data.data_utils | loaded 241 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-bin/valid.src-tgt.tgt
2022-12-30 10:22:08 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/eLife_fs-bin valid src-tgt 241 examples
2022-12-30 10:22:09 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-12-30 10:22:09 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2022-12-30 10:22:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-30 10:22:09 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-30 10:22:09 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- graph_cross_attention.q_proj_weight
2022-12-30 10:22:09 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- graph_cross_attention.k_proj_weight
2022-12-30 10:22:09 | INFO | fairseq.trainer | detected shared parameter: decoder.output_projection.bias <- graph_cross_attention.v_proj_weight
2022-12-30 10:22:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-30 10:22:10 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-12-30 10:22:10 | INFO | fairseq.utils | rank   1: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-12-30 10:22:10 | INFO | fairseq.utils | rank   2: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-12-30 10:22:10 | INFO | fairseq.utils | rank   3: capabilities =  7.0  ; total memory = 31.749 GB ; name = Tesla V100-SXM2-32GB                    
2022-12-30 10:22:10 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-30 10:22:10 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2022-12-30 10:22:10 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2022-12-30 10:22:10 | INFO | fairseq.trainer | Preparing to load checkpoint ../checkpoints/model_100k.pt
2022-12-30 10:22:47 | INFO | fairseq.optim.adam | using FusedAdam
2022-12-30 10:22:47 | INFO | fairseq.trainer | Loaded checkpoint ../checkpoints/model_100k.pt (epoch 1 @ 0 updates)
2022-12-30 10:22:47 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-30 10:22:48 | INFO | fairseq.data.data_utils | loaded 4,346 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-bin/train.src-tgt.src
2022-12-30 10:22:48 | INFO | fairseq.data.data_utils | loaded 4,346 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-bin/train.src-tgt.tgt
2022-12-30 10:22:48 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/eLife_fs-bin train src-tgt 4346 examples
2022-12-30 10:22:48 | INFO | fairseq.trainer | begin training epoch 1
2022-12-30 10:22:48 | INFO | fairseq_cli.train | Start iterating over samples
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
IDs:  tensor([ 495, 3784], device='cuda:2')
ARGS:  TransformerConfig(_name='bart_large', activation_fn='gelu', dropout=0.1, attention_dropout=0.0, activation_dropout=0.0, adaptive_input=False, encoder=EncDecBaseConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None), max_source_positions=16384, decoder=DecoderConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None, input_dim=1024, output_dim=1024), max_target_positions=1024, share_decoder_input_output_embed=True, share_all_embeddings=True, no_token_positional_embeddings=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, layernorm_embedding=True, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=True, checkpoint_activations=True, offload_activations=False, no_cross_attention=False, cross_self_attention=False, quant_noise=QuantNoiseConfig(_name='bart_large', pq=0, pq_block_size=8, scalar=0), min_params_to_wrap=100000000, char_inputs=False, relu_dropout=0.0, base_layers=0, base_sublayers=1, base_shuffle=1, export=False, no_decoder_final_norm=False, alibi=False, truncate_alibi=None, use_xformers=False, attention_name='block_noglobal', xformer_config='{}', pooling_layers=4)
Enc out shape:  3
Enc out:  [tensor([[[-8.4915e-03,  3.6560e-02,  2.8000e-02,  ...,  2.3605e-02,
          -9.7122e-03, -2.4551e-02],
         [ 7.6294e-05,  3.5767e-02,  1.2451e-02,  ...,  1.7899e-02,
          -2.3560e-02, -2.0523e-02]],

        [[ 4.8248e-02, -8.7842e-01, -5.9937e-02,  ..., -1.7236e-01,
           2.4878e-01,  1.1652e-01],
         [ 2.4146e-01, -4.5593e-02,  8.5571e-02,  ...,  6.4148e-02,
           8.4534e-02,  9.8572e-02]],

        [[-7.8003e-02, -1.0859e+00,  1.2286e-01,  ..., -1.5091e-02,
          -7.5684e-02,  1.2793e-01],
         [ 1.6931e-01, -1.9934e-01,  1.6504e-01,  ..., -6.2744e-02,
          -4.2511e-02,  3.8916e-01]],

        ...,

        [[-1.4343e-02,  3.8574e-02,  3.2715e-02,  ...,  2.1881e-02,
           1.9440e-02, -7.0419e-03],
         [-2.3155e-03,  2.4124e-02,  2.5589e-02,  ...,  1.7929e-02,
           7.8430e-03, -4.5700e-03]],

        [[-4.3488e-03,  2.0477e-02,  2.0325e-02,  ...,  1.1803e-02,
           5.0659e-03, -8.2550e-03],
         [-1.9989e-03,  2.6245e-02,  2.1606e-02,  ...,  1.1368e-02,
           5.3711e-03, -7.2479e-03]],

        [[-1.8555e-02,  2.8824e-02,  2.3193e-02,  ...,  8.2016e-04,
           4.4861e-02, -2.4567e-02],
         [-4.9477e-03,  2.9938e-02,  1.9485e-02,  ...,  1.9989e-02,
           7.0953e-03, -1.0483e-02]]], device='cuda:2', dtype=torch.float16,
       grad_fn=<CheckpointFunctionBackward>)]
[['elife-02109-v1', 'has_title', 'Long term functional plasticity of sensory inputs mediated by olfactory learning'], ['elife-02109-v1_Sec3', 'has_title', 'Materials and methods'], ['elife-02109-v1_Sec0', 'has_title', 'Introduction'], ['elife-02109-v1_Sec1', 'has_title', 'Results'], ['elife-02109-v1_Sec2', 'has_title', 'Discussion']]
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
IDs:  tensor([3644,  196], device='cuda:0')
ARGS:  TransformerConfig(_name='bart_large', activation_fn='gelu', dropout=0.1, attention_dropout=0.0, activation_dropout=0.0, adaptive_input=False, encoder=EncDecBaseConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None), max_source_positions=16384, decoder=DecoderConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None, input_dim=1024, output_dim=1024), max_target_positions=1024, share_decoder_input_output_embed=True, share_all_embeddings=True, no_token_positional_embeddings=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, layernorm_embedding=True, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=True, checkpoint_activations=True, offload_activations=False, no_cross_attention=False, cross_self_attention=False, quant_noise=QuantNoiseConfig(_name='bart_large', pq=0, pq_block_size=8, scalar=0), min_params_to_wrap=100000000, char_inputs=False, relu_dropout=0.0, base_layers=0, base_sublayers=1, base_shuffle=1, export=False, no_decoder_final_norm=False, alibi=False, truncate_alibi=None, use_xformers=False, attention_name='block_noglobal', xformer_config='{}', pooling_layers=4)
Enc out shape:  3
Enc out:  [tensor([[[ 3.6240e-03,  3.9062e-02,  2.5574e-02,  ...,  1.2543e-02,
          -2.3544e-02, -1.4153e-02],
         [-5.5771e-03,  3.2013e-02,  1.9272e-02,  ...,  2.2919e-02,
          -1.1810e-02, -1.9958e-02]],

        [[ 9.3140e-02, -7.0996e-01,  4.2334e-01,  ..., -1.2524e-01,
          -6.0150e-02, -1.4490e-01],
         [ 3.3203e-02, -4.0796e-01,  1.8079e-01,  ...,  2.9694e-02,
          -1.6895e-01,  2.7634e-02]],

        [[-1.8665e-01, -7.2412e-01,  3.4204e-01,  ...,  7.6477e-02,
          -1.4563e-01, -1.2537e-01],
         [ 3.2202e-01, -4.8315e-01,  2.4109e-01,  ...,  2.5085e-02,
           3.8513e-02, -2.2595e-01]],

        ...,

        [[-4.7302e-04,  2.3987e-02,  9.4986e-03,  ...,  1.5450e-04,
           2.3224e-02, -1.5587e-02],
         [ 3.8696e-02,  2.2156e-02, -2.6352e-02,  ..., -9.4376e-03,
           3.4424e-02, -4.7119e-02]],

        [[ 1.3831e-01,  5.0293e-02, -1.4758e-01,  ..., -4.6783e-02,
           2.9053e-01, -2.2339e-01],
         [ 1.6638e-01, -1.1725e-01,  2.0032e-01,  ...,  3.3539e-02,
           5.3177e-03, -1.6211e-01]],

        [[ 2.6855e-01, -3.4570e-01, -4.5801e-01,  ...,  1.9775e-02,
           2.4731e-01, -1.9067e-01],
         [ 2.9877e-02,  4.1565e-02, -1.4099e-02,  ...,  7.8049e-03,
           2.4323e-02, -4.6448e-02]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<CheckpointFunctionBackward>)]
[['elife-21937-v2_Sec2', 'has_title', 'Discussion'], ['elife-21937-v2', 'has_title', 'Peptidoglycan sensing by octopaminergic neurons modulates Drosophila oviposition'], ['elife-21937-v2_Sec0', 'has_title', 'Introduction'], ['elife-21937-v2_Sec3', 'has_title', 'Material and methods'], ['elife-21937-v2_Sec1', 'has_title', 'Results']]
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
IDs:  tensor([2477,  849], device='cuda:1')
ARGS:  TransformerConfig(_name='bart_large', activation_fn='gelu', dropout=0.1, attention_dropout=0.0, activation_dropout=0.0, adaptive_input=False, encoder=EncDecBaseConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None), max_source_positions=16384, decoder=DecoderConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None, input_dim=1024, output_dim=1024), max_target_positions=1024, share_decoder_input_output_embed=True, share_all_embeddings=True, no_token_positional_embeddings=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, layernorm_embedding=True, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=True, checkpoint_activations=True, offload_activations=False, no_cross_attention=False, cross_self_attention=False, quant_noise=QuantNoiseConfig(_name='bart_large', pq=0, pq_block_size=8, scalar=0), min_params_to_wrap=100000000, char_inputs=False, relu_dropout=0.0, base_layers=0, base_sublayers=1, base_shuffle=1, export=False, no_decoder_final_norm=False, alibi=False, truncate_alibi=None, use_xformers=False, attention_name='block_noglobal', xformer_config='{}', pooling_layers=4)
Enc out shape:  3
Enc out:  [tensor([[[-0.0081,  0.0334,  0.0259,  ...,  0.0124, -0.0043, -0.0247],
         [-0.0048, -0.0158, -0.0020,  ...,  0.0249, -0.0209, -0.0426]],

        [[ 0.0725, -0.4465,  0.2155,  ...,  0.0075, -0.0146,  0.3955],
         [-0.0161, -0.4138, -0.5991,  ..., -0.0642,  0.0404,  0.3157]],

        [[-0.0676, -0.7090,  0.4009,  ..., -0.0359, -0.2615, -0.0783],
         [ 0.0255, -0.4041, -0.1301,  ...,  0.1677, -0.0688,  0.5312]],

        ...,

        [[ 0.1388, -0.4316,  0.1162,  ...,  0.3882, -0.0046,  0.0279],
         [-0.0056, -0.6733,  0.0544,  ...,  0.1626, -0.0153,  0.0947]],

        [[ 0.0319, -0.5098,  0.0231,  ...,  0.3843, -0.1746,  0.0889],
         [-0.1481, -0.5757,  0.1042,  ...,  0.1578,  0.0920, -0.0336]],

        [[-0.0137,  0.6904,  0.0820,  ...,  0.0895, -0.0233,  0.1157],
         [-0.0132,  0.6919,  0.0988,  ...,  0.0573,  0.0109,  0.0428]]],
       device='cuda:1', dtype=torch.float16,
       grad_fn=<CheckpointFunctionBackward>)]
[['elife-05151-v2_Sec2', 'has_title', 'Discussion'], ['elife-05151-v2_Sec1', 'has_title', 'Results'], ['elife-05151-v2_Sec0', 'has_title', 'Introduction'], ['elife-05151-v2', 'has_title', 'Transient inhibition of the ERK pathway prevents cerebellar developmental defects and improves long-term motor functions in murine models of neurofibromatosis type 1'], ['elife-05151-v2_Sec3', 'has_title', 'Materials and methods']]
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
IDs:  tensor([ 122, 4235], device='cuda:3')
ARGS:  TransformerConfig(_name='bart_large', activation_fn='gelu', dropout=0.1, attention_dropout=0.0, activation_dropout=0.0, adaptive_input=False, encoder=EncDecBaseConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None), max_source_positions=16384, decoder=DecoderConfig(_name='bart_large', embed_path=None, embed_dim=1024, ffn_embed_dim=4096, layers=12, attention_heads=16, normalize_before=False, learned_pos=True, layerdrop=0, layers_to_keep=None, input_dim=1024, output_dim=1024), max_target_positions=1024, share_decoder_input_output_embed=True, share_all_embeddings=True, no_token_positional_embeddings=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, adaptive_softmax_factor=4, layernorm_embedding=True, tie_adaptive_weights=False, tie_adaptive_proj=False, no_scale_embedding=True, checkpoint_activations=True, offload_activations=False, no_cross_attention=False, cross_self_attention=False, quant_noise=QuantNoiseConfig(_name='bart_large', pq=0, pq_block_size=8, scalar=0), min_params_to_wrap=100000000, char_inputs=False, relu_dropout=0.0, base_layers=0, base_sublayers=1, base_shuffle=1, export=False, no_decoder_final_norm=False, alibi=False, truncate_alibi=None, use_xformers=False, attention_name='block_noglobal', xformer_config='{}', pooling_layers=4)
Enc out shape:  3
Enc out:  [tensor([[[-9.5825e-03,  3.5675e-02,  2.7832e-02,  ...,  1.3374e-02,
          -8.2321e-03, -2.4567e-02],
         [-1.3443e-02, -1.1520e-02, -2.3087e-02,  ...,  1.8799e-02,
          -1.2489e-02, -4.6112e-02]],

        [[ 1.7834e-01,  4.9951e-01,  1.1243e-01,  ...,  1.7349e-02,
          -1.0486e-01,  2.4365e-01],
         [-5.6061e-02, -1.0498e+00,  6.6211e-01,  ..., -2.3291e-01,
           9.8572e-02, -1.2140e-01]],

        [[ 7.3486e-02,  3.4155e-01,  1.2457e-01,  ..., -5.3497e-02,
          -1.0809e-01,  3.7158e-01],
         [ 8.6792e-02, -8.2764e-01,  5.0586e-01,  ..., -9.8206e-02,
          -2.9182e-03, -7.8125e-02]],

        ...,

        [[ 2.3712e-02, -5.0659e-02, -5.8228e-02,  ...,  3.3722e-02,
           5.0690e-02,  2.1255e-02],
         [-2.7504e-03,  1.8860e-02,  2.0920e-02,  ...,  1.0826e-02,
           4.1275e-03, -9.0027e-03]],

        [[-6.0272e-04,  1.8234e-02,  1.9562e-02,  ...,  1.2909e-02,
           5.1880e-04, -7.1106e-03],
         [-2.3766e-03,  2.0798e-02,  1.9562e-02,  ...,  1.0719e-02,
           1.2360e-03, -1.0574e-02]],

        [[-8.1253e-03,  2.9846e-02,  2.5696e-02,  ...,  1.5457e-02,
           7.2174e-03, -5.1804e-03],
         [-3.5973e-03,  2.0218e-02,  1.8234e-02,  ...,  8.2016e-03,
          -1.6785e-04, -1.0941e-02]]], device='cuda:3', dtype=torch.float16,
       grad_fn=<CheckpointFunctionBackward>)]
[['elife-02667-v1', 'has_title', 'Molecular mechanism of Aurora A kinase autophosphorylation and its allosteric activation by TPX2'], ['elife-02667-v1_Sec2', 'has_title', 'Materials and methods'], ['elife-02667-v1_Sec0', 'has_title', 'Introduction'], ['elife-02667-v1_Sec1', 'has_title', 'Results and discussion']]
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
aid  torch.Size([50])
Traceback (most recent call last):
  File "train.py", line 13, in <module>
    cli_main()
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq_cli/train.py", line 505, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq_cli/train.py", line 178, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq_cli/train.py", line 289, in train
    log_output = trainer.train_step(samples)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/trainer.py", line 755, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion("train", model, sample)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(split, sample['id'], **sample["net_input"])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/distributed/module_proxy_wrapper.py", line 55, in forward
    return self.module(*args, **kwargs)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/models/bart/model.py", line 691, in forward
    graph_encoder_out = self.graph_encoder.forward(aid, split, device)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/models/bart/model.py", line 512, in forward
    G = self.get_graph(final_nodes, final_edges).to(device)
  File "/home/acp20tg/bart_ls/fairseq-py/fairseq/models/bart/model.py", line 464, in get_graph
    graph_data[(NODE, edge_type, NODE)] = (torch.tensor(edgetype2tensor1[edge_type]),
NameError: name 'NODE' is not defined

/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
