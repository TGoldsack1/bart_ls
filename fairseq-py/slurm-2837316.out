2022-12-15 19:28:33 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10759
2022-12-15 19:28:33 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10759
2022-12-15 19:28:33 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10759
2022-12-15 19:28:33 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10759
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-15 19:28:34 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 0
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-15 19:28:34 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 2
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-15 19:28:34 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 1
2022-12-15 19:28:34 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2022-12-15 19:28:34 | INFO | fairseq.distributed.utils | initialized host gpu-node012.shef.ac.uk as rank 3
2022-12-15 19:28:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 3, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10759', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 4, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 2, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1024, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 2, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '../checkpoints/model_100k.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'rouge_avg', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bart_large', activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bart_large', attention_dropout=0.0, attention_name='block_noglobal', azureml_logging=False, batch_size=2, batch_size_valid=2, best_checkpoint_metric='rouge_avg', bf16=False, block_attention=False, block_size=1024, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=True, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=True, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, custom_dict='../checkpoints/dict.txt', data='/home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, dual_graph_encoder=False, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_rouge=True, eval_rouge_args='{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', eval_rouge_detok='space', eval_rouge_detok_args='{}', eval_rouge_print_samples=False, eval_rouge_remove_bpe=None, fast_stat_sync=True, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, input_pattern='concat', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.0, layernorm_embedding=True, left_pad_source=False, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0001], lr_scheduler='polynomial_decay', max_epoch=50, max_query_positions=50, max_source_positions=16384, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=True, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=4, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, pad_query=0, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, pooling_layers=4, power=1.0, profile=False, quantization_config_path=None, query_based=False, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1024, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='../checkpoints/model_100k.pt', restrict_position_embed=False, save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=3, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, sliding_window=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='src', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='tgt', task='summarization', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, top_down=False, total_num_update='10000', tpu=False, train_subset='train', truncate_source=True, truncate_target=True, unk=3, update_freq=[4], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_xformers=True, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=500, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'summarization', 'data': '/home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin', 'source_lang': 'src', 'target_lang': 'tgt', 'load_alignments': False, 'left_pad_source': False, 'left_pad_target': False, 'max_source_positions': 16384, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': True, 'truncate_target': True, 'query_lang': 'query', 'query_based': False, 'max_query_positions': 50, 'pad_query': 0, 'input_pattern': 'concat', 'block_size': 1024, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1024, 'eval_rouge': True, 'eval_rouge_args': '{"beam": 4, "max_len_b": 700, "lenpen": 2.0, "no_repeat_ngram_size": 3, "min_len": 20}', 'eval_rouge_detok': 'space', 'eval_rouge_detok_args': '{}', 'eval_rouge_remove_bpe': None, 'eval_rouge_print_samples': False, 'custom_dict': '../checkpoints/dict.txt'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.0, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 500, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-12-15 19:28:41 | INFO | fairseq.tasks.summarization | [src] dictionary: 50606 types
2022-12-15 19:28:41 | INFO | fairseq.tasks.summarization | [tgt] dictionary: 50606 types
2022-12-15 19:29:06 | INFO | fairseq_cli.train | BARTModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(16386, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (9): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (10): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
      (11): PoolEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (attention): BlockNoglobalAttention(
            (drop_attn): Dropout(p=0.0, inplace=False)
          )
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (top_pool): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (top_pool_mask): AvgPool1d(kernel_size=(18,), stride=(12,), padding=(9,))
        (pool_attn): MultiheadAttentionNoProj(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50606, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50606, bias=False)
  )
  (classification_heads): ModuleDict()
)
2022-12-15 19:29:06 | INFO | fairseq_cli.train | task: SummarizationTask
2022-12-15 19:29:06 | INFO | fairseq_cli.train | model: BARTModel
2022-12-15 19:29:06 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-12-15 19:29:06 | INFO | fairseq_cli.train | num. shared model params: 439,162,880 (num. trained: 439,162,880)
2022-12-15 19:29:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-12-15 19:29:06 | INFO | fairseq.data.data_utils | loaded 241 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin/valid.src-tgt.src
2022-12-15 19:29:06 | INFO | fairseq.data.data_utils | loaded 241 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin/valid.src-tgt.tgt
2022-12-15 19:29:06 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin valid src-tgt 241 examples
2022-12-15 19:29:07 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2022-12-15 19:29:07 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2022-12-15 19:29:07 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-12-15 19:29:07 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-12-15 19:29:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-15 19:29:09 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-15 19:29:09 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-15 19:29:09 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-15 19:29:09 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 79.210 GB ; name = NVIDIA A100-SXM4-80GB                   
2022-12-15 19:29:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2022-12-15 19:29:09 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2022-12-15 19:29:09 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 2
2022-12-15 19:29:09 | INFO | fairseq.trainer | Preparing to load checkpoint ../checkpoints/model_100k.pt
2022-12-15 19:29:42 | INFO | fairseq.optim.adam | using FusedAdam
2022-12-15 19:29:42 | INFO | fairseq.trainer | Loaded checkpoint ../checkpoints/model_100k.pt (epoch 1 @ 0 updates)
2022-12-15 19:29:42 | INFO | fairseq.trainer | loading train data for epoch 1
2022-12-15 19:29:43 | INFO | fairseq.data.data_utils | loaded 4,346 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin/train.src-tgt.src
2022-12-15 19:29:43 | INFO | fairseq.data.data_utils | loaded 4,346 examples from: /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin/train.src-tgt.tgt
2022-12-15 19:29:43 | INFO | fairseq.tasks.translation | /home/acp20tg/bart_ls/resources/eLife_fs-graph_text-bin train src-tgt 4346 examples
2022-12-15 19:29:43 | INFO | fairseq.trainer | begin training epoch 1
2022-12-15 19:29:43 | INFO | fairseq_cli.train | Start iterating over samples
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2022-12-15 19:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-12-15 19:30:37 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.
2022-12-15 19:30:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-12-15 19:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-12-15 19:31:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-12-15 19:31:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-12-15 19:31:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2022-12-15 19:31:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2022-12-15 19:31:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2022-12-15 19:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2022-12-15 19:33:02 | INFO | train_inner | {"epoch": 1, "update": 0.14, "loss": "4.477", "nll_loss": "4.477", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "22.27", "wps": "1300.1", "ups": "0.09", "wpb": "13957.8", "bsz": "32", "num_updates": "10", "lr": "2e-06", "gnorm": "15.661", "clip": "100", "loss_scale": "0.25", "train_wall": "153", "gb_free": "62.2", "wall": "233"}
2022-12-15 19:33:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2022-12-15 19:34:29 | INFO | train_inner | {"epoch": 1, "update": 0.221, "loss": "4.314", "nll_loss": "4.314", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "19.89", "wps": "1634", "ups": "0.12", "wpb": "14200.6", "bsz": "32", "num_updates": "20", "lr": "4e-06", "gnorm": "13.18", "clip": "100", "loss_scale": "0.125", "train_wall": "86", "gb_free": "62.2", "wall": "320"}
2022-12-15 19:35:50 | INFO | train_inner | {"epoch": 1, "update": 0.294, "loss": "3.962", "nll_loss": "3.962", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "15.58", "wps": "1715.5", "ups": "0.12", "wpb": "13874.2", "bsz": "32", "num_updates": "30", "lr": "6e-06", "gnorm": "7.91", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "401"}
2022-12-15 19:37:11 | INFO | train_inner | {"epoch": 1, "update": 0.368, "loss": "3.745", "nll_loss": "3.745", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "13.41", "wps": "1707.7", "ups": "0.12", "wpb": "13776.3", "bsz": "32", "num_updates": "40", "lr": "8e-06", "gnorm": "4.521", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "482"}
2022-12-15 19:38:31 | INFO | train_inner | {"epoch": 1, "update": 0.441, "loss": "3.602", "nll_loss": "3.602", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "12.15", "wps": "1725.1", "ups": "0.12", "wpb": "13887.1", "bsz": "32", "num_updates": "50", "lr": "1e-05", "gnorm": "4.498", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "562"}
2022-12-15 19:39:53 | INFO | train_inner | {"epoch": 1, "update": 0.515, "loss": "3.514", "nll_loss": "3.514", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "11.43", "wps": "1721.3", "ups": "0.12", "wpb": "14002.7", "bsz": "32", "num_updates": "60", "lr": "1.2e-05", "gnorm": "2.739", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "644"}
2022-12-15 19:41:14 | INFO | train_inner | {"epoch": 1, "update": 0.588, "loss": "3.418", "nll_loss": "3.418", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.69", "wps": "1718.9", "ups": "0.12", "wpb": "13998.8", "bsz": "32", "num_updates": "70", "lr": "1.4e-05", "gnorm": "2.273", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "725"}
2022-12-15 19:42:36 | INFO | train_inner | {"epoch": 1, "update": 0.662, "loss": "3.41", "nll_loss": "3.41", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.63", "wps": "1732.6", "ups": "0.12", "wpb": "14159", "bsz": "32", "num_updates": "80", "lr": "1.6e-05", "gnorm": "2.391", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "64.3", "wall": "807"}
2022-12-15 19:43:51 | INFO | train_inner | {"epoch": 1, "update": 0.735, "loss": "3.398", "nll_loss": "3.398", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.54", "wps": "1837.6", "ups": "0.13", "wpb": "13874", "bsz": "32", "num_updates": "90", "lr": "1.8e-05", "gnorm": "3.766", "clip": "100", "loss_scale": "0.125", "train_wall": "75", "gb_free": "62.2", "wall": "882"}
2022-12-15 19:45:10 | INFO | train_inner | {"epoch": 1, "update": 0.809, "loss": "3.339", "nll_loss": "3.339", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.12", "wps": "1792.1", "ups": "0.13", "wpb": "14165.5", "bsz": "32", "num_updates": "100", "lr": "2e-05", "gnorm": "1.953", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.9", "wall": "961"}
2022-12-15 19:46:33 | INFO | train_inner | {"epoch": 1, "update": 0.882, "loss": "3.325", "nll_loss": "3.325", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "10.02", "wps": "1701.6", "ups": "0.12", "wpb": "13984.3", "bsz": "32", "num_updates": "110", "lr": "2.2e-05", "gnorm": "1.9", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "1044"}
2022-12-15 19:47:53 | INFO | train_inner | {"epoch": 1, "update": 0.956, "loss": "3.285", "nll_loss": "3.285", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.74", "wps": "1733.9", "ups": "0.12", "wpb": "13960.4", "bsz": "32", "num_updates": "120", "lr": "2.4e-05", "gnorm": "1.91", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "1124"}
2022-12-15 19:48:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 19:49:28 | INFO | absl | Using default tokenizer.
2022-12-15 19:50:10 | INFO | absl | Using default tokenizer.
2022-12-15 19:50:51 | INFO | absl | Using default tokenizer.
2022-12-15 19:51:35 | INFO | absl | Using default tokenizer.
2022-12-15 19:52:19 | INFO | absl | Using default tokenizer.
2022-12-15 19:53:03 | INFO | absl | Using default tokenizer.
2022-12-15 19:53:45 | INFO | absl | Using default tokenizer.
2022-12-15 19:54:28 | INFO | absl | Using default tokenizer.
2022-12-15 19:55:13 | INFO | absl | Using default tokenizer.
2022-12-15 19:55:58 | INFO | absl | Using default tokenizer.
2022-12-15 19:56:42 | INFO | absl | Using default tokenizer.
2022-12-15 19:57:27 | INFO | absl | Using default tokenizer.
2022-12-15 19:58:14 | INFO | absl | Using default tokenizer.
2022-12-15 19:58:57 | INFO | absl | Using default tokenizer.
2022-12-15 19:59:44 | INFO | absl | Using default tokenizer.
2022-12-15 20:00:32 | INFO | absl | Using default tokenizer.
2022-12-15 20:01:19 | INFO | absl | Using default tokenizer.
2022-12-15 20:02:08 | INFO | absl | Using default tokenizer.
2022-12-15 20:02:57 | INFO | absl | Using default tokenizer.
2022-12-15 20:03:45 | INFO | absl | Using default tokenizer.
2022-12-15 20:04:34 | INFO | absl | Using default tokenizer.
2022-12-15 20:05:26 | INFO | absl | Using default tokenizer.
2022-12-15 20:06:14 | INFO | absl | Using default tokenizer.
2022-12-15 20:07:03 | INFO | absl | Using default tokenizer.
2022-12-15 20:07:51 | INFO | absl | Using default tokenizer.
2022-12-15 20:08:39 | INFO | absl | Using default tokenizer.
2022-12-15 20:09:27 | INFO | absl | Using default tokenizer.
2022-12-15 20:10:16 | INFO | absl | Using default tokenizer.
2022-12-15 20:11:03 | INFO | absl | Using default tokenizer.
2022-12-15 20:11:51 | INFO | absl | Using default tokenizer.
2022-12-15 20:12:41 | INFO | absl | Using default tokenizer.
2022-12-15 20:13:21 | INFO | valid | {"epoch": 1, "valid_loss": "3.2", "valid_nll_loss": "3.2", "valid_rouge1": 0.2982858779433437, "valid_rouge2": 0.0629818127466586, "valid_rougel": 0.17365628902304445, "valid_rouge_avg": 0.18063384534500107, "valid_ppl": "9.19", "valid_wps": "73.3", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "126"}
2022-12-15 20:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 126 updates
2022-12-15 20:13:21 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 20:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 20:14:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 126 updates, score 0.18063384534500107) (writing took 54.02941659302451 seconds)
2022-12-15 20:14:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-12-15 20:14:15 | INFO | train | {"epoch": 1, "train_loss": "3.635", "train_nll_loss": "3.635", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "12.42", "train_wps": "676.3", "train_ups": "0.05", "train_wpb": "13899.8", "train_bsz": "31.8", "train_num_updates": "126", "train_lr": "2.52e-05", "train_gnorm": "5.083", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1090", "train_gb_free": "62.2", "train_wall": "2706"}
2022-12-15 20:14:15 | INFO | fairseq.trainer | begin training epoch 2
2022-12-15 20:14:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 20:15:17 | INFO | train_inner | {"epoch": 2, "update": 1.029, "loss": "3.27", "nll_loss": "3.27", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.64", "wps": "78.8", "ups": "0.01", "wpb": "12961.8", "bsz": "29.6", "num_updates": "130", "lr": "2.6e-05", "gnorm": "2.015", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "2768"}
2022-12-15 20:16:37 | INFO | train_inner | {"epoch": 2, "update": 1.103, "loss": "3.223", "nll_loss": "3.223", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.34", "wps": "1745.1", "ups": "0.13", "wpb": "13894.3", "bsz": "32", "num_updates": "140", "lr": "2.8e-05", "gnorm": "1.72", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63", "wall": "2848"}
2022-12-15 20:17:57 | INFO | train_inner | {"epoch": 2, "update": 1.176, "loss": "3.216", "nll_loss": "3.216", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.29", "wps": "1723.9", "ups": "0.12", "wpb": "13885.3", "bsz": "32", "num_updates": "150", "lr": "3e-05", "gnorm": "2.041", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "2928"}
2022-12-15 20:19:19 | INFO | train_inner | {"epoch": 2, "update": 1.25, "loss": "3.186", "nll_loss": "3.186", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.1", "wps": "1704.4", "ups": "0.12", "wpb": "13927.7", "bsz": "32", "num_updates": "160", "lr": "3.2e-05", "gnorm": "2.081", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "3010"}
2022-12-15 20:20:40 | INFO | train_inner | {"epoch": 2, "update": 1.324, "loss": "3.161", "nll_loss": "3.161", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.94", "wps": "1721.9", "ups": "0.12", "wpb": "13874.6", "bsz": "32", "num_updates": "170", "lr": "3.4e-05", "gnorm": "1.977", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3091"}
2022-12-15 20:22:00 | INFO | train_inner | {"epoch": 2, "update": 1.397, "loss": "3.158", "nll_loss": "3.158", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.92", "wps": "1729", "ups": "0.12", "wpb": "13914.6", "bsz": "32", "num_updates": "180", "lr": "3.6e-05", "gnorm": "1.754", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3171"}
2022-12-15 20:23:21 | INFO | train_inner | {"epoch": 2, "update": 1.471, "loss": "3.176", "nll_loss": "3.176", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9.04", "wps": "1746.9", "ups": "0.12", "wpb": "14062.8", "bsz": "32", "num_updates": "190", "lr": "3.8e-05", "gnorm": "1.644", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3252"}
2022-12-15 20:24:41 | INFO | train_inner | {"epoch": 2, "update": 1.544, "loss": "3.171", "nll_loss": "3.171", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9", "wps": "1746", "ups": "0.13", "wpb": "13947", "bsz": "32", "num_updates": "200", "lr": "4e-05", "gnorm": "1.564", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3332"}
2022-12-15 20:26:02 | INFO | train_inner | {"epoch": 2, "update": 1.618, "loss": "3.136", "nll_loss": "3.136", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.79", "wps": "1713.6", "ups": "0.12", "wpb": "13914.1", "bsz": "32", "num_updates": "210", "lr": "4.2e-05", "gnorm": "1.762", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "3413"}
2022-12-15 20:27:22 | INFO | train_inner | {"epoch": 2, "update": 1.691, "loss": "3.157", "nll_loss": "3.157", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.92", "wps": "1726", "ups": "0.12", "wpb": "13914.7", "bsz": "32", "num_updates": "220", "lr": "4.4e-05", "gnorm": "2.661", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3493"}
2022-12-15 20:28:44 | INFO | train_inner | {"epoch": 2, "update": 1.765, "loss": "3.143", "nll_loss": "3.143", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.83", "wps": "1696", "ups": "0.12", "wpb": "13879.7", "bsz": "32", "num_updates": "230", "lr": "4.6e-05", "gnorm": "1.947", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "3575"}
2022-12-15 20:30:05 | INFO | train_inner | {"epoch": 2, "update": 1.838, "loss": "3.126", "nll_loss": "3.126", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.73", "wps": "1733", "ups": "0.12", "wpb": "13924", "bsz": "32", "num_updates": "240", "lr": "4.8e-05", "gnorm": "2.164", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3656"}
2022-12-15 20:31:24 | INFO | train_inner | {"epoch": 2, "update": 1.912, "loss": "3.131", "nll_loss": "3.131", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.76", "wps": "1786.1", "ups": "0.13", "wpb": "14090.5", "bsz": "32", "num_updates": "250", "lr": "5e-05", "gnorm": "1.617", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "3735"}
2022-12-15 20:32:44 | INFO | train_inner | {"epoch": 2, "update": 1.985, "loss": "3.169", "nll_loss": "3.169", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "9", "wps": "1747.4", "ups": "0.12", "wpb": "14066.8", "bsz": "32", "num_updates": "260", "lr": "5.2e-05", "gnorm": "1.49", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "3815"}
2022-12-15 20:33:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 20:33:45 | INFO | absl | Using default tokenizer.
2022-12-15 20:34:25 | INFO | absl | Using default tokenizer.
2022-12-15 20:35:06 | INFO | absl | Using default tokenizer.
2022-12-15 20:35:51 | INFO | absl | Using default tokenizer.
2022-12-15 20:36:32 | INFO | absl | Using default tokenizer.
2022-12-15 20:37:13 | INFO | absl | Using default tokenizer.
2022-12-15 20:37:56 | INFO | absl | Using default tokenizer.
2022-12-15 20:38:40 | INFO | absl | Using default tokenizer.
2022-12-15 20:39:25 | INFO | absl | Using default tokenizer.
2022-12-15 20:40:04 | INFO | absl | Using default tokenizer.
2022-12-15 20:40:47 | INFO | absl | Using default tokenizer.
2022-12-15 20:41:34 | INFO | absl | Using default tokenizer.
2022-12-15 20:42:21 | INFO | absl | Using default tokenizer.
2022-12-15 20:43:05 | INFO | absl | Using default tokenizer.
2022-12-15 20:43:36 | INFO | absl | Using default tokenizer.
2022-12-15 20:44:21 | INFO | absl | Using default tokenizer.
2022-12-15 20:45:05 | INFO | absl | Using default tokenizer.
2022-12-15 20:45:43 | INFO | absl | Using default tokenizer.
2022-12-15 20:46:27 | INFO | absl | Using default tokenizer.
2022-12-15 20:47:15 | INFO | absl | Using default tokenizer.
2022-12-15 20:48:00 | INFO | absl | Using default tokenizer.
2022-12-15 20:48:48 | INFO | absl | Using default tokenizer.
2022-12-15 20:49:34 | INFO | absl | Using default tokenizer.
2022-12-15 20:50:19 | INFO | absl | Using default tokenizer.
2022-12-15 20:51:06 | INFO | absl | Using default tokenizer.
2022-12-15 20:51:50 | INFO | absl | Using default tokenizer.
2022-12-15 20:52:36 | INFO | absl | Using default tokenizer.
2022-12-15 20:53:22 | INFO | absl | Using default tokenizer.
2022-12-15 20:54:08 | INFO | absl | Using default tokenizer.
2022-12-15 20:54:43 | INFO | absl | Using default tokenizer.
2022-12-15 20:55:27 | INFO | absl | Using default tokenizer.
2022-12-15 20:55:56 | INFO | valid | {"epoch": 2, "valid_loss": "3.097", "valid_nll_loss": "3.097", "valid_rouge1": 0.3857633840607857, "valid_rouge2": 0.10059881724410885, "valid_rougel": 0.20093811806901624, "valid_rouge_avg": 0.2431811006524472, "valid_ppl": "8.56", "valid_wps": "79.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "262", "valid_best_rouge_avg": 0.2431811006524472}
2022-12-15 20:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 262 updates
2022-12-15 20:55:56 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 20:56:16 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 20:56:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 2 @ 262 updates, score 0.2431811006524472) (writing took 48.06511026399676 seconds)
2022-12-15 20:56:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-12-15 20:56:44 | INFO | train | {"epoch": 2, "train_loss": "3.167", "train_nll_loss": "3.167", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "8.98", "train_wps": "740.4", "train_ups": "0.05", "train_wpb": "13876.6", "train_bsz": "31.8", "train_num_updates": "262", "train_lr": "5.24e-05", "train_gnorm": "1.904", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "5255"}
2022-12-15 20:56:44 | INFO | fairseq.trainer | begin training epoch 3
2022-12-15 20:56:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 20:58:21 | INFO | train_inner | {"epoch": 3, "update": 2.059, "loss": "3.013", "nll_loss": "3.013", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.07", "wps": "82.7", "ups": "0.01", "wpb": "12717.9", "bsz": "29.6", "num_updates": "270", "lr": "5.4e-05", "gnorm": "2.047", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.9", "wall": "5353"}
2022-12-15 20:59:43 | INFO | train_inner | {"epoch": 3, "update": 2.132, "loss": "2.964", "nll_loss": "2.964", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.8", "wps": "1708.5", "ups": "0.12", "wpb": "13927.3", "bsz": "32", "num_updates": "280", "lr": "5.6e-05", "gnorm": "1.856", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "5434"}
2022-12-15 21:01:03 | INFO | train_inner | {"epoch": 3, "update": 2.206, "loss": "2.986", "nll_loss": "2.986", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.92", "wps": "1757.4", "ups": "0.13", "wpb": "14053.3", "bsz": "32", "num_updates": "290", "lr": "5.8e-05", "gnorm": "1.635", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "5514"}
2022-12-15 21:02:25 | INFO | train_inner | {"epoch": 3, "update": 2.279, "loss": "2.998", "nll_loss": "2.998", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.99", "wps": "1711.9", "ups": "0.12", "wpb": "14036.2", "bsz": "32", "num_updates": "300", "lr": "6e-05", "gnorm": "1.52", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "5596"}
2022-12-15 21:03:45 | INFO | train_inner | {"epoch": 3, "update": 2.353, "loss": "2.958", "nll_loss": "2.958", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.77", "wps": "1716.9", "ups": "0.12", "wpb": "13767.4", "bsz": "32", "num_updates": "310", "lr": "6.2e-05", "gnorm": "1.522", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "5676"}
2022-12-15 21:05:03 | INFO | train_inner | {"epoch": 3, "update": 2.426, "loss": "2.991", "nll_loss": "2.991", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.95", "wps": "1805.3", "ups": "0.13", "wpb": "14100.1", "bsz": "32", "num_updates": "320", "lr": "6.4e-05", "gnorm": "1.424", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "5754"}
2022-12-15 21:06:22 | INFO | train_inner | {"epoch": 3, "update": 2.5, "loss": "3", "nll_loss": "3", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8", "wps": "1771.4", "ups": "0.13", "wpb": "13983.1", "bsz": "32", "num_updates": "330", "lr": "6.6e-05", "gnorm": "1.439", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "5833"}
2022-12-15 21:07:43 | INFO | train_inner | {"epoch": 3, "update": 2.574, "loss": "2.995", "nll_loss": "2.995", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.97", "wps": "1733.4", "ups": "0.12", "wpb": "13922.1", "bsz": "32", "num_updates": "340", "lr": "6.8e-05", "gnorm": "1.475", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "5914"}
2022-12-15 21:09:04 | INFO | train_inner | {"epoch": 3, "update": 2.647, "loss": "3.02", "nll_loss": "3.02", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.11", "wps": "1780.6", "ups": "0.12", "wpb": "14452", "bsz": "32", "num_updates": "350", "lr": "7e-05", "gnorm": "1.524", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "5995"}
2022-12-15 21:10:25 | INFO | train_inner | {"epoch": 3, "update": 2.721, "loss": "2.987", "nll_loss": "2.987", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.93", "wps": "1697.3", "ups": "0.12", "wpb": "13792.7", "bsz": "32", "num_updates": "360", "lr": "7.2e-05", "gnorm": "1.994", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "6076"}
2022-12-15 21:11:43 | INFO | train_inner | {"epoch": 3, "update": 2.794, "loss": "3.024", "nll_loss": "3.024", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.13", "wps": "1744.2", "ups": "0.13", "wpb": "13608.2", "bsz": "32", "num_updates": "370", "lr": "7.4e-05", "gnorm": "1.46", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "63", "wall": "6154"}
2022-12-15 21:13:03 | INFO | train_inner | {"epoch": 3, "update": 2.868, "loss": "3.001", "nll_loss": "3.001", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "8.01", "wps": "1757.9", "ups": "0.12", "wpb": "14083.5", "bsz": "32", "num_updates": "380", "lr": "7.6e-05", "gnorm": "1.409", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "6234"}
2022-12-15 21:14:23 | INFO | train_inner | {"epoch": 3, "update": 2.941, "loss": "2.984", "nll_loss": "2.984", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.91", "wps": "1747.4", "ups": "0.12", "wpb": "13980.2", "bsz": "32", "num_updates": "390", "lr": "7.8e-05", "gnorm": "1.414", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "6314"}
2022-12-15 21:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 21:16:14 | INFO | absl | Using default tokenizer.
2022-12-15 21:16:57 | INFO | absl | Using default tokenizer.
2022-12-15 21:17:29 | INFO | absl | Using default tokenizer.
2022-12-15 21:18:12 | INFO | absl | Using default tokenizer.
2022-12-15 21:18:55 | INFO | absl | Using default tokenizer.
2022-12-15 21:19:36 | INFO | absl | Using default tokenizer.
2022-12-15 21:20:20 | INFO | absl | Using default tokenizer.
2022-12-15 21:21:02 | INFO | absl | Using default tokenizer.
2022-12-15 21:21:46 | INFO | absl | Using default tokenizer.
2022-12-15 21:22:29 | INFO | absl | Using default tokenizer.
2022-12-15 21:22:59 | INFO | absl | Using default tokenizer.
2022-12-15 21:23:43 | INFO | absl | Using default tokenizer.
2022-12-15 21:24:08 | INFO | absl | Using default tokenizer.
2022-12-15 21:24:55 | INFO | absl | Using default tokenizer.
2022-12-15 21:25:38 | INFO | absl | Using default tokenizer.
2022-12-15 21:26:24 | INFO | absl | Using default tokenizer.
2022-12-15 21:26:54 | INFO | absl | Using default tokenizer.
2022-12-15 21:27:26 | INFO | absl | Using default tokenizer.
2022-12-15 21:28:11 | INFO | absl | Using default tokenizer.
2022-12-15 21:28:56 | INFO | absl | Using default tokenizer.
2022-12-15 21:29:44 | INFO | absl | Using default tokenizer.
2022-12-15 21:30:16 | INFO | absl | Using default tokenizer.
2022-12-15 21:30:59 | INFO | absl | Using default tokenizer.
2022-12-15 21:31:34 | INFO | absl | Using default tokenizer.
2022-12-15 21:32:18 | INFO | absl | Using default tokenizer.
2022-12-15 21:32:52 | INFO | absl | Using default tokenizer.
2022-12-15 21:33:37 | INFO | absl | Using default tokenizer.
2022-12-15 21:34:26 | INFO | absl | Using default tokenizer.
2022-12-15 21:35:14 | INFO | absl | Using default tokenizer.
2022-12-15 21:35:57 | INFO | absl | Using default tokenizer.
2022-12-15 21:36:44 | INFO | absl | Using default tokenizer.
2022-12-15 21:37:10 | INFO | valid | {"epoch": 3, "valid_loss": "3.054", "valid_nll_loss": "3.054", "valid_rouge1": 0.41055952070378177, "valid_rouge2": 0.11152884532903977, "valid_rougel": 0.2121809502120997, "valid_rouge_avg": 0.26104418301641086, "valid_ppl": "8.31", "valid_wps": "84.4", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "398", "valid_best_rouge_avg": 0.26104418301641086}
2022-12-15 21:37:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 398 updates
2022-12-15 21:37:10 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 21:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 21:38:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 3 @ 398 updates, score 0.26104418301641086) (writing took 50.83171230903827 seconds)
2022-12-15 21:38:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-12-15 21:38:01 | INFO | train | {"epoch": 3, "train_loss": "2.992", "train_nll_loss": "2.992", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "7.96", "train_wps": "762.1", "train_ups": "0.05", "train_wpb": "13882.3", "train_bsz": "31.8", "train_num_updates": "398", "train_lr": "7.96e-05", "train_gnorm": "1.577", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "7732"}
2022-12-15 21:38:01 | INFO | fairseq.trainer | begin training epoch 4
2022-12-15 21:38:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 21:38:48 | INFO | train_inner | {"epoch": 4, "update": 3.015, "loss": "2.926", "nll_loss": "2.926", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.6", "wps": "87.7", "ups": "0.01", "wpb": "12842.2", "bsz": "29.6", "num_updates": "400", "lr": "8e-05", "gnorm": "1.794", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "7779"}
2022-12-15 21:40:08 | INFO | train_inner | {"epoch": 4, "update": 3.088, "loss": "2.796", "nll_loss": "2.796", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.94", "wps": "1786.2", "ups": "0.12", "wpb": "14353.8", "bsz": "32", "num_updates": "410", "lr": "8.2e-05", "gnorm": "1.485", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "7859"}
2022-12-15 21:41:28 | INFO | train_inner | {"epoch": 4, "update": 3.162, "loss": "2.775", "nll_loss": "2.775", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.85", "wps": "1763.1", "ups": "0.13", "wpb": "14087.2", "bsz": "32", "num_updates": "420", "lr": "8.4e-05", "gnorm": "1.396", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "7939"}
2022-12-15 21:42:48 | INFO | train_inner | {"epoch": 4, "update": 3.235, "loss": "2.784", "nll_loss": "2.784", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.89", "wps": "1731.2", "ups": "0.13", "wpb": "13775.6", "bsz": "32", "num_updates": "430", "lr": "8.6e-05", "gnorm": "1.499", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "8019"}
2022-12-15 21:44:07 | INFO | train_inner | {"epoch": 4, "update": 3.309, "loss": "2.763", "nll_loss": "2.763", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.79", "wps": "1751.8", "ups": "0.13", "wpb": "13778.4", "bsz": "32", "num_updates": "440", "lr": "8.8e-05", "gnorm": "1.415", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "8098"}
2022-12-15 21:45:26 | INFO | train_inner | {"epoch": 4, "update": 3.382, "loss": "2.777", "nll_loss": "2.777", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.85", "wps": "1720.7", "ups": "0.13", "wpb": "13749.4", "bsz": "32", "num_updates": "450", "lr": "9e-05", "gnorm": "1.425", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "8177"}
2022-12-15 21:46:46 | INFO | train_inner | {"epoch": 4, "update": 3.456, "loss": "2.789", "nll_loss": "2.789", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.91", "wps": "1763.4", "ups": "0.13", "wpb": "13983.5", "bsz": "32", "num_updates": "460", "lr": "9.2e-05", "gnorm": "1.574", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "8257"}
2022-12-15 21:48:06 | INFO | train_inner | {"epoch": 4, "update": 3.529, "loss": "2.793", "nll_loss": "2.793", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.93", "wps": "1736.3", "ups": "0.12", "wpb": "13932.6", "bsz": "32", "num_updates": "470", "lr": "9.4e-05", "gnorm": "1.399", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "8337"}
2022-12-15 21:49:26 | INFO | train_inner | {"epoch": 4, "update": 3.603, "loss": "2.801", "nll_loss": "2.801", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.97", "wps": "1788.3", "ups": "0.13", "wpb": "14249.9", "bsz": "32", "num_updates": "480", "lr": "9.6e-05", "gnorm": "1.365", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "8417"}
2022-12-15 21:50:47 | INFO | train_inner | {"epoch": 4, "update": 3.676, "loss": "2.825", "nll_loss": "2.825", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.08", "wps": "1710.9", "ups": "0.12", "wpb": "13849.6", "bsz": "32", "num_updates": "490", "lr": "9.8e-05", "gnorm": "1.406", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "8498"}
2022-12-15 21:52:07 | INFO | train_inner | {"epoch": 4, "update": 3.75, "loss": "2.804", "nll_loss": "2.804", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.98", "wps": "1725.3", "ups": "0.12", "wpb": "13919.1", "bsz": "32", "num_updates": "500", "lr": "0.0001", "gnorm": "1.48", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "8578"}
2022-12-15 21:53:28 | INFO | train_inner | {"epoch": 4, "update": 3.824, "loss": "2.822", "nll_loss": "2.822", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.07", "wps": "1699.8", "ups": "0.12", "wpb": "13650", "bsz": "32", "num_updates": "510", "lr": "9.98947e-05", "gnorm": "1.357", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "8659"}
2022-12-15 21:54:48 | INFO | train_inner | {"epoch": 4, "update": 3.897, "loss": "2.818", "nll_loss": "2.818", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.05", "wps": "1737.5", "ups": "0.12", "wpb": "13965.9", "bsz": "32", "num_updates": "520", "lr": "9.97895e-05", "gnorm": "1.469", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "8739"}
2022-12-15 21:56:08 | INFO | train_inner | {"epoch": 4, "update": 3.971, "loss": "2.814", "nll_loss": "2.814", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "7.03", "wps": "1780.9", "ups": "0.12", "wpb": "14285.2", "bsz": "32", "num_updates": "530", "lr": "9.96842e-05", "gnorm": "1.34", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "8819"}
2022-12-15 21:56:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 21:57:25 | INFO | absl | Using default tokenizer.
2022-12-15 21:57:52 | INFO | absl | Using default tokenizer.
2022-12-15 21:58:33 | INFO | absl | Using default tokenizer.
2022-12-15 21:59:10 | INFO | absl | Using default tokenizer.
2022-12-15 21:59:34 | INFO | absl | Using default tokenizer.
2022-12-15 22:00:15 | INFO | absl | Using default tokenizer.
2022-12-15 22:00:57 | INFO | absl | Using default tokenizer.
2022-12-15 22:01:36 | INFO | absl | Using default tokenizer.
2022-12-15 22:02:18 | INFO | absl | Using default tokenizer.
2022-12-15 22:02:44 | INFO | absl | Using default tokenizer.
2022-12-15 22:03:16 | INFO | absl | Using default tokenizer.
2022-12-15 22:03:44 | INFO | absl | Using default tokenizer.
2022-12-15 22:04:13 | INFO | absl | Using default tokenizer.
2022-12-15 22:04:58 | INFO | absl | Using default tokenizer.
2022-12-15 22:05:40 | INFO | absl | Using default tokenizer.
2022-12-15 22:06:25 | INFO | absl | Using default tokenizer.
2022-12-15 22:06:57 | INFO | absl | Using default tokenizer.
2022-12-15 22:07:24 | INFO | absl | Using default tokenizer.
2022-12-15 22:07:53 | INFO | absl | Using default tokenizer.
2022-12-15 22:08:25 | INFO | absl | Using default tokenizer.
2022-12-15 22:08:53 | INFO | absl | Using default tokenizer.
2022-12-15 22:09:41 | INFO | absl | Using default tokenizer.
2022-12-15 22:10:11 | INFO | absl | Using default tokenizer.
2022-12-15 22:10:37 | INFO | absl | Using default tokenizer.
2022-12-15 22:11:21 | INFO | absl | Using default tokenizer.
2022-12-15 22:11:49 | INFO | absl | Using default tokenizer.
2022-12-15 22:12:34 | INFO | absl | Using default tokenizer.
2022-12-15 22:13:19 | INFO | absl | Using default tokenizer.
2022-12-15 22:14:05 | INFO | absl | Using default tokenizer.
2022-12-15 22:14:34 | INFO | absl | Using default tokenizer.
2022-12-15 22:15:23 | INFO | absl | Using default tokenizer.
2022-12-15 22:15:57 | INFO | valid | {"epoch": 4, "valid_loss": "3.05", "valid_nll_loss": "3.05", "valid_rouge1": 0.4446238744570717, "valid_rouge2": 0.12580313443434965, "valid_rougel": 0.21922259978142533, "valid_rouge_avg": 0.28521350444571075, "valid_ppl": "8.28", "valid_wps": "95.1", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "534", "valid_best_rouge_avg": 0.28521350444571075}
2022-12-15 22:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 534 updates
2022-12-15 22:15:57 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 22:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 22:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 534 updates, score 0.28521350444571075) (writing took 42.807496062945575 seconds)
2022-12-15 22:16:40 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-12-15 22:16:40 | INFO | train | {"epoch": 4, "train_loss": "2.796", "train_nll_loss": "2.796", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "6.95", "train_wps": "813.9", "train_ups": "0.06", "train_wpb": "13876.1", "train_bsz": "31.8", "train_num_updates": "534", "train_lr": "9.96421e-05", "train_gnorm": "1.456", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1084", "train_gb_free": "63.7", "train_wall": "10051"}
2022-12-15 22:16:40 | INFO | fairseq.trainer | begin training epoch 5
2022-12-15 22:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 22:18:01 | INFO | train_inner | {"epoch": 5, "update": 4.044, "loss": "2.667", "nll_loss": "2.667", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.35", "wps": "97.5", "ups": "0.01", "wpb": "12799.6", "bsz": "29.6", "num_updates": "540", "lr": "9.95789e-05", "gnorm": "1.85", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "10132"}
2022-12-15 22:19:21 | INFO | train_inner | {"epoch": 5, "update": 4.118, "loss": "2.527", "nll_loss": "2.527", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.76", "wps": "1788.2", "ups": "0.13", "wpb": "14215.6", "bsz": "32", "num_updates": "550", "lr": "9.94737e-05", "gnorm": "1.453", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "10212"}
2022-12-15 22:20:40 | INFO | train_inner | {"epoch": 5, "update": 4.191, "loss": "2.529", "nll_loss": "2.529", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.77", "wps": "1773.8", "ups": "0.13", "wpb": "13964.9", "bsz": "32", "num_updates": "560", "lr": "9.93684e-05", "gnorm": "1.345", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "10291"}
2022-12-15 22:22:01 | INFO | train_inner | {"epoch": 5, "update": 4.265, "loss": "2.539", "nll_loss": "2.539", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.81", "wps": "1704.4", "ups": "0.12", "wpb": "13852.4", "bsz": "32", "num_updates": "570", "lr": "9.92632e-05", "gnorm": "1.401", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "10372"}
2022-12-15 22:23:20 | INFO | train_inner | {"epoch": 5, "update": 4.338, "loss": "2.511", "nll_loss": "2.511", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.7", "wps": "1756.1", "ups": "0.13", "wpb": "13910.3", "bsz": "32", "num_updates": "580", "lr": "9.91579e-05", "gnorm": "1.358", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "10451"}
2022-12-15 22:24:40 | INFO | train_inner | {"epoch": 5, "update": 4.412, "loss": "2.55", "nll_loss": "2.55", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.86", "wps": "1738.5", "ups": "0.13", "wpb": "13826.4", "bsz": "32", "num_updates": "590", "lr": "9.90526e-05", "gnorm": "1.398", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "10531"}
2022-12-15 22:26:00 | INFO | train_inner | {"epoch": 5, "update": 4.485, "loss": "2.559", "nll_loss": "2.559", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.89", "wps": "1746.9", "ups": "0.12", "wpb": "13979.9", "bsz": "32", "num_updates": "600", "lr": "9.89474e-05", "gnorm": "1.376", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "10611"}
2022-12-15 22:27:20 | INFO | train_inner | {"epoch": 5, "update": 4.559, "loss": "2.541", "nll_loss": "2.541", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.82", "wps": "1732.4", "ups": "0.13", "wpb": "13840.2", "bsz": "32", "num_updates": "610", "lr": "9.88421e-05", "gnorm": "1.415", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "10691"}
2022-12-15 22:28:41 | INFO | train_inner | {"epoch": 5, "update": 4.632, "loss": "2.562", "nll_loss": "2.562", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.9", "wps": "1710.8", "ups": "0.12", "wpb": "13934.1", "bsz": "32", "num_updates": "620", "lr": "9.87368e-05", "gnorm": "1.394", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "10772"}
2022-12-15 22:30:03 | INFO | train_inner | {"epoch": 5, "update": 4.706, "loss": "2.594", "nll_loss": "2.594", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.04", "wps": "1708.4", "ups": "0.12", "wpb": "13893.2", "bsz": "32", "num_updates": "630", "lr": "9.86316e-05", "gnorm": "1.39", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "10854"}
2022-12-15 22:31:22 | INFO | train_inner | {"epoch": 5, "update": 4.779, "loss": "2.6", "nll_loss": "2.6", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.06", "wps": "1785.5", "ups": "0.13", "wpb": "14237.3", "bsz": "32", "num_updates": "640", "lr": "9.85263e-05", "gnorm": "1.321", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "10933"}
2022-12-15 22:32:43 | INFO | train_inner | {"epoch": 5, "update": 4.853, "loss": "2.589", "nll_loss": "2.589", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.02", "wps": "1783.7", "ups": "0.12", "wpb": "14313.9", "bsz": "32", "num_updates": "650", "lr": "9.84211e-05", "gnorm": "1.354", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "11014"}
2022-12-15 22:34:05 | INFO | train_inner | {"epoch": 5, "update": 4.926, "loss": "2.589", "nll_loss": "2.589", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.02", "wps": "1679.3", "ups": "0.12", "wpb": "13783.2", "bsz": "32", "num_updates": "660", "lr": "9.83158e-05", "gnorm": "1.362", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "11096"}
2022-12-15 22:35:26 | INFO | train_inner | {"epoch": 5, "update": 5.0, "loss": "2.594", "nll_loss": "2.594", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "6.04", "wps": "1557.4", "ups": "0.12", "wpb": "12587.5", "bsz": "29.6", "num_updates": "670", "lr": "9.82105e-05", "gnorm": "1.729", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "11177"}
2022-12-15 22:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 22:36:10 | INFO | absl | Using default tokenizer.
2022-12-15 22:36:51 | INFO | absl | Using default tokenizer.
2022-12-15 22:37:32 | INFO | absl | Using default tokenizer.
2022-12-15 22:37:58 | INFO | absl | Using default tokenizer.
2022-12-15 22:38:27 | INFO | absl | Using default tokenizer.
2022-12-15 22:38:55 | INFO | absl | Using default tokenizer.
2022-12-15 22:39:32 | INFO | absl | Using default tokenizer.
2022-12-15 22:40:15 | INFO | absl | Using default tokenizer.
2022-12-15 22:40:59 | INFO | absl | Using default tokenizer.
2022-12-15 22:41:41 | INFO | absl | Using default tokenizer.
2022-12-15 22:42:08 | INFO | absl | Using default tokenizer.
2022-12-15 22:42:40 | INFO | absl | Using default tokenizer.
2022-12-15 22:43:08 | INFO | absl | Using default tokenizer.
2022-12-15 22:43:52 | INFO | absl | Using default tokenizer.
2022-12-15 22:44:37 | INFO | absl | Using default tokenizer.
2022-12-15 22:45:19 | INFO | absl | Using default tokenizer.
2022-12-15 22:46:04 | INFO | absl | Using default tokenizer.
2022-12-15 22:46:31 | INFO | absl | Using default tokenizer.
2022-12-15 22:47:04 | INFO | absl | Using default tokenizer.
2022-12-15 22:47:35 | INFO | absl | Using default tokenizer.
2022-12-15 22:48:10 | INFO | absl | Using default tokenizer.
2022-12-15 22:48:52 | INFO | absl | Using default tokenizer.
2022-12-15 22:49:20 | INFO | absl | Using default tokenizer.
2022-12-15 22:49:45 | INFO | absl | Using default tokenizer.
2022-12-15 22:50:29 | INFO | absl | Using default tokenizer.
2022-12-15 22:51:12 | INFO | absl | Using default tokenizer.
2022-12-15 22:51:43 | INFO | absl | Using default tokenizer.
2022-12-15 22:52:29 | INFO | absl | Using default tokenizer.
2022-12-15 22:53:15 | INFO | absl | Using default tokenizer.
2022-12-15 22:53:42 | INFO | absl | Using default tokenizer.
2022-12-15 22:54:28 | INFO | absl | Using default tokenizer.
2022-12-15 22:54:56 | INFO | valid | {"epoch": 5, "valid_loss": "3.057", "valid_nll_loss": "3.057", "valid_rouge1": 0.4557612945582341, "valid_rouge2": 0.13189515150858716, "valid_rougel": 0.22520103132936745, "valid_rouge_avg": 0.2938282230334106, "valid_ppl": "8.32", "valid_wps": "94.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "670", "valid_best_rouge_avg": 0.2938282230334106}
2022-12-15 22:54:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 670 updates
2022-12-15 22:54:56 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 22:55:16 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 22:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 5 @ 670 updates, score 0.2938282230334106) (writing took 51.44933883694466 seconds)
2022-12-15 22:55:47 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-12-15 22:55:47 | INFO | train | {"epoch": 5, "train_loss": "2.562", "train_nll_loss": "2.562", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "5.9", "train_wps": "803.9", "train_ups": "0.06", "train_wpb": "13875.7", "train_bsz": "31.8", "train_num_updates": "670", "train_lr": "9.82105e-05", "train_gnorm": "1.413", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "12398"}
2022-12-15 22:55:47 | INFO | fairseq.trainer | begin training epoch 6
2022-12-15 22:55:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 22:57:38 | INFO | train_inner | {"epoch": 6, "update": 5.074, "loss": "2.277", "nll_loss": "2.277", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.85", "wps": "105.8", "ups": "0.01", "wpb": "14092.4", "bsz": "32", "num_updates": "680", "lr": "9.81053e-05", "gnorm": "1.371", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "12509"}
2022-12-15 22:58:59 | INFO | train_inner | {"epoch": 6, "update": 5.147, "loss": "2.29", "nll_loss": "2.29", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.89", "wps": "1744.2", "ups": "0.12", "wpb": "14094.8", "bsz": "32", "num_updates": "690", "lr": "9.8e-05", "gnorm": "1.324", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "12590"}
2022-12-15 23:00:19 | INFO | train_inner | {"epoch": 6, "update": 5.221, "loss": "2.295", "nll_loss": "2.295", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.91", "wps": "1749.2", "ups": "0.12", "wpb": "14151.7", "bsz": "32", "num_updates": "700", "lr": "9.78947e-05", "gnorm": "1.36", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "12671"}
2022-12-15 23:01:40 | INFO | train_inner | {"epoch": 6, "update": 5.294, "loss": "2.303", "nll_loss": "2.303", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.93", "wps": "1693.5", "ups": "0.12", "wpb": "13618.1", "bsz": "32", "num_updates": "710", "lr": "9.77895e-05", "gnorm": "1.416", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "12751"}
2022-12-15 23:03:00 | INFO | train_inner | {"epoch": 6, "update": 5.368, "loss": "2.283", "nll_loss": "2.283", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.87", "wps": "1745.1", "ups": "0.13", "wpb": "13918.8", "bsz": "32", "num_updates": "720", "lr": "9.76842e-05", "gnorm": "1.416", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "12831"}
2022-12-15 23:04:20 | INFO | train_inner | {"epoch": 6, "update": 5.441, "loss": "2.322", "nll_loss": "2.322", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5", "wps": "1724.4", "ups": "0.12", "wpb": "13814.4", "bsz": "32", "num_updates": "730", "lr": "9.75789e-05", "gnorm": "1.38", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "12911"}
2022-12-15 23:05:41 | INFO | train_inner | {"epoch": 6, "update": 5.515, "loss": "2.341", "nll_loss": "2.341", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.07", "wps": "1708.4", "ups": "0.12", "wpb": "13923.7", "bsz": "32", "num_updates": "740", "lr": "9.74737e-05", "gnorm": "4.935", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "12992"}
2022-12-15 23:07:02 | INFO | train_inner | {"epoch": 6, "update": 5.588, "loss": "2.356", "nll_loss": "2.356", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.12", "wps": "1781", "ups": "0.12", "wpb": "14298.3", "bsz": "32", "num_updates": "750", "lr": "9.73684e-05", "gnorm": "1.399", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "13073"}
2022-12-15 23:08:22 | INFO | train_inner | {"epoch": 6, "update": 5.662, "loss": "2.347", "nll_loss": "2.347", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.09", "wps": "1712.4", "ups": "0.12", "wpb": "13708", "bsz": "32", "num_updates": "760", "lr": "9.72632e-05", "gnorm": "1.43", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "13153"}
2022-12-15 23:09:42 | INFO | train_inner | {"epoch": 6, "update": 5.735, "loss": "2.343", "nll_loss": "2.343", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.07", "wps": "1748.5", "ups": "0.13", "wpb": "13964.4", "bsz": "32", "num_updates": "770", "lr": "9.71579e-05", "gnorm": "1.43", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "13233"}
2022-12-15 23:11:01 | INFO | train_inner | {"epoch": 6, "update": 5.809, "loss": "2.337", "nll_loss": "2.337", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.05", "wps": "1775.9", "ups": "0.13", "wpb": "14031.4", "bsz": "32", "num_updates": "780", "lr": "9.70526e-05", "gnorm": "1.319", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "13312"}
2022-12-15 23:12:20 | INFO | train_inner | {"epoch": 6, "update": 5.882, "loss": "2.333", "nll_loss": "2.333", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.04", "wps": "1735", "ups": "0.13", "wpb": "13816.4", "bsz": "32", "num_updates": "790", "lr": "9.69474e-05", "gnorm": "1.413", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "13391"}
2022-12-15 23:13:40 | INFO | train_inner | {"epoch": 6, "update": 5.956, "loss": "2.402", "nll_loss": "2.402", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "5.29", "wps": "1746.1", "ups": "0.13", "wpb": "13932", "bsz": "32", "num_updates": "800", "lr": "9.68421e-05", "gnorm": "5.612", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63.6", "wall": "13471"}
2022-12-15 23:14:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 23:15:14 | INFO | absl | Using default tokenizer.
2022-12-15 23:15:48 | INFO | absl | Using default tokenizer.
2022-12-15 23:16:29 | INFO | absl | Using default tokenizer.
2022-12-15 23:16:56 | INFO | absl | Using default tokenizer.
2022-12-15 23:17:20 | INFO | absl | Using default tokenizer.
2022-12-15 23:18:00 | INFO | absl | Using default tokenizer.
2022-12-15 23:18:45 | INFO | absl | Using default tokenizer.
2022-12-15 23:19:19 | INFO | absl | Using default tokenizer.
2022-12-15 23:20:01 | INFO | absl | Using default tokenizer.
2022-12-15 23:20:40 | INFO | absl | Using default tokenizer.
2022-12-15 23:21:12 | INFO | absl | Using default tokenizer.
2022-12-15 23:21:48 | INFO | absl | Using default tokenizer.
2022-12-15 23:22:23 | INFO | absl | Using default tokenizer.
2022-12-15 23:22:54 | INFO | absl | Using default tokenizer.
2022-12-15 23:23:30 | INFO | absl | Using default tokenizer.
2022-12-15 23:23:57 | INFO | absl | Using default tokenizer.
2022-12-15 23:24:41 | INFO | absl | Using default tokenizer.
2022-12-15 23:25:24 | INFO | absl | Using default tokenizer.
2022-12-15 23:25:54 | INFO | absl | Using default tokenizer.
2022-12-15 23:26:23 | INFO | absl | Using default tokenizer.
2022-12-15 23:27:00 | INFO | absl | Using default tokenizer.
2022-12-15 23:27:33 | INFO | absl | Using default tokenizer.
2022-12-15 23:28:08 | INFO | absl | Using default tokenizer.
2022-12-15 23:28:51 | INFO | absl | Using default tokenizer.
2022-12-15 23:29:35 | INFO | absl | Using default tokenizer.
2022-12-15 23:30:19 | INFO | absl | Using default tokenizer.
2022-12-15 23:30:55 | INFO | absl | Using default tokenizer.
2022-12-15 23:31:28 | INFO | absl | Using default tokenizer.
2022-12-15 23:32:12 | INFO | absl | Using default tokenizer.
2022-12-15 23:32:45 | INFO | absl | Using default tokenizer.
2022-12-15 23:33:13 | INFO | absl | Using default tokenizer.
2022-12-15 23:33:43 | INFO | valid | {"epoch": 6, "valid_loss": "3.111", "valid_nll_loss": "3.111", "valid_rouge1": 0.46885238240301597, "valid_rouge2": 0.13623076014663774, "valid_rougel": 0.2295914125910278, "valid_rouge_avg": 0.3025415712748268, "valid_ppl": "8.64", "valid_wps": "96.1", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "806", "valid_best_rouge_avg": 0.3025415712748268}
2022-12-15 23:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 806 updates
2022-12-15 23:33:43 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 23:34:00 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-15 23:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 806 updates, score 0.3025415712748268) (writing took 36.784663123078644 seconds)
2022-12-15 23:34:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-12-15 23:34:19 | INFO | train | {"epoch": 6, "train_loss": "2.33", "train_nll_loss": "2.33", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "5.03", "train_wps": "816.6", "train_ups": "0.06", "train_wpb": "13882.6", "train_bsz": "31.8", "train_num_updates": "806", "train_lr": "9.67789e-05", "train_gnorm": "2.277", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "14710"}
2022-12-15 23:34:19 | INFO | fairseq.trainer | begin training epoch 7
2022-12-15 23:34:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-15 23:35:27 | INFO | train_inner | {"epoch": 7, "update": 6.029, "loss": "2.302", "nll_loss": "2.302", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.93", "wps": "98.9", "ups": "0.01", "wpb": "12934.2", "bsz": "29.6", "num_updates": "810", "lr": "9.67368e-05", "gnorm": "9.481", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "14779"}
2022-12-15 23:36:47 | INFO | train_inner | {"epoch": 7, "update": 6.103, "loss": "2.084", "nll_loss": "2.084", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.24", "wps": "1779", "ups": "0.13", "wpb": "14186.5", "bsz": "32", "num_updates": "820", "lr": "9.66316e-05", "gnorm": "5.172", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "14858"}
2022-12-15 23:38:08 | INFO | train_inner | {"epoch": 7, "update": 6.176, "loss": "2.13", "nll_loss": "2.13", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.38", "wps": "1761.8", "ups": "0.12", "wpb": "14149.3", "bsz": "32", "num_updates": "830", "lr": "9.65263e-05", "gnorm": "19.055", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.9", "wall": "14939"}
2022-12-15 23:39:27 | INFO | train_inner | {"epoch": 7, "update": 6.25, "loss": "2.161", "nll_loss": "2.161", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.47", "wps": "1749.8", "ups": "0.13", "wpb": "13994", "bsz": "32", "num_updates": "840", "lr": "9.64211e-05", "gnorm": "17.811", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "15019"}
2022-12-15 23:40:48 | INFO | train_inner | {"epoch": 7, "update": 6.324, "loss": "2.084", "nll_loss": "2.084", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.24", "wps": "1736.9", "ups": "0.12", "wpb": "14013", "bsz": "32", "num_updates": "850", "lr": "9.63158e-05", "gnorm": "13.63", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "15099"}
2022-12-15 23:42:09 | INFO | train_inner | {"epoch": 7, "update": 6.397, "loss": "2.095", "nll_loss": "2.095", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.27", "wps": "1710.5", "ups": "0.12", "wpb": "13813.7", "bsz": "32", "num_updates": "860", "lr": "9.62105e-05", "gnorm": "13.549", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "15180"}
2022-12-15 23:43:31 | INFO | train_inner | {"epoch": 7, "update": 6.471, "loss": "2.127", "nll_loss": "2.127", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.37", "wps": "1695.5", "ups": "0.12", "wpb": "13959.2", "bsz": "32", "num_updates": "870", "lr": "9.61053e-05", "gnorm": "10.367", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "15262"}
2022-12-15 23:44:52 | INFO | train_inner | {"epoch": 7, "update": 6.544, "loss": "2.044", "nll_loss": "2.044", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.12", "wps": "1714.1", "ups": "0.12", "wpb": "13926", "bsz": "32", "num_updates": "880", "lr": "9.6e-05", "gnorm": "4.662", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "15344"}
2022-12-15 23:46:14 | INFO | train_inner | {"epoch": 7, "update": 6.618, "loss": "2.13", "nll_loss": "2.13", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.38", "wps": "1747.2", "ups": "0.12", "wpb": "14153.4", "bsz": "32", "num_updates": "890", "lr": "9.58947e-05", "gnorm": "10.41", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "15425"}
2022-12-15 23:47:32 | INFO | train_inner | {"epoch": 7, "update": 6.691, "loss": "2.099", "nll_loss": "2.099", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.28", "wps": "1775", "ups": "0.13", "wpb": "13955.6", "bsz": "32", "num_updates": "900", "lr": "9.57895e-05", "gnorm": "16.415", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "15503"}
2022-12-15 23:48:54 | INFO | train_inner | {"epoch": 7, "update": 6.765, "loss": "2.101", "nll_loss": "2.101", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.29", "wps": "1675.2", "ups": "0.12", "wpb": "13773.2", "bsz": "32", "num_updates": "910", "lr": "9.56842e-05", "gnorm": "9.684", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "15585"}
2022-12-15 23:50:14 | INFO | train_inner | {"epoch": 7, "update": 6.838, "loss": "2.109", "nll_loss": "2.109", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.31", "wps": "1770.2", "ups": "0.13", "wpb": "14062.5", "bsz": "32", "num_updates": "920", "lr": "9.55789e-05", "gnorm": "14.161", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "15665"}
2022-12-15 23:51:33 | INFO | train_inner | {"epoch": 7, "update": 6.912, "loss": "2.105", "nll_loss": "2.105", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.3", "wps": "1725.4", "ups": "0.13", "wpb": "13655", "bsz": "32", "num_updates": "930", "lr": "9.54737e-05", "gnorm": "5.45", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63", "wall": "15744"}
2022-12-15 23:52:55 | INFO | train_inner | {"epoch": 7, "update": 6.985, "loss": "2.165", "nll_loss": "2.165", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.49", "wps": "1686.6", "ups": "0.12", "wpb": "13893.7", "bsz": "32", "num_updates": "940", "lr": "9.53684e-05", "gnorm": "4.245", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "15826"}
2022-12-15 23:53:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-15 23:53:56 | INFO | absl | Using default tokenizer.
2022-12-15 23:54:25 | INFO | absl | Using default tokenizer.
2022-12-15 23:54:52 | INFO | absl | Using default tokenizer.
2022-12-15 23:55:20 | INFO | absl | Using default tokenizer.
2022-12-15 23:55:49 | INFO | absl | Using default tokenizer.
2022-12-15 23:56:16 | INFO | absl | Using default tokenizer.
2022-12-15 23:56:42 | INFO | absl | Using default tokenizer.
2022-12-15 23:57:11 | INFO | absl | Using default tokenizer.
2022-12-15 23:57:44 | INFO | absl | Using default tokenizer.
2022-12-15 23:58:11 | INFO | absl | Using default tokenizer.
2022-12-15 23:58:36 | INFO | absl | Using default tokenizer.
2022-12-15 23:59:04 | INFO | absl | Using default tokenizer.
2022-12-15 23:59:29 | INFO | absl | Using default tokenizer.
2022-12-15 23:59:56 | INFO | absl | Using default tokenizer.
2022-12-16 00:00:19 | INFO | absl | Using default tokenizer.
2022-12-16 00:00:43 | INFO | absl | Using default tokenizer.
2022-12-16 00:01:25 | INFO | absl | Using default tokenizer.
2022-12-16 00:01:50 | INFO | absl | Using default tokenizer.
2022-12-16 00:02:14 | INFO | absl | Using default tokenizer.
2022-12-16 00:02:41 | INFO | absl | Using default tokenizer.
2022-12-16 00:03:15 | INFO | absl | Using default tokenizer.
2022-12-16 00:03:44 | INFO | absl | Using default tokenizer.
2022-12-16 00:04:09 | INFO | absl | Using default tokenizer.
2022-12-16 00:04:36 | INFO | absl | Using default tokenizer.
2022-12-16 00:05:08 | INFO | absl | Using default tokenizer.
2022-12-16 00:05:37 | INFO | absl | Using default tokenizer.
2022-12-16 00:06:05 | INFO | absl | Using default tokenizer.
2022-12-16 00:06:33 | INFO | absl | Using default tokenizer.
2022-12-16 00:07:06 | INFO | absl | Using default tokenizer.
2022-12-16 00:07:36 | INFO | absl | Using default tokenizer.
2022-12-16 00:08:05 | INFO | absl | Using default tokenizer.
2022-12-16 00:08:45 | INFO | valid | {"epoch": 7, "valid_loss": "3.239", "valid_nll_loss": "3.239", "valid_rouge1": 0.47012009692388884, "valid_rouge2": 0.13509031856968676, "valid_rougel": 0.22429593929184918, "valid_rouge_avg": 0.30260520774678773, "valid_ppl": "9.44", "valid_wps": "120.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "942", "valid_best_rouge_avg": 0.30260520774678773}
2022-12-16 00:08:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 942 updates
2022-12-16 00:08:45 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 00:09:03 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 00:09:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 7 @ 942 updates, score 0.30260520774678773) (writing took 61.96894338494167 seconds)
2022-12-16 00:09:47 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-12-16 00:09:47 | INFO | train | {"epoch": 7, "train_loss": "2.11", "train_nll_loss": "2.11", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "4.32", "train_wps": "887.4", "train_ups": "0.06", "train_wpb": "13880", "train_bsz": "31.8", "train_num_updates": "942", "train_lr": "9.53474e-05", "train_gnorm": "11.018", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1091", "train_gb_free": "62.2", "train_wall": "16838"}
2022-12-16 00:09:47 | INFO | fairseq.trainer | begin training epoch 8
2022-12-16 00:09:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:11:19 | INFO | train_inner | {"epoch": 8, "update": 7.059, "loss": "2.005", "nll_loss": "2.005", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.01", "wps": "115.9", "ups": "0.01", "wpb": "12800.1", "bsz": "29.6", "num_updates": "950", "lr": "9.52632e-05", "gnorm": "2.363", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "16930"}
2022-12-16 00:12:39 | INFO | train_inner | {"epoch": 8, "update": 7.132, "loss": "2.064", "nll_loss": "2.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.18", "wps": "1778.8", "ups": "0.13", "wpb": "14150.4", "bsz": "32", "num_updates": "960", "lr": "9.51579e-05", "gnorm": "2.725", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "17010"}
2022-12-16 00:13:59 | INFO | train_inner | {"epoch": 8, "update": 7.206, "loss": "2.106", "nll_loss": "2.106", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.31", "wps": "1733.6", "ups": "0.13", "wpb": "13812.5", "bsz": "32", "num_updates": "970", "lr": "9.50526e-05", "gnorm": "13.524", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "17090"}
2022-12-16 00:15:19 | INFO | train_inner | {"epoch": 8, "update": 7.279, "loss": "2.111", "nll_loss": "2.111", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.32", "wps": "1691.8", "ups": "0.12", "wpb": "13650.4", "bsz": "32", "num_updates": "980", "lr": "9.49474e-05", "gnorm": "6.547", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "17170"}
2022-12-16 00:16:39 | INFO | train_inner | {"epoch": 8, "update": 7.353, "loss": "2.043", "nll_loss": "2.043", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.12", "wps": "1740.2", "ups": "0.13", "wpb": "13857.2", "bsz": "32", "num_updates": "990", "lr": "9.48421e-05", "gnorm": "2.238", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "17250"}
2022-12-16 00:17:59 | INFO | train_inner | {"epoch": 8, "update": 7.426, "loss": "2.055", "nll_loss": "2.055", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.16", "wps": "1781.2", "ups": "0.13", "wpb": "14215.2", "bsz": "32", "num_updates": "1000", "lr": "9.47368e-05", "gnorm": "1.938", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "17330"}
2022-12-16 00:19:20 | INFO | train_inner | {"epoch": 8, "update": 7.5, "loss": "2.051", "nll_loss": "2.051", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.14", "wps": "1720.4", "ups": "0.12", "wpb": "13958", "bsz": "32", "num_updates": "1010", "lr": "9.46316e-05", "gnorm": "2.257", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "17411"}
2022-12-16 00:20:40 | INFO | train_inner | {"epoch": 8, "update": 7.574, "loss": "2.101", "nll_loss": "2.101", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.29", "wps": "1724.1", "ups": "0.12", "wpb": "13872.5", "bsz": "32", "num_updates": "1020", "lr": "9.45263e-05", "gnorm": "2.24", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "17491"}
2022-12-16 00:22:02 | INFO | train_inner | {"epoch": 8, "update": 7.647, "loss": "2.141", "nll_loss": "2.141", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.41", "wps": "1691.4", "ups": "0.12", "wpb": "13857.5", "bsz": "32", "num_updates": "1030", "lr": "9.44211e-05", "gnorm": "8.267", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "17573"}
2022-12-16 00:23:23 | INFO | train_inner | {"epoch": 8, "update": 7.721, "loss": "2.213", "nll_loss": "2.213", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.64", "wps": "1720.7", "ups": "0.12", "wpb": "13925.2", "bsz": "32", "num_updates": "1040", "lr": "9.43158e-05", "gnorm": "18.885", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "17654"}
2022-12-16 00:24:40 | INFO | train_inner | {"epoch": 8, "update": 7.794, "loss": "2.178", "nll_loss": "2.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.53", "wps": "1836.2", "ups": "0.13", "wpb": "14025.4", "bsz": "32", "num_updates": "1050", "lr": "9.42105e-05", "gnorm": "19.696", "clip": "100", "loss_scale": "0.25", "train_wall": "76", "gb_free": "62.2", "wall": "17731"}
2022-12-16 00:26:01 | INFO | train_inner | {"epoch": 8, "update": 7.868, "loss": "2.129", "nll_loss": "2.129", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.37", "wps": "1717.1", "ups": "0.12", "wpb": "13893.7", "bsz": "32", "num_updates": "1060", "lr": "9.41053e-05", "gnorm": "14.202", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "17812"}
2022-12-16 00:27:23 | INFO | train_inner | {"epoch": 8, "update": 7.941, "loss": "2.045", "nll_loss": "2.045", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.13", "wps": "1732.5", "ups": "0.12", "wpb": "14189.3", "bsz": "32", "num_updates": "1070", "lr": "9.4e-05", "gnorm": "19.542", "clip": "100", "loss_scale": "0.25", "train_wall": "82", "gb_free": "62.2", "wall": "17894"}
2022-12-16 00:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 00:29:14 | INFO | absl | Using default tokenizer.
2022-12-16 00:29:44 | INFO | absl | Using default tokenizer.
2022-12-16 00:30:06 | INFO | absl | Using default tokenizer.
2022-12-16 00:30:38 | INFO | absl | Using default tokenizer.
2022-12-16 00:31:06 | INFO | absl | Using default tokenizer.
2022-12-16 00:31:34 | INFO | absl | Using default tokenizer.
2022-12-16 00:32:00 | INFO | absl | Using default tokenizer.
2022-12-16 00:32:28 | INFO | absl | Using default tokenizer.
2022-12-16 00:32:53 | INFO | absl | Using default tokenizer.
2022-12-16 00:33:20 | INFO | absl | Using default tokenizer.
2022-12-16 00:33:53 | INFO | absl | Using default tokenizer.
2022-12-16 00:34:19 | INFO | absl | Using default tokenizer.
2022-12-16 00:34:44 | INFO | absl | Using default tokenizer.
2022-12-16 00:35:11 | INFO | absl | Using default tokenizer.
2022-12-16 00:35:39 | INFO | absl | Using default tokenizer.
2022-12-16 00:36:06 | INFO | absl | Using default tokenizer.
2022-12-16 00:36:49 | INFO | absl | Using default tokenizer.
2022-12-16 00:37:14 | INFO | absl | Using default tokenizer.
2022-12-16 00:37:39 | INFO | absl | Using default tokenizer.
2022-12-16 00:38:03 | INFO | absl | Using default tokenizer.
2022-12-16 00:38:32 | INFO | absl | Using default tokenizer.
2022-12-16 00:39:05 | INFO | absl | Using default tokenizer.
2022-12-16 00:39:31 | INFO | absl | Using default tokenizer.
2022-12-16 00:39:58 | INFO | absl | Using default tokenizer.
2022-12-16 00:40:34 | INFO | absl | Using default tokenizer.
2022-12-16 00:41:05 | INFO | absl | Using default tokenizer.
2022-12-16 00:41:32 | INFO | absl | Using default tokenizer.
2022-12-16 00:42:17 | INFO | absl | Using default tokenizer.
2022-12-16 00:42:49 | INFO | absl | Using default tokenizer.
2022-12-16 00:43:20 | INFO | absl | Using default tokenizer.
2022-12-16 00:43:49 | INFO | absl | Using default tokenizer.
2022-12-16 00:44:14 | INFO | valid | {"epoch": 8, "valid_loss": "3.223", "valid_nll_loss": "3.223", "valid_rouge1": 0.47868461168392196, "valid_rouge2": 0.1400496545081563, "valid_rougel": 0.22615408685620708, "valid_rouge_avg": 0.30936713309603914, "valid_ppl": "9.34", "valid_wps": "118.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1078", "valid_best_rouge_avg": 0.30936713309603914}
2022-12-16 00:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1078 updates
2022-12-16 00:44:14 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 00:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 00:45:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 8 @ 1078 updates, score 0.30936713309603914) (writing took 51.18897900800221 seconds)
2022-12-16 00:45:05 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-12-16 00:45:05 | INFO | train | {"epoch": 8, "train_loss": "2.094", "train_nll_loss": "2.094", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "4.27", "train_wps": "891.2", "train_ups": "0.06", "train_wpb": "13879.8", "train_bsz": "31.8", "train_num_updates": "1078", "train_lr": "9.39158e-05", "train_gnorm": "8.836", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "18956"}
2022-12-16 00:45:05 | INFO | fairseq.trainer | begin training epoch 9
2022-12-16 00:45:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 00:45:52 | INFO | train_inner | {"epoch": 9, "update": 8.015, "loss": "2.015", "nll_loss": "2.015", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.04", "wps": "118.4", "ups": "0.01", "wpb": "13141", "bsz": "29.6", "num_updates": "1080", "lr": "9.38947e-05", "gnorm": "8.628", "clip": "100", "loss_scale": "0.25", "train_wall": "82", "gb_free": "62.2", "wall": "19003"}
2022-12-16 00:47:11 | INFO | train_inner | {"epoch": 9, "update": 8.088, "loss": "1.857", "nll_loss": "1.857", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.62", "wps": "1768.9", "ups": "0.13", "wpb": "13986.5", "bsz": "32", "num_updates": "1090", "lr": "9.37895e-05", "gnorm": "4.241", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "19082"}
2022-12-16 00:48:33 | INFO | train_inner | {"epoch": 9, "update": 8.162, "loss": "1.897", "nll_loss": "1.897", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.73", "wps": "1723.9", "ups": "0.12", "wpb": "14103.2", "bsz": "32", "num_updates": "1100", "lr": "9.36842e-05", "gnorm": "8.977", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "19164"}
2022-12-16 00:49:52 | INFO | train_inner | {"epoch": 9, "update": 8.235, "loss": "1.981", "nll_loss": "1.981", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.95", "wps": "1766", "ups": "0.13", "wpb": "13908.2", "bsz": "32", "num_updates": "1110", "lr": "9.35789e-05", "gnorm": "4.094", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "19243"}
2022-12-16 00:51:12 | INFO | train_inner | {"epoch": 9, "update": 8.309, "loss": "1.958", "nll_loss": "1.958", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.89", "wps": "1733.7", "ups": "0.12", "wpb": "13990.7", "bsz": "32", "num_updates": "1120", "lr": "9.34737e-05", "gnorm": "9.049", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19323"}
2022-12-16 00:52:33 | INFO | train_inner | {"epoch": 9, "update": 8.382, "loss": "1.922", "nll_loss": "1.922", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.79", "wps": "1733.1", "ups": "0.12", "wpb": "13885.3", "bsz": "32", "num_updates": "1130", "lr": "9.33684e-05", "gnorm": "2.951", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19404"}
2022-12-16 00:53:52 | INFO | train_inner | {"epoch": 9, "update": 8.456, "loss": "1.923", "nll_loss": "1.923", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.79", "wps": "1755.6", "ups": "0.13", "wpb": "13988", "bsz": "32", "num_updates": "1140", "lr": "9.32632e-05", "gnorm": "1.663", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "19483"}
2022-12-16 00:55:12 | INFO | train_inner | {"epoch": 9, "update": 8.529, "loss": "1.993", "nll_loss": "1.993", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.98", "wps": "1698.6", "ups": "0.13", "wpb": "13568.6", "bsz": "32", "num_updates": "1150", "lr": "9.31579e-05", "gnorm": "1.742", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19563"}
2022-12-16 00:56:32 | INFO | train_inner | {"epoch": 9, "update": 8.603, "loss": "1.969", "nll_loss": "1.969", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.92", "wps": "1764.7", "ups": "0.12", "wpb": "14128.4", "bsz": "32", "num_updates": "1160", "lr": "9.30526e-05", "gnorm": "2.465", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19643"}
2022-12-16 00:57:53 | INFO | train_inner | {"epoch": 9, "update": 8.676, "loss": "1.96", "nll_loss": "1.96", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.89", "wps": "1740.1", "ups": "0.12", "wpb": "14116.8", "bsz": "32", "num_updates": "1170", "lr": "9.29474e-05", "gnorm": "1.992", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "19724"}
2022-12-16 00:59:14 | INFO | train_inner | {"epoch": 9, "update": 8.75, "loss": "1.978", "nll_loss": "1.978", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.94", "wps": "1756.3", "ups": "0.12", "wpb": "14106", "bsz": "32", "num_updates": "1180", "lr": "9.28421e-05", "gnorm": "3.772", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19805"}
2022-12-16 01:00:34 | INFO | train_inner | {"epoch": 9, "update": 8.824, "loss": "1.958", "nll_loss": "1.958", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.88", "wps": "1715.9", "ups": "0.12", "wpb": "13779.4", "bsz": "32", "num_updates": "1190", "lr": "9.27368e-05", "gnorm": "1.855", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "19885"}
2022-12-16 01:01:55 | INFO | train_inner | {"epoch": 9, "update": 8.897, "loss": "1.971", "nll_loss": "1.971", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.92", "wps": "1687.5", "ups": "0.12", "wpb": "13714.6", "bsz": "32", "num_updates": "1200", "lr": "9.26316e-05", "gnorm": "2.774", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "19966"}
2022-12-16 01:03:16 | INFO | train_inner | {"epoch": 9, "update": 8.971, "loss": "1.963", "nll_loss": "1.963", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.9", "wps": "1771.5", "ups": "0.12", "wpb": "14270.2", "bsz": "32", "num_updates": "1210", "lr": "9.25263e-05", "gnorm": "2.401", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "20047"}
2022-12-16 01:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 01:04:41 | INFO | absl | Using default tokenizer.
2022-12-16 01:05:07 | INFO | absl | Using default tokenizer.
2022-12-16 01:05:34 | INFO | absl | Using default tokenizer.
2022-12-16 01:06:02 | INFO | absl | Using default tokenizer.
2022-12-16 01:06:27 | INFO | absl | Using default tokenizer.
2022-12-16 01:06:59 | INFO | absl | Using default tokenizer.
2022-12-16 01:07:20 | INFO | absl | Using default tokenizer.
2022-12-16 01:07:44 | INFO | absl | Using default tokenizer.
2022-12-16 01:08:13 | INFO | absl | Using default tokenizer.
2022-12-16 01:08:44 | INFO | absl | Using default tokenizer.
2022-12-16 01:09:12 | INFO | absl | Using default tokenizer.
2022-12-16 01:09:45 | INFO | absl | Using default tokenizer.
2022-12-16 01:10:18 | INFO | absl | Using default tokenizer.
2022-12-16 01:10:44 | INFO | absl | Using default tokenizer.
2022-12-16 01:11:06 | INFO | absl | Using default tokenizer.
2022-12-16 01:11:32 | INFO | absl | Using default tokenizer.
2022-12-16 01:12:00 | INFO | absl | Using default tokenizer.
2022-12-16 01:12:25 | INFO | absl | Using default tokenizer.
2022-12-16 01:12:57 | INFO | absl | Using default tokenizer.
2022-12-16 01:13:22 | INFO | absl | Using default tokenizer.
2022-12-16 01:13:51 | INFO | absl | Using default tokenizer.
2022-12-16 01:14:23 | INFO | absl | Using default tokenizer.
2022-12-16 01:14:54 | INFO | absl | Using default tokenizer.
2022-12-16 01:15:17 | INFO | absl | Using default tokenizer.
2022-12-16 01:15:48 | INFO | absl | Using default tokenizer.
2022-12-16 01:16:18 | INFO | absl | Using default tokenizer.
2022-12-16 01:16:49 | INFO | absl | Using default tokenizer.
2022-12-16 01:17:14 | INFO | absl | Using default tokenizer.
2022-12-16 01:17:58 | INFO | absl | Using default tokenizer.
2022-12-16 01:18:25 | INFO | absl | Using default tokenizer.
2022-12-16 01:18:59 | INFO | absl | Using default tokenizer.
2022-12-16 01:19:25 | INFO | valid | {"epoch": 9, "valid_loss": "3.334", "valid_nll_loss": "3.334", "valid_rouge1": 0.47457250961444475, "valid_rouge2": 0.13532080559686896, "valid_rougel": 0.22283669483547527, "valid_rouge_avg": 0.30494665760565687, "valid_ppl": "10.08", "valid_wps": "121", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1214", "valid_best_rouge_avg": 0.30936713309603914}
2022-12-16 01:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1214 updates
2022-12-16 01:19:25 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 01:19:46 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 01:19:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 9 @ 1214 updates, score 0.30494665760565687) (writing took 20.439423392992467 seconds)
2022-12-16 01:19:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-12-16 01:19:46 | INFO | train | {"epoch": 9, "train_loss": "1.946", "train_nll_loss": "1.946", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "3.85", "train_wps": "907.2", "train_ups": "0.07", "train_wpb": "13881.3", "train_bsz": "31.8", "train_num_updates": "1214", "train_lr": "9.24842e-05", "train_gnorm": "3.811", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "21037"}
2022-12-16 01:19:46 | INFO | fairseq.trainer | begin training epoch 10
2022-12-16 01:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 01:21:11 | INFO | train_inner | {"epoch": 10, "update": 9.044, "loss": "1.796", "nll_loss": "1.796", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.47", "wps": "118.6", "ups": "0.01", "wpb": "12746", "bsz": "29.6", "num_updates": "1220", "lr": "9.24211e-05", "gnorm": "5.961", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "63", "wall": "21122"}
2022-12-16 01:22:31 | INFO | train_inner | {"epoch": 10, "update": 9.118, "loss": "1.656", "nll_loss": "1.656", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.15", "wps": "1745.2", "ups": "0.12", "wpb": "13971", "bsz": "32", "num_updates": "1230", "lr": "9.23158e-05", "gnorm": "5.567", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "21202"}
2022-12-16 01:23:52 | INFO | train_inner | {"epoch": 10, "update": 9.191, "loss": "1.744", "nll_loss": "1.744", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.35", "wps": "1760.4", "ups": "0.12", "wpb": "14276.1", "bsz": "32", "num_updates": "1240", "lr": "9.22105e-05", "gnorm": "11.995", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "21283"}
2022-12-16 01:25:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2022-12-16 01:25:20 | INFO | train_inner | {"epoch": 10, "update": 9.272, "loss": "1.783", "nll_loss": "1.783", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.44", "wps": "1573.1", "ups": "0.11", "wpb": "13867.9", "bsz": "32", "num_updates": "1250", "lr": "9.21053e-05", "gnorm": "11.823", "clip": "100", "loss_scale": "0.125", "train_wall": "88", "gb_free": "62.2", "wall": "21371"}
2022-12-16 01:26:41 | INFO | train_inner | {"epoch": 10, "update": 9.346, "loss": "1.887", "nll_loss": "1.887", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.7", "wps": "1736.5", "ups": "0.12", "wpb": "13990.9", "bsz": "32", "num_updates": "1260", "lr": "9.2e-05", "gnorm": "13.642", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "21452"}
2022-12-16 01:28:01 | INFO | train_inner | {"epoch": 10, "update": 9.419, "loss": "1.773", "nll_loss": "1.773", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.42", "wps": "1752.7", "ups": "0.13", "wpb": "14015.5", "bsz": "32", "num_updates": "1270", "lr": "9.18947e-05", "gnorm": "12.941", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "21532"}
2022-12-16 01:29:21 | INFO | train_inner | {"epoch": 10, "update": 9.493, "loss": "1.785", "nll_loss": "1.785", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.45", "wps": "1742.3", "ups": "0.12", "wpb": "14065", "bsz": "32", "num_updates": "1280", "lr": "9.17895e-05", "gnorm": "14.614", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "21613"}
2022-12-16 01:30:41 | INFO | train_inner | {"epoch": 10, "update": 9.566, "loss": "1.695", "nll_loss": "1.695", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.24", "wps": "1782.2", "ups": "0.13", "wpb": "14195.9", "bsz": "32", "num_updates": "1290", "lr": "9.16842e-05", "gnorm": "21.157", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "21692"}
2022-12-16 01:32:01 | INFO | train_inner | {"epoch": 10, "update": 9.64, "loss": "1.704", "nll_loss": "1.704", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.26", "wps": "1722.2", "ups": "0.13", "wpb": "13706.3", "bsz": "32", "num_updates": "1300", "lr": "9.15789e-05", "gnorm": "20.998", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "21772"}
2022-12-16 01:33:20 | INFO | train_inner | {"epoch": 10, "update": 9.713, "loss": "1.717", "nll_loss": "1.717", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.29", "wps": "1754.8", "ups": "0.13", "wpb": "13916.1", "bsz": "32", "num_updates": "1310", "lr": "9.14737e-05", "gnorm": "13.447", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "21851"}
2022-12-16 01:34:41 | INFO | train_inner | {"epoch": 10, "update": 9.787, "loss": "1.746", "nll_loss": "1.746", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.35", "wps": "1716", "ups": "0.12", "wpb": "13924.6", "bsz": "32", "num_updates": "1320", "lr": "9.13684e-05", "gnorm": "18.371", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "21932"}
2022-12-16 01:36:01 | INFO | train_inner | {"epoch": 10, "update": 9.86, "loss": "1.715", "nll_loss": "1.715", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.28", "wps": "1753.1", "ups": "0.13", "wpb": "14005.6", "bsz": "32", "num_updates": "1330", "lr": "9.12632e-05", "gnorm": "17.078", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "22012"}
2022-12-16 01:37:23 | INFO | train_inner | {"epoch": 10, "update": 9.934, "loss": "1.693", "nll_loss": "1.693", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.23", "wps": "1683", "ups": "0.12", "wpb": "13817.7", "bsz": "32", "num_updates": "1340", "lr": "9.11579e-05", "gnorm": "29.482", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "22094"}
2022-12-16 01:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 01:39:21 | INFO | absl | Using default tokenizer.
2022-12-16 01:39:55 | INFO | absl | Using default tokenizer.
2022-12-16 01:40:36 | INFO | absl | Using default tokenizer.
2022-12-16 01:41:19 | INFO | absl | Using default tokenizer.
2022-12-16 01:41:53 | INFO | absl | Using default tokenizer.
2022-12-16 01:42:23 | INFO | absl | Using default tokenizer.
2022-12-16 01:42:58 | INFO | absl | Using default tokenizer.
2022-12-16 01:43:40 | INFO | absl | Using default tokenizer.
2022-12-16 01:44:13 | INFO | absl | Using default tokenizer.
2022-12-16 01:44:56 | INFO | absl | Using default tokenizer.
2022-12-16 01:45:33 | INFO | absl | Using default tokenizer.
2022-12-16 01:46:18 | INFO | absl | Using default tokenizer.
2022-12-16 01:46:55 | INFO | absl | Using default tokenizer.
2022-12-16 01:47:33 | INFO | absl | Using default tokenizer.
2022-12-16 01:48:17 | INFO | absl | Using default tokenizer.
2022-12-16 01:48:48 | INFO | absl | Using default tokenizer.
2022-12-16 01:49:32 | INFO | absl | Using default tokenizer.
2022-12-16 01:50:16 | INFO | absl | Using default tokenizer.
2022-12-16 01:50:51 | INFO | absl | Using default tokenizer.
2022-12-16 01:51:29 | INFO | absl | Using default tokenizer.
2022-12-16 01:52:18 | INFO | absl | Using default tokenizer.
2022-12-16 01:52:58 | INFO | absl | Using default tokenizer.
2022-12-16 01:53:43 | INFO | absl | Using default tokenizer.
2022-12-16 01:54:18 | INFO | absl | Using default tokenizer.
2022-12-16 01:54:51 | INFO | absl | Using default tokenizer.
2022-12-16 01:55:38 | INFO | absl | Using default tokenizer.
2022-12-16 01:56:14 | INFO | absl | Using default tokenizer.
2022-12-16 01:56:54 | INFO | absl | Using default tokenizer.
2022-12-16 01:57:40 | INFO | absl | Using default tokenizer.
2022-12-16 01:58:21 | INFO | absl | Using default tokenizer.
2022-12-16 01:59:05 | INFO | absl | Using default tokenizer.
2022-12-16 01:59:37 | INFO | valid | {"epoch": 10, "valid_loss": "3.452", "valid_nll_loss": "3.452", "valid_rouge1": 0.4655051746806749, "valid_rouge2": 0.13494867395553162, "valid_rougel": 0.22273690666053428, "valid_rouge_avg": 0.30022692431810327, "valid_ppl": "10.94", "valid_wps": "87.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1349", "valid_best_rouge_avg": 0.30936713309603914}
2022-12-16 01:59:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1349 updates
2022-12-16 01:59:37 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 01:59:58 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 01:59:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 10 @ 1349 updates, score 0.30022692431810327) (writing took 20.970200416864827 seconds)
2022-12-16 01:59:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-12-16 01:59:58 | INFO | train | {"epoch": 10, "train_loss": "1.742", "train_nll_loss": "1.742", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "3.35", "train_wps": "776.9", "train_ups": "0.06", "train_wpb": "13883.2", "train_bsz": "31.8", "train_num_updates": "1349", "train_lr": "9.10632e-05", "train_gnorm": "16.111", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "23449"}
2022-12-16 01:59:58 | INFO | fairseq.trainer | begin training epoch 11
2022-12-16 01:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 02:00:43 | INFO | train_inner | {"epoch": 11, "update": 10.007, "loss": "1.73", "nll_loss": "1.73", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.32", "wps": "91.2", "ups": "0.01", "wpb": "12760.7", "bsz": "29.6", "num_updates": "1350", "lr": "9.10526e-05", "gnorm": "22.838", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "23494"}
2022-12-16 02:02:02 | INFO | train_inner | {"epoch": 11, "update": 10.081, "loss": "1.634", "nll_loss": "1.634", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.1", "wps": "1754.3", "ups": "0.13", "wpb": "13882.4", "bsz": "32", "num_updates": "1360", "lr": "9.09474e-05", "gnorm": "17.854", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "23573"}
2022-12-16 02:03:23 | INFO | train_inner | {"epoch": 11, "update": 10.154, "loss": "1.666", "nll_loss": "1.666", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.17", "wps": "1723.8", "ups": "0.12", "wpb": "13845.9", "bsz": "32", "num_updates": "1370", "lr": "9.08421e-05", "gnorm": "16.588", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "23654"}
2022-12-16 02:04:44 | INFO | train_inner | {"epoch": 11, "update": 10.228, "loss": "1.624", "nll_loss": "1.624", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.08", "wps": "1696.9", "ups": "0.12", "wpb": "13760.4", "bsz": "32", "num_updates": "1380", "lr": "9.07368e-05", "gnorm": "18.545", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "23735"}
2022-12-16 02:06:05 | INFO | train_inner | {"epoch": 11, "update": 10.301, "loss": "1.743", "nll_loss": "1.743", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.35", "wps": "1741.6", "ups": "0.12", "wpb": "14119.3", "bsz": "32", "num_updates": "1390", "lr": "9.06316e-05", "gnorm": "7.289", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "23816"}
2022-12-16 02:07:25 | INFO | train_inner | {"epoch": 11, "update": 10.375, "loss": "1.764", "nll_loss": "1.764", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.4", "wps": "1757.1", "ups": "0.12", "wpb": "14117.9", "bsz": "32", "num_updates": "1400", "lr": "9.05263e-05", "gnorm": "7.555", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "23896"}
2022-12-16 02:08:46 | INFO | train_inner | {"epoch": 11, "update": 10.449, "loss": "1.777", "nll_loss": "1.777", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.43", "wps": "1753.9", "ups": "0.12", "wpb": "14200.6", "bsz": "32", "num_updates": "1410", "lr": "9.04211e-05", "gnorm": "7.788", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "23977"}
2022-12-16 02:10:06 | INFO | train_inner | {"epoch": 11, "update": 10.522, "loss": "1.79", "nll_loss": "1.79", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.46", "wps": "1733.8", "ups": "0.13", "wpb": "13801.2", "bsz": "32", "num_updates": "1420", "lr": "9.03158e-05", "gnorm": "3.873", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "24057"}
2022-12-16 02:11:25 | INFO | train_inner | {"epoch": 11, "update": 10.596, "loss": "1.791", "nll_loss": "1.791", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.46", "wps": "1747.8", "ups": "0.13", "wpb": "13917", "bsz": "32", "num_updates": "1430", "lr": "9.02105e-05", "gnorm": "5.786", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "24137"}
2022-12-16 02:12:45 | INFO | train_inner | {"epoch": 11, "update": 10.669, "loss": "1.757", "nll_loss": "1.757", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.38", "wps": "1770", "ups": "0.13", "wpb": "14138.9", "bsz": "32", "num_updates": "1440", "lr": "9.01053e-05", "gnorm": "4.865", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "24216"}
2022-12-16 02:14:07 | INFO | train_inner | {"epoch": 11, "update": 10.743, "loss": "1.77", "nll_loss": "1.77", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.41", "wps": "1704.2", "ups": "0.12", "wpb": "13871.9", "bsz": "32", "num_updates": "1450", "lr": "9e-05", "gnorm": "9.23", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "24298"}
2022-12-16 02:15:28 | INFO | train_inner | {"epoch": 11, "update": 10.816, "loss": "1.786", "nll_loss": "1.786", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.45", "wps": "1719", "ups": "0.12", "wpb": "13900.3", "bsz": "32", "num_updates": "1460", "lr": "8.98947e-05", "gnorm": "12.463", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "24379"}
2022-12-16 02:16:48 | INFO | train_inner | {"epoch": 11, "update": 10.89, "loss": "1.807", "nll_loss": "1.807", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.5", "wps": "1715", "ups": "0.12", "wpb": "13751.4", "bsz": "32", "num_updates": "1470", "lr": "8.97895e-05", "gnorm": "16", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "24459"}
2022-12-16 02:18:07 | INFO | train_inner | {"epoch": 11, "update": 10.963, "loss": "1.894", "nll_loss": "1.894", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.72", "wps": "1772.1", "ups": "0.13", "wpb": "14038.5", "bsz": "32", "num_updates": "1480", "lr": "8.96842e-05", "gnorm": "20.972", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "24538"}
2022-12-16 02:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 02:19:33 | INFO | absl | Using default tokenizer.
2022-12-16 02:20:00 | INFO | absl | Using default tokenizer.
2022-12-16 02:20:42 | INFO | absl | Using default tokenizer.
2022-12-16 02:21:23 | INFO | absl | Using default tokenizer.
2022-12-16 02:21:50 | INFO | absl | Using default tokenizer.
2022-12-16 02:22:20 | INFO | absl | Using default tokenizer.
2022-12-16 02:22:46 | INFO | absl | Using default tokenizer.
2022-12-16 02:23:11 | INFO | absl | Using default tokenizer.
2022-12-16 02:23:53 | INFO | absl | Using default tokenizer.
2022-12-16 02:24:36 | INFO | absl | Using default tokenizer.
2022-12-16 02:25:19 | INFO | absl | Using default tokenizer.
2022-12-16 02:26:04 | INFO | absl | Using default tokenizer.
2022-12-16 02:26:46 | INFO | absl | Using default tokenizer.
2022-12-16 02:27:20 | INFO | absl | Using default tokenizer.
2022-12-16 02:27:51 | INFO | absl | Using default tokenizer.
2022-12-16 02:28:19 | INFO | absl | Using default tokenizer.
2022-12-16 02:28:49 | INFO | absl | Using default tokenizer.
2022-12-16 02:29:23 | INFO | absl | Using default tokenizer.
2022-12-16 02:30:07 | INFO | absl | Using default tokenizer.
2022-12-16 02:30:34 | INFO | absl | Using default tokenizer.
2022-12-16 02:31:19 | INFO | absl | Using default tokenizer.
2022-12-16 02:31:59 | INFO | absl | Using default tokenizer.
2022-12-16 02:32:37 | INFO | absl | Using default tokenizer.
2022-12-16 02:33:22 | INFO | absl | Using default tokenizer.
2022-12-16 02:33:53 | INFO | absl | Using default tokenizer.
2022-12-16 02:34:31 | INFO | absl | Using default tokenizer.
2022-12-16 02:35:16 | INFO | absl | Using default tokenizer.
2022-12-16 02:35:58 | INFO | absl | Using default tokenizer.
2022-12-16 02:36:31 | INFO | absl | Using default tokenizer.
2022-12-16 02:37:08 | INFO | absl | Using default tokenizer.
2022-12-16 02:37:37 | INFO | absl | Using default tokenizer.
2022-12-16 02:38:04 | INFO | valid | {"epoch": 11, "valid_loss": "3.629", "valid_nll_loss": "3.629", "valid_rouge1": 0.44286046993815215, "valid_rouge2": 0.11467125432453101, "valid_rougel": 0.21036853836177297, "valid_rouge_avg": 0.27876586213134164, "valid_ppl": "12.37", "valid_wps": "95.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1485", "valid_best_rouge_avg": 0.30936713309603914}
2022-12-16 02:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1485 updates
2022-12-16 02:38:04 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 02:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 02:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 11 @ 1485 updates, score 0.27876586213134164) (writing took 18.472750424873084 seconds)
2022-12-16 02:38:22 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-12-16 02:38:22 | INFO | train | {"epoch": 11, "train_loss": "1.765", "train_nll_loss": "1.765", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "3.4", "train_wps": "819.4", "train_ups": "0.06", "train_wpb": "13881.6", "train_bsz": "31.8", "train_num_updates": "1485", "train_lr": "8.96316e-05", "train_gnorm": "12.49", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "25753"}
2022-12-16 02:38:22 | INFO | fairseq.trainer | begin training epoch 12
2022-12-16 02:38:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 02:39:41 | INFO | train_inner | {"epoch": 12, "update": 11.037, "loss": "2.12", "nll_loss": "2.12", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.35", "wps": "100", "ups": "0.01", "wpb": "12934.7", "bsz": "29.6", "num_updates": "1490", "lr": "8.95789e-05", "gnorm": "35.17", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "25832"}
2022-12-16 02:41:03 | INFO | train_inner | {"epoch": 12, "update": 11.11, "loss": "1.749", "nll_loss": "1.749", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.36", "wps": "1704.4", "ups": "0.12", "wpb": "13906.6", "bsz": "32", "num_updates": "1500", "lr": "8.94737e-05", "gnorm": "20.541", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "25914"}
2022-12-16 02:42:22 | INFO | train_inner | {"epoch": 12, "update": 11.184, "loss": "1.55", "nll_loss": "1.55", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.93", "wps": "1766.9", "ups": "0.13", "wpb": "14043.9", "bsz": "32", "num_updates": "1510", "lr": "8.93684e-05", "gnorm": "13.023", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "25993"}
2022-12-16 02:43:42 | INFO | train_inner | {"epoch": 12, "update": 11.257, "loss": "1.524", "nll_loss": "1.524", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.88", "wps": "1737.4", "ups": "0.12", "wpb": "13938.9", "bsz": "32", "num_updates": "1520", "lr": "8.92632e-05", "gnorm": "9.646", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "26073"}
2022-12-16 02:45:02 | INFO | train_inner | {"epoch": 12, "update": 11.331, "loss": "1.616", "nll_loss": "1.616", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.07", "wps": "1747.4", "ups": "0.13", "wpb": "13904.9", "bsz": "32", "num_updates": "1530", "lr": "8.91579e-05", "gnorm": "13.309", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "26153"}
2022-12-16 02:46:22 | INFO | train_inner | {"epoch": 12, "update": 11.404, "loss": "1.553", "nll_loss": "1.553", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.93", "wps": "1721", "ups": "0.13", "wpb": "13737.1", "bsz": "32", "num_updates": "1540", "lr": "8.90526e-05", "gnorm": "19.364", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "26233"}
2022-12-16 02:47:43 | INFO | train_inner | {"epoch": 12, "update": 11.478, "loss": "1.601", "nll_loss": "1.601", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.03", "wps": "1744.6", "ups": "0.12", "wpb": "14137.4", "bsz": "32", "num_updates": "1550", "lr": "8.89474e-05", "gnorm": "11.529", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "26314"}
2022-12-16 02:49:03 | INFO | train_inner | {"epoch": 12, "update": 11.551, "loss": "1.634", "nll_loss": "1.634", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.1", "wps": "1746", "ups": "0.12", "wpb": "14009.8", "bsz": "32", "num_updates": "1560", "lr": "8.88421e-05", "gnorm": "15.261", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "26394"}
2022-12-16 02:50:23 | INFO | train_inner | {"epoch": 12, "update": 11.625, "loss": "1.635", "nll_loss": "1.635", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.11", "wps": "1760.9", "ups": "0.13", "wpb": "14082.3", "bsz": "32", "num_updates": "1570", "lr": "8.87368e-05", "gnorm": "10.882", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "26474"}
2022-12-16 02:51:44 | INFO | train_inner | {"epoch": 12, "update": 11.699, "loss": "1.694", "nll_loss": "1.694", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.24", "wps": "1752.6", "ups": "0.12", "wpb": "14108", "bsz": "32", "num_updates": "1580", "lr": "8.86316e-05", "gnorm": "11.71", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "26555"}
2022-12-16 02:53:04 | INFO | train_inner | {"epoch": 12, "update": 11.772, "loss": "1.7", "nll_loss": "1.7", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.25", "wps": "1744.2", "ups": "0.12", "wpb": "14050.6", "bsz": "32", "num_updates": "1590", "lr": "8.85263e-05", "gnorm": "19.069", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "64.3", "wall": "26635"}
2022-12-16 02:54:25 | INFO | train_inner | {"epoch": 12, "update": 11.846, "loss": "1.634", "nll_loss": "1.634", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.1", "wps": "1739.1", "ups": "0.12", "wpb": "13970.4", "bsz": "32", "num_updates": "1600", "lr": "8.84211e-05", "gnorm": "12.862", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "26716"}
2022-12-16 02:55:45 | INFO | train_inner | {"epoch": 12, "update": 11.919, "loss": "1.577", "nll_loss": "1.577", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.98", "wps": "1727.3", "ups": "0.12", "wpb": "13853.4", "bsz": "32", "num_updates": "1610", "lr": "8.83158e-05", "gnorm": "12.303", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "26796"}
2022-12-16 02:57:09 | INFO | train_inner | {"epoch": 12, "update": 11.993, "loss": "1.78", "nll_loss": "1.78", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.43", "wps": "1633.1", "ups": "0.12", "wpb": "13696", "bsz": "32", "num_updates": "1620", "lr": "8.82105e-05", "gnorm": "21.869", "clip": "100", "loss_scale": "0.125", "train_wall": "83", "gb_free": "63.6", "wall": "26880"}
2022-12-16 02:57:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 02:58:00 | INFO | absl | Using default tokenizer.
2022-12-16 02:58:36 | INFO | absl | Using default tokenizer.
2022-12-16 02:59:18 | INFO | absl | Using default tokenizer.
2022-12-16 02:59:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:00:23 | INFO | absl | Using default tokenizer.
2022-12-16 03:00:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:01:37 | INFO | absl | Using default tokenizer.
2022-12-16 03:02:07 | INFO | absl | Using default tokenizer.
2022-12-16 03:02:42 | INFO | absl | Using default tokenizer.
2022-12-16 03:03:19 | INFO | absl | Using default tokenizer.
2022-12-16 03:03:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:04:25 | INFO | absl | Using default tokenizer.
2022-12-16 03:04:57 | INFO | absl | Using default tokenizer.
2022-12-16 03:05:42 | INFO | absl | Using default tokenizer.
2022-12-16 03:06:13 | INFO | absl | Using default tokenizer.
2022-12-16 03:06:43 | INFO | absl | Using default tokenizer.
2022-12-16 03:07:15 | INFO | absl | Using default tokenizer.
2022-12-16 03:07:50 | INFO | absl | Using default tokenizer.
2022-12-16 03:08:22 | INFO | absl | Using default tokenizer.
2022-12-16 03:08:52 | INFO | absl | Using default tokenizer.
2022-12-16 03:09:30 | INFO | absl | Using default tokenizer.
2022-12-16 03:10:07 | INFO | absl | Using default tokenizer.
2022-12-16 03:10:36 | INFO | absl | Using default tokenizer.
2022-12-16 03:11:10 | INFO | absl | Using default tokenizer.
2022-12-16 03:11:46 | INFO | absl | Using default tokenizer.
2022-12-16 03:12:16 | INFO | absl | Using default tokenizer.
2022-12-16 03:12:49 | INFO | absl | Using default tokenizer.
2022-12-16 03:13:19 | INFO | absl | Using default tokenizer.
2022-12-16 03:13:51 | INFO | absl | Using default tokenizer.
2022-12-16 03:14:26 | INFO | absl | Using default tokenizer.
2022-12-16 03:14:58 | INFO | absl | Using default tokenizer.
2022-12-16 03:15:34 | INFO | valid | {"epoch": 12, "valid_loss": "3.593", "valid_nll_loss": "3.593", "valid_rouge1": 0.47659738660117607, "valid_rouge2": 0.13250143001553374, "valid_rougel": 0.22076692034369644, "valid_rouge_avg": 0.30454940830835486, "valid_ppl": "12.07", "valid_wps": "102", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1621", "valid_best_rouge_avg": 0.30936713309603914}
2022-12-16 03:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1621 updates
2022-12-16 03:15:34 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 03:15:55 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 03:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 12 @ 1621 updates, score 0.30454940830835486) (writing took 20.720084079075605 seconds)
2022-12-16 03:15:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-12-16 03:15:55 | INFO | train | {"epoch": 12, "train_loss": "1.652", "train_nll_loss": "1.652", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "3.14", "train_wps": "837.6", "train_ups": "0.06", "train_wpb": "13874", "train_bsz": "31.8", "train_num_updates": "1621", "train_lr": "8.82e-05", "train_gnorm": "15.328", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1091", "train_gb_free": "62.2", "train_wall": "28006"}
2022-12-16 03:15:55 | INFO | fairseq.trainer | begin training epoch 13
2022-12-16 03:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 03:17:46 | INFO | train_inner | {"epoch": 13, "update": 12.066, "loss": "1.697", "nll_loss": "1.697", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.24", "wps": "103.2", "ups": "0.01", "wpb": "12771.3", "bsz": "29.6", "num_updates": "1630", "lr": "8.81053e-05", "gnorm": "21.231", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63", "wall": "28117"}
2022-12-16 03:19:08 | INFO | train_inner | {"epoch": 13, "update": 12.14, "loss": "1.643", "nll_loss": "1.643", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.12", "wps": "1676.6", "ups": "0.12", "wpb": "13747.2", "bsz": "32", "num_updates": "1640", "lr": "8.8e-05", "gnorm": "8.555", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.9", "wall": "28199"}
2022-12-16 03:20:27 | INFO | train_inner | {"epoch": 13, "update": 12.213, "loss": "1.572", "nll_loss": "1.572", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.97", "wps": "1745.8", "ups": "0.13", "wpb": "13868.3", "bsz": "32", "num_updates": "1650", "lr": "8.78947e-05", "gnorm": "16.123", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "28278"}
2022-12-16 03:21:47 | INFO | train_inner | {"epoch": 13, "update": 12.287, "loss": "1.562", "nll_loss": "1.562", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.95", "wps": "1740.1", "ups": "0.13", "wpb": "13849.7", "bsz": "32", "num_updates": "1660", "lr": "8.77895e-05", "gnorm": "14.56", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "28358"}
2022-12-16 03:23:08 | INFO | train_inner | {"epoch": 13, "update": 12.36, "loss": "1.674", "nll_loss": "1.674", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.19", "wps": "1790.1", "ups": "0.12", "wpb": "14595.7", "bsz": "32", "num_updates": "1670", "lr": "8.76842e-05", "gnorm": "15.296", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63.6", "wall": "28439"}
2022-12-16 03:24:30 | INFO | train_inner | {"epoch": 13, "update": 12.434, "loss": "1.614", "nll_loss": "1.614", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.06", "wps": "1726.6", "ups": "0.12", "wpb": "14063", "bsz": "32", "num_updates": "1680", "lr": "8.75789e-05", "gnorm": "7.386", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "28521"}
2022-12-16 03:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-12-16 03:25:58 | INFO | train_inner | {"epoch": 13, "update": 12.515, "loss": "1.66", "nll_loss": "1.66", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.16", "wps": "1532.2", "ups": "0.11", "wpb": "13514.7", "bsz": "32", "num_updates": "1690", "lr": "8.74737e-05", "gnorm": "12.813", "clip": "100", "loss_scale": "0.0625", "train_wall": "88", "gb_free": "62.2", "wall": "28609"}
2022-12-16 03:27:17 | INFO | train_inner | {"epoch": 13, "update": 12.588, "loss": "1.821", "nll_loss": "1.821", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.53", "wps": "1769.2", "ups": "0.13", "wpb": "14052.9", "bsz": "32", "num_updates": "1700", "lr": "8.73684e-05", "gnorm": "18.276", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "28689"}
2022-12-16 03:28:38 | INFO | train_inner | {"epoch": 13, "update": 12.662, "loss": "2.025", "nll_loss": "2.025", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "4.07", "wps": "1746.8", "ups": "0.12", "wpb": "14041.7", "bsz": "32", "num_updates": "1710", "lr": "8.72632e-05", "gnorm": "24.483", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63", "wall": "28769"}
2022-12-16 03:29:58 | INFO | train_inner | {"epoch": 13, "update": 12.735, "loss": "1.967", "nll_loss": "1.967", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.91", "wps": "1749.8", "ups": "0.12", "wpb": "14001.5", "bsz": "32", "num_updates": "1720", "lr": "8.71579e-05", "gnorm": "15.189", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "28849"}
2022-12-16 03:31:18 | INFO | train_inner | {"epoch": 13, "update": 12.809, "loss": "1.822", "nll_loss": "1.822", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.54", "wps": "1732.4", "ups": "0.13", "wpb": "13778.1", "bsz": "32", "num_updates": "1730", "lr": "8.70526e-05", "gnorm": "18.204", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "28929"}
2022-12-16 03:32:37 | INFO | train_inner | {"epoch": 13, "update": 12.882, "loss": "1.864", "nll_loss": "1.864", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.64", "wps": "1765.8", "ups": "0.13", "wpb": "14031.3", "bsz": "32", "num_updates": "1740", "lr": "8.69474e-05", "gnorm": "16.253", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "29008"}
2022-12-16 03:33:58 | INFO | train_inner | {"epoch": 13, "update": 12.956, "loss": "1.823", "nll_loss": "1.823", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.54", "wps": "1747", "ups": "0.12", "wpb": "14177.9", "bsz": "32", "num_updates": "1750", "lr": "8.68421e-05", "gnorm": "18.093", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "29089"}
2022-12-16 03:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 03:35:32 | INFO | absl | Using default tokenizer.
2022-12-16 03:36:12 | INFO | absl | Using default tokenizer.
2022-12-16 03:36:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:37:36 | INFO | absl | Using default tokenizer.
2022-12-16 03:38:19 | INFO | absl | Using default tokenizer.
2022-12-16 03:39:01 | INFO | absl | Using default tokenizer.
2022-12-16 03:39:38 | INFO | absl | Using default tokenizer.
2022-12-16 03:40:07 | INFO | absl | Using default tokenizer.
2022-12-16 03:40:46 | INFO | absl | Using default tokenizer.
2022-12-16 03:41:25 | INFO | absl | Using default tokenizer.
2022-12-16 03:42:04 | INFO | absl | Using default tokenizer.
2022-12-16 03:42:40 | INFO | absl | Using default tokenizer.
2022-12-16 03:43:19 | INFO | absl | Using default tokenizer.
2022-12-16 03:44:02 | INFO | absl | Using default tokenizer.
2022-12-16 03:44:46 | INFO | absl | Using default tokenizer.
2022-12-16 03:45:20 | INFO | absl | Using default tokenizer.
2022-12-16 03:45:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:46:33 | INFO | absl | Using default tokenizer.
2022-12-16 03:47:06 | INFO | absl | Using default tokenizer.
2022-12-16 03:47:46 | INFO | absl | Using default tokenizer.
2022-12-16 03:48:28 | INFO | absl | Using default tokenizer.
2022-12-16 03:49:17 | INFO | absl | Using default tokenizer.
2022-12-16 03:49:54 | INFO | absl | Using default tokenizer.
2022-12-16 03:50:33 | INFO | absl | Using default tokenizer.
2022-12-16 03:51:17 | INFO | absl | Using default tokenizer.
2022-12-16 03:52:03 | INFO | absl | Using default tokenizer.
2022-12-16 03:52:49 | INFO | absl | Using default tokenizer.
2022-12-16 03:53:36 | INFO | absl | Using default tokenizer.
2022-12-16 03:54:23 | INFO | absl | Using default tokenizer.
2022-12-16 03:55:05 | INFO | absl | Using default tokenizer.
2022-12-16 03:55:52 | INFO | absl | Using default tokenizer.
2022-12-16 03:56:24 | INFO | valid | {"epoch": 13, "valid_loss": "3.575", "valid_nll_loss": "3.575", "valid_rouge1": 0.48169215686072087, "valid_rouge2": 0.13853034306293327, "valid_rougel": 0.22305231341921744, "valid_rouge_avg": 0.3101112499618271, "valid_ppl": "11.92", "valid_wps": "85.1", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1756", "valid_best_rouge_avg": 0.3101112499618271}
2022-12-16 03:56:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1756 updates
2022-12-16 03:56:24 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 03:56:44 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 03:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 13 @ 1756 updates, score 0.3101112499618271) (writing took 54.32132389699109 seconds)
2022-12-16 03:57:18 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-12-16 03:57:18 | INFO | train | {"epoch": 13, "train_loss": "1.751", "train_nll_loss": "1.751", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "3.37", "train_wps": "754.9", "train_ups": "0.05", "train_wpb": "13887.5", "train_bsz": "31.8", "train_num_updates": "1756", "train_lr": "8.67789e-05", "train_gnorm": "15.7", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "30490"}
2022-12-16 03:57:18 | INFO | fairseq.trainer | begin training epoch 14
2022-12-16 03:57:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 03:58:22 | INFO | train_inner | {"epoch": 14, "update": 13.029, "loss": "1.624", "nll_loss": "1.624", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "3.08", "wps": "87.9", "ups": "0.01", "wpb": "12868.5", "bsz": "29.6", "num_updates": "1760", "lr": "8.67368e-05", "gnorm": "11.206", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "30553"}
2022-12-16 03:59:41 | INFO | train_inner | {"epoch": 14, "update": 13.103, "loss": "1.447", "nll_loss": "1.447", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.73", "wps": "1764.6", "ups": "0.13", "wpb": "13960.6", "bsz": "32", "num_updates": "1770", "lr": "8.66316e-05", "gnorm": "15.788", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "30632"}
2022-12-16 04:01:01 | INFO | train_inner | {"epoch": 14, "update": 13.176, "loss": "1.513", "nll_loss": "1.513", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.85", "wps": "1752.8", "ups": "0.13", "wpb": "14008.3", "bsz": "32", "num_updates": "1780", "lr": "8.65263e-05", "gnorm": "18.901", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "30712"}
2022-12-16 04:02:20 | INFO | train_inner | {"epoch": 14, "update": 13.25, "loss": "1.455", "nll_loss": "1.455", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.74", "wps": "1739.5", "ups": "0.13", "wpb": "13697", "bsz": "32", "num_updates": "1790", "lr": "8.64211e-05", "gnorm": "8.37", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "30791"}
2022-12-16 04:03:41 | INFO | train_inner | {"epoch": 14, "update": 13.324, "loss": "1.512", "nll_loss": "1.512", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.85", "wps": "1759.7", "ups": "0.12", "wpb": "14234.4", "bsz": "32", "num_updates": "1800", "lr": "8.63158e-05", "gnorm": "8.373", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "30872"}
2022-12-16 04:05:00 | INFO | train_inner | {"epoch": 14, "update": 13.397, "loss": "1.489", "nll_loss": "1.489", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.81", "wps": "1761.1", "ups": "0.13", "wpb": "14059.2", "bsz": "32", "num_updates": "1810", "lr": "8.62105e-05", "gnorm": "6.049", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "30952"}
2022-12-16 04:06:20 | INFO | train_inner | {"epoch": 14, "update": 13.471, "loss": "1.478", "nll_loss": "1.478", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.78", "wps": "1786.2", "ups": "0.13", "wpb": "14149.9", "bsz": "32", "num_updates": "1820", "lr": "8.61053e-05", "gnorm": "4.928", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "31031"}
2022-12-16 04:07:40 | INFO | train_inner | {"epoch": 14, "update": 13.544, "loss": "1.503", "nll_loss": "1.503", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.83", "wps": "1729.1", "ups": "0.12", "wpb": "13937.5", "bsz": "32", "num_updates": "1830", "lr": "8.6e-05", "gnorm": "6.002", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "31111"}
2022-12-16 04:09:01 | INFO | train_inner | {"epoch": 14, "update": 13.618, "loss": "1.54", "nll_loss": "1.54", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.91", "wps": "1724.2", "ups": "0.12", "wpb": "13886.4", "bsz": "32", "num_updates": "1840", "lr": "8.58947e-05", "gnorm": "6.632", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "31192"}
2022-12-16 04:10:20 | INFO | train_inner | {"epoch": 14, "update": 13.691, "loss": "1.538", "nll_loss": "1.538", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.9", "wps": "1764.1", "ups": "0.13", "wpb": "14025.2", "bsz": "32", "num_updates": "1850", "lr": "8.57895e-05", "gnorm": "3.996", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "31271"}
2022-12-16 04:11:40 | INFO | train_inner | {"epoch": 14, "update": 13.765, "loss": "1.57", "nll_loss": "1.57", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.97", "wps": "1737.4", "ups": "0.13", "wpb": "13844.3", "bsz": "32", "num_updates": "1860", "lr": "8.56842e-05", "gnorm": "8.666", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "31351"}
2022-12-16 04:13:02 | INFO | train_inner | {"epoch": 14, "update": 13.838, "loss": "1.455", "nll_loss": "1.455", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.74", "wps": "1690.1", "ups": "0.12", "wpb": "13840.4", "bsz": "32", "num_updates": "1870", "lr": "8.55789e-05", "gnorm": "5.679", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "31433"}
2022-12-16 04:14:24 | INFO | train_inner | {"epoch": 14, "update": 13.912, "loss": "1.47", "nll_loss": "1.47", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.77", "wps": "1705.8", "ups": "0.12", "wpb": "13896.2", "bsz": "32", "num_updates": "1880", "lr": "8.54737e-05", "gnorm": "11.729", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "31515"}
2022-12-16 04:15:46 | INFO | train_inner | {"epoch": 14, "update": 13.985, "loss": "1.484", "nll_loss": "1.484", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.8", "wps": "1674.9", "ups": "0.12", "wpb": "13869.7", "bsz": "32", "num_updates": "1890", "lr": "8.53684e-05", "gnorm": "4.067", "clip": "100", "loss_scale": "0.0625", "train_wall": "83", "gb_free": "62.2", "wall": "31597"}
2022-12-16 04:16:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 04:16:45 | INFO | absl | Using default tokenizer.
2022-12-16 04:17:15 | INFO | absl | Using default tokenizer.
2022-12-16 04:17:43 | INFO | absl | Using default tokenizer.
2022-12-16 04:18:15 | INFO | absl | Using default tokenizer.
2022-12-16 04:18:44 | INFO | absl | Using default tokenizer.
2022-12-16 04:19:12 | INFO | absl | Using default tokenizer.
2022-12-16 04:19:39 | INFO | absl | Using default tokenizer.
2022-12-16 04:20:09 | INFO | absl | Using default tokenizer.
2022-12-16 04:20:52 | INFO | absl | Using default tokenizer.
2022-12-16 04:21:24 | INFO | absl | Using default tokenizer.
2022-12-16 04:22:04 | INFO | absl | Using default tokenizer.
2022-12-16 04:22:47 | INFO | absl | Using default tokenizer.
2022-12-16 04:23:17 | INFO | absl | Using default tokenizer.
2022-12-16 04:23:46 | INFO | absl | Using default tokenizer.
2022-12-16 04:24:29 | INFO | absl | Using default tokenizer.
2022-12-16 04:24:58 | INFO | absl | Using default tokenizer.
2022-12-16 04:25:30 | INFO | absl | Using default tokenizer.
2022-12-16 04:26:07 | INFO | absl | Using default tokenizer.
2022-12-16 04:26:37 | INFO | absl | Using default tokenizer.
2022-12-16 04:27:13 | INFO | absl | Using default tokenizer.
2022-12-16 04:27:46 | INFO | absl | Using default tokenizer.
2022-12-16 04:28:24 | INFO | absl | Using default tokenizer.
2022-12-16 04:28:55 | INFO | absl | Using default tokenizer.
2022-12-16 04:29:24 | INFO | absl | Using default tokenizer.
2022-12-16 04:29:54 | INFO | absl | Using default tokenizer.
2022-12-16 04:30:32 | INFO | absl | Using default tokenizer.
2022-12-16 04:31:05 | INFO | absl | Using default tokenizer.
2022-12-16 04:31:42 | INFO | absl | Using default tokenizer.
2022-12-16 04:32:20 | INFO | absl | Using default tokenizer.
2022-12-16 04:32:53 | INFO | absl | Using default tokenizer.
2022-12-16 04:33:30 | INFO | absl | Using default tokenizer.
2022-12-16 04:34:01 | INFO | valid | {"epoch": 14, "valid_loss": "3.774", "valid_nll_loss": "3.774", "valid_rouge1": 0.49196322702822054, "valid_rouge2": 0.14355638239968235, "valid_rougel": 0.22992559411332805, "valid_rouge_avg": 0.31775980471395143, "valid_ppl": "13.68", "valid_wps": "103.2", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "1892", "valid_best_rouge_avg": 0.31775980471395143}
2022-12-16 04:34:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1892 updates
2022-12-16 04:34:01 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 04:34:20 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 04:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 14 @ 1892 updates, score 0.31775980471395143) (writing took 36.65445089014247 seconds)
2022-12-16 04:34:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-12-16 04:34:37 | INFO | train | {"epoch": 14, "train_loss": "1.494", "train_nll_loss": "1.494", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "2.82", "train_wps": "842.9", "train_ups": "0.06", "train_wpb": "13877.4", "train_bsz": "31.8", "train_num_updates": "1892", "train_lr": "8.53474e-05", "train_gnorm": "8.388", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "32729"}
2022-12-16 04:34:37 | INFO | fairseq.trainer | begin training epoch 15
2022-12-16 04:34:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 04:36:18 | INFO | train_inner | {"epoch": 15, "update": 14.059, "loss": "1.232", "nll_loss": "1.232", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.35", "wps": "103.7", "ups": "0.01", "wpb": "12768.9", "bsz": "29.6", "num_updates": "1900", "lr": "8.52632e-05", "gnorm": "4.804", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "32829"}
2022-12-16 04:37:39 | INFO | train_inner | {"epoch": 15, "update": 14.132, "loss": "1.247", "nll_loss": "1.247", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.37", "wps": "1750.9", "ups": "0.12", "wpb": "14235.8", "bsz": "32", "num_updates": "1910", "lr": "8.51579e-05", "gnorm": "4.547", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "32910"}
2022-12-16 04:38:58 | INFO | train_inner | {"epoch": 15, "update": 14.206, "loss": "1.262", "nll_loss": "1.262", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.4", "wps": "1783.4", "ups": "0.13", "wpb": "14153.4", "bsz": "32", "num_updates": "1920", "lr": "8.50526e-05", "gnorm": "5.604", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "32989"}
2022-12-16 04:40:17 | INFO | train_inner | {"epoch": 15, "update": 14.279, "loss": "1.257", "nll_loss": "1.257", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.39", "wps": "1772.7", "ups": "0.13", "wpb": "14011.8", "bsz": "32", "num_updates": "1930", "lr": "8.49474e-05", "gnorm": "5.897", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.9", "wall": "33068"}
2022-12-16 04:41:39 | INFO | train_inner | {"epoch": 15, "update": 14.353, "loss": "1.268", "nll_loss": "1.268", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.41", "wps": "1679.2", "ups": "0.12", "wpb": "13671.7", "bsz": "32", "num_updates": "1940", "lr": "8.48421e-05", "gnorm": "5.628", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "33150"}
2022-12-16 04:42:59 | INFO | train_inner | {"epoch": 15, "update": 14.426, "loss": "1.285", "nll_loss": "1.285", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.44", "wps": "1699.9", "ups": "0.12", "wpb": "13616.6", "bsz": "32", "num_updates": "1950", "lr": "8.47368e-05", "gnorm": "6.391", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "33230"}
2022-12-16 04:44:19 | INFO | train_inner | {"epoch": 15, "update": 14.5, "loss": "1.289", "nll_loss": "1.289", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.44", "wps": "1747.7", "ups": "0.13", "wpb": "13955.3", "bsz": "32", "num_updates": "1960", "lr": "8.46316e-05", "gnorm": "7.204", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "33310"}
2022-12-16 04:45:39 | INFO | train_inner | {"epoch": 15, "update": 14.574, "loss": "1.296", "nll_loss": "1.296", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.46", "wps": "1749.6", "ups": "0.13", "wpb": "13973.4", "bsz": "32", "num_updates": "1970", "lr": "8.45263e-05", "gnorm": "3.957", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "33390"}
2022-12-16 04:47:00 | INFO | train_inner | {"epoch": 15, "update": 14.647, "loss": "1.323", "nll_loss": "1.323", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.5", "wps": "1746.7", "ups": "0.12", "wpb": "14121.3", "bsz": "32", "num_updates": "1980", "lr": "8.44211e-05", "gnorm": "5.731", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "33471"}
2022-12-16 04:48:21 | INFO | train_inner | {"epoch": 15, "update": 14.721, "loss": "1.375", "nll_loss": "1.375", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.59", "wps": "1721.8", "ups": "0.12", "wpb": "14009.1", "bsz": "32", "num_updates": "1990", "lr": "8.43158e-05", "gnorm": "6.747", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "33552"}
2022-12-16 04:49:40 | INFO | train_inner | {"epoch": 15, "update": 14.794, "loss": "1.376", "nll_loss": "1.376", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.59", "wps": "1769.5", "ups": "0.13", "wpb": "14007.9", "bsz": "32", "num_updates": "2000", "lr": "8.42105e-05", "gnorm": "7.258", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "63", "wall": "33631"}
2022-12-16 04:51:00 | INFO | train_inner | {"epoch": 15, "update": 14.868, "loss": "1.375", "nll_loss": "1.375", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.59", "wps": "1725.4", "ups": "0.12", "wpb": "13817", "bsz": "32", "num_updates": "2010", "lr": "8.41053e-05", "gnorm": "6.089", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "33711"}
2022-12-16 04:52:21 | INFO | train_inner | {"epoch": 15, "update": 14.941, "loss": "1.309", "nll_loss": "1.309", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.48", "wps": "1731.8", "ups": "0.12", "wpb": "14042.4", "bsz": "32", "num_updates": "2020", "lr": "8.4e-05", "gnorm": "3.03", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "33792"}
2022-12-16 04:53:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 04:54:11 | INFO | absl | Using default tokenizer.
2022-12-16 04:54:38 | INFO | absl | Using default tokenizer.
2022-12-16 04:55:07 | INFO | absl | Using default tokenizer.
2022-12-16 04:55:39 | INFO | absl | Using default tokenizer.
2022-12-16 04:56:09 | INFO | absl | Using default tokenizer.
2022-12-16 04:56:40 | INFO | absl | Using default tokenizer.
2022-12-16 04:57:09 | INFO | absl | Using default tokenizer.
2022-12-16 04:57:52 | INFO | absl | Using default tokenizer.
2022-12-16 04:58:26 | INFO | absl | Using default tokenizer.
2022-12-16 04:58:59 | INFO | absl | Using default tokenizer.
2022-12-16 04:59:37 | INFO | absl | Using default tokenizer.
2022-12-16 05:00:11 | INFO | absl | Using default tokenizer.
2022-12-16 05:00:44 | INFO | absl | Using default tokenizer.
2022-12-16 05:01:28 | INFO | absl | Using default tokenizer.
2022-12-16 05:01:56 | INFO | absl | Using default tokenizer.
2022-12-16 05:02:26 | INFO | absl | Using default tokenizer.
2022-12-16 05:02:58 | INFO | absl | Using default tokenizer.
2022-12-16 05:03:31 | INFO | absl | Using default tokenizer.
2022-12-16 05:04:01 | INFO | absl | Using default tokenizer.
2022-12-16 05:04:37 | INFO | absl | Using default tokenizer.
2022-12-16 05:05:07 | INFO | absl | Using default tokenizer.
2022-12-16 05:05:57 | INFO | absl | Using default tokenizer.
2022-12-16 05:06:35 | INFO | absl | Using default tokenizer.
2022-12-16 05:07:09 | INFO | absl | Using default tokenizer.
2022-12-16 05:07:41 | INFO | absl | Using default tokenizer.
2022-12-16 05:08:29 | INFO | absl | Using default tokenizer.
2022-12-16 05:09:02 | INFO | absl | Using default tokenizer.
2022-12-16 05:09:39 | INFO | absl | Using default tokenizer.
2022-12-16 05:10:18 | INFO | absl | Using default tokenizer.
2022-12-16 05:10:52 | INFO | absl | Using default tokenizer.
2022-12-16 05:11:23 | INFO | absl | Using default tokenizer.
2022-12-16 05:11:49 | INFO | valid | {"epoch": 15, "valid_loss": "3.859", "valid_nll_loss": "3.859", "valid_rouge1": 0.4967411974715807, "valid_rouge2": 0.1422972153441642, "valid_rougel": 0.22757293472449272, "valid_rouge_avg": 0.3195192064078724, "valid_ppl": "14.51", "valid_wps": "100.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2028", "valid_best_rouge_avg": 0.3195192064078724}
2022-12-16 05:11:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2028 updates
2022-12-16 05:11:49 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 05:12:06 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 05:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 15 @ 2028 updates, score 0.3195192064078724) (writing took 36.65764026995748 seconds)
2022-12-16 05:12:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-12-16 05:12:25 | INFO | train | {"epoch": 15, "train_loss": "1.301", "train_nll_loss": "1.301", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "2.46", "train_wps": "832.2", "train_ups": "0.06", "train_wpb": "13879.1", "train_bsz": "31.8", "train_num_updates": "2028", "train_lr": "8.39158e-05", "train_gnorm": "5.536", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1089", "train_gb_free": "63", "train_wall": "34997"}
2022-12-16 05:12:25 | INFO | fairseq.trainer | begin training epoch 16
2022-12-16 05:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 05:13:18 | INFO | train_inner | {"epoch": 16, "update": 15.015, "loss": "1.285", "nll_loss": "1.285", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.44", "wps": "103.2", "ups": "0.01", "wpb": "12974.1", "bsz": "29.6", "num_updates": "2030", "lr": "8.38947e-05", "gnorm": "4.385", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "35049"}
2022-12-16 05:14:40 | INFO | train_inner | {"epoch": 16, "update": 15.088, "loss": "1.115", "nll_loss": "1.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.17", "wps": "1718.6", "ups": "0.12", "wpb": "14100.5", "bsz": "32", "num_updates": "2040", "lr": "8.37895e-05", "gnorm": "2.659", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "35131"}
2022-12-16 05:16:00 | INFO | train_inner | {"epoch": 16, "update": 15.162, "loss": "1.148", "nll_loss": "1.148", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.22", "wps": "1764.8", "ups": "0.13", "wpb": "14000.7", "bsz": "32", "num_updates": "2050", "lr": "8.36842e-05", "gnorm": "8.806", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.9", "wall": "35211"}
2022-12-16 05:17:19 | INFO | train_inner | {"epoch": 16, "update": 15.235, "loss": "1.136", "nll_loss": "1.136", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.2", "wps": "1742.7", "ups": "0.13", "wpb": "13890.2", "bsz": "32", "num_updates": "2060", "lr": "8.35789e-05", "gnorm": "2.975", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "63", "wall": "35290"}
2022-12-16 05:18:39 | INFO | train_inner | {"epoch": 16, "update": 15.309, "loss": "1.18", "nll_loss": "1.18", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.27", "wps": "1748", "ups": "0.13", "wpb": "13954", "bsz": "32", "num_updates": "2070", "lr": "8.34737e-05", "gnorm": "4.362", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "35370"}
2022-12-16 05:20:00 | INFO | train_inner | {"epoch": 16, "update": 15.382, "loss": "1.178", "nll_loss": "1.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.26", "wps": "1753", "ups": "0.12", "wpb": "14135.7", "bsz": "32", "num_updates": "2080", "lr": "8.33684e-05", "gnorm": "4.297", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "35451"}
2022-12-16 05:21:20 | INFO | train_inner | {"epoch": 16, "update": 15.456, "loss": "1.143", "nll_loss": "1.143", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.21", "wps": "1765.8", "ups": "0.12", "wpb": "14219", "bsz": "32", "num_updates": "2090", "lr": "8.32632e-05", "gnorm": "3.096", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "35531"}
2022-12-16 05:22:42 | INFO | train_inner | {"epoch": 16, "update": 15.529, "loss": "1.182", "nll_loss": "1.182", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.27", "wps": "1735.1", "ups": "0.12", "wpb": "14235.3", "bsz": "32", "num_updates": "2100", "lr": "8.31579e-05", "gnorm": "2.801", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "35614"}
2022-12-16 05:24:00 | INFO | train_inner | {"epoch": 16, "update": 15.603, "loss": "1.182", "nll_loss": "1.182", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.27", "wps": "1779.2", "ups": "0.13", "wpb": "13802.3", "bsz": "32", "num_updates": "2110", "lr": "8.30526e-05", "gnorm": "18.335", "clip": "100", "loss_scale": "0.0625", "train_wall": "77", "gb_free": "62.2", "wall": "35691"}
2022-12-16 05:25:21 | INFO | train_inner | {"epoch": 16, "update": 15.676, "loss": "1.199", "nll_loss": "1.199", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.3", "wps": "1701.3", "ups": "0.12", "wpb": "13817.8", "bsz": "32", "num_updates": "2120", "lr": "8.29474e-05", "gnorm": "6.517", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "65", "wall": "35772"}
2022-12-16 05:26:40 | INFO | train_inner | {"epoch": 16, "update": 15.75, "loss": "1.206", "nll_loss": "1.206", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.31", "wps": "1747.6", "ups": "0.13", "wpb": "13805.5", "bsz": "32", "num_updates": "2130", "lr": "8.28421e-05", "gnorm": "6.349", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "35851"}
2022-12-16 05:27:59 | INFO | train_inner | {"epoch": 16, "update": 15.824, "loss": "1.174", "nll_loss": "1.174", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.26", "wps": "1726.5", "ups": "0.13", "wpb": "13664.3", "bsz": "32", "num_updates": "2140", "lr": "8.27368e-05", "gnorm": "4.088", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "35931"}
2022-12-16 05:29:20 | INFO | train_inner | {"epoch": 16, "update": 15.897, "loss": "1.176", "nll_loss": "1.176", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.26", "wps": "1740.6", "ups": "0.12", "wpb": "13930.2", "bsz": "32", "num_updates": "2150", "lr": "8.26316e-05", "gnorm": "3.226", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63.6", "wall": "36011"}
2022-12-16 05:30:40 | INFO | train_inner | {"epoch": 16, "update": 15.971, "loss": "1.187", "nll_loss": "1.187", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.28", "wps": "1739.5", "ups": "0.12", "wpb": "13929.6", "bsz": "32", "num_updates": "2160", "lr": "8.25263e-05", "gnorm": "4.634", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "36091"}
2022-12-16 05:31:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 05:31:58 | INFO | absl | Using default tokenizer.
2022-12-16 05:32:38 | INFO | absl | Using default tokenizer.
2022-12-16 05:33:04 | INFO | absl | Using default tokenizer.
2022-12-16 05:33:41 | INFO | absl | Using default tokenizer.
2022-12-16 05:34:18 | INFO | absl | Using default tokenizer.
2022-12-16 05:34:46 | INFO | absl | Using default tokenizer.
2022-12-16 05:35:28 | INFO | absl | Using default tokenizer.
2022-12-16 05:35:58 | INFO | absl | Using default tokenizer.
2022-12-16 05:36:27 | INFO | absl | Using default tokenizer.
2022-12-16 05:37:01 | INFO | absl | Using default tokenizer.
2022-12-16 05:37:33 | INFO | absl | Using default tokenizer.
2022-12-16 05:38:17 | INFO | absl | Using default tokenizer.
2022-12-16 05:38:46 | INFO | absl | Using default tokenizer.
2022-12-16 05:39:16 | INFO | absl | Using default tokenizer.
2022-12-16 05:39:45 | INFO | absl | Using default tokenizer.
2022-12-16 05:40:14 | INFO | absl | Using default tokenizer.
2022-12-16 05:40:48 | INFO | absl | Using default tokenizer.
2022-12-16 05:41:20 | INFO | absl | Using default tokenizer.
2022-12-16 05:42:04 | INFO | absl | Using default tokenizer.
2022-12-16 05:42:35 | INFO | absl | Using default tokenizer.
2022-12-16 05:43:07 | INFO | absl | Using default tokenizer.
2022-12-16 05:43:45 | INFO | absl | Using default tokenizer.
2022-12-16 05:44:25 | INFO | absl | Using default tokenizer.
2022-12-16 05:44:54 | INFO | absl | Using default tokenizer.
2022-12-16 05:45:30 | INFO | absl | Using default tokenizer.
2022-12-16 05:46:02 | INFO | absl | Using default tokenizer.
2022-12-16 05:46:46 | INFO | absl | Using default tokenizer.
2022-12-16 05:47:18 | INFO | absl | Using default tokenizer.
2022-12-16 05:47:52 | INFO | absl | Using default tokenizer.
2022-12-16 05:48:22 | INFO | absl | Using default tokenizer.
2022-12-16 05:49:08 | INFO | absl | Using default tokenizer.
2022-12-16 05:49:49 | INFO | valid | {"epoch": 16, "valid_loss": "4.026", "valid_nll_loss": "4.026", "valid_rouge1": 0.48081630333517666, "valid_rouge2": 0.13466726221316025, "valid_rougel": 0.22055725544876942, "valid_rouge_avg": 0.3077417827741685, "valid_ppl": "16.3", "valid_wps": "100.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2164", "valid_best_rouge_avg": 0.3195192064078724}
2022-12-16 05:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2164 updates
2022-12-16 05:49:49 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 05:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 05:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 16 @ 2164 updates, score 0.3077417827741685) (writing took 17.776480769971386 seconds)
2022-12-16 05:50:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-12-16 05:50:07 | INFO | train | {"epoch": 16, "train_loss": "1.169", "train_nll_loss": "1.169", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "2.25", "train_wps": "834.8", "train_ups": "0.06", "train_wpb": "13881.3", "train_bsz": "31.8", "train_num_updates": "2164", "train_lr": "8.24842e-05", "train_gnorm": "5.644", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "37258"}
2022-12-16 05:50:07 | INFO | fairseq.trainer | begin training epoch 17
2022-12-16 05:50:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 05:51:33 | INFO | train_inner | {"epoch": 17, "update": 16.044, "loss": "1.089", "nll_loss": "1.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.13", "wps": "103.5", "ups": "0.01", "wpb": "12967.8", "bsz": "29.6", "num_updates": "2170", "lr": "8.24211e-05", "gnorm": "7.218", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "37344"}
2022-12-16 05:52:53 | INFO | train_inner | {"epoch": 17, "update": 16.118, "loss": "1.01", "nll_loss": "1.01", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.01", "wps": "1737.1", "ups": "0.13", "wpb": "13861.8", "bsz": "32", "num_updates": "2180", "lr": "8.23158e-05", "gnorm": "3.182", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "37424"}
2022-12-16 05:54:13 | INFO | train_inner | {"epoch": 17, "update": 16.191, "loss": "0.986", "nll_loss": "0.986", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.98", "wps": "1743.3", "ups": "0.12", "wpb": "13981.1", "bsz": "32", "num_updates": "2190", "lr": "8.22105e-05", "gnorm": "6.932", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "37504"}
2022-12-16 05:55:32 | INFO | train_inner | {"epoch": 17, "update": 16.265, "loss": "0.95", "nll_loss": "0.95", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.93", "wps": "1737.9", "ups": "0.13", "wpb": "13687.9", "bsz": "32", "num_updates": "2200", "lr": "8.21053e-05", "gnorm": "2.651", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "37583"}
2022-12-16 05:56:51 | INFO | train_inner | {"epoch": 17, "update": 16.338, "loss": "0.988", "nll_loss": "0.988", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.98", "wps": "1799.4", "ups": "0.13", "wpb": "14336.5", "bsz": "32", "num_updates": "2210", "lr": "8.2e-05", "gnorm": "8.911", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "37663"}
2022-12-16 05:58:13 | INFO | train_inner | {"epoch": 17, "update": 16.412, "loss": "1.033", "nll_loss": "1.033", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.05", "wps": "1696.3", "ups": "0.12", "wpb": "13765.9", "bsz": "32", "num_updates": "2220", "lr": "8.18947e-05", "gnorm": "5.505", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "37744"}
2022-12-16 05:59:33 | INFO | train_inner | {"epoch": 17, "update": 16.485, "loss": "1.016", "nll_loss": "1.016", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.02", "wps": "1718.6", "ups": "0.13", "wpb": "13723.5", "bsz": "32", "num_updates": "2230", "lr": "8.17895e-05", "gnorm": "3.189", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "37824"}
2022-12-16 06:00:54 | INFO | train_inner | {"epoch": 17, "update": 16.559, "loss": "1.043", "nll_loss": "1.043", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.06", "wps": "1714.5", "ups": "0.12", "wpb": "13876.5", "bsz": "32", "num_updates": "2240", "lr": "8.16842e-05", "gnorm": "4.356", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "37905"}
2022-12-16 06:02:14 | INFO | train_inner | {"epoch": 17, "update": 16.632, "loss": "1.015", "nll_loss": "1.015", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.02", "wps": "1737.7", "ups": "0.12", "wpb": "13993.5", "bsz": "32", "num_updates": "2250", "lr": "8.15789e-05", "gnorm": "6.834", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "37985"}
2022-12-16 06:03:34 | INFO | train_inner | {"epoch": 17, "update": 16.706, "loss": "1.033", "nll_loss": "1.033", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.05", "wps": "1774.1", "ups": "0.13", "wpb": "14123.5", "bsz": "32", "num_updates": "2260", "lr": "8.14737e-05", "gnorm": "3.608", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "38065"}
2022-12-16 06:04:56 | INFO | train_inner | {"epoch": 17, "update": 16.779, "loss": "1.067", "nll_loss": "1.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.1", "wps": "1668.1", "ups": "0.12", "wpb": "13666.7", "bsz": "32", "num_updates": "2270", "lr": "8.13684e-05", "gnorm": "5.146", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "38147"}
2022-12-16 06:06:16 | INFO | train_inner | {"epoch": 17, "update": 16.853, "loss": "1.054", "nll_loss": "1.054", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.08", "wps": "1751.4", "ups": "0.12", "wpb": "14158.4", "bsz": "32", "num_updates": "2280", "lr": "8.12632e-05", "gnorm": "3.43", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "38228"}
2022-12-16 06:07:38 | INFO | train_inner | {"epoch": 17, "update": 16.926, "loss": "1.087", "nll_loss": "1.087", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.12", "wps": "1726.2", "ups": "0.12", "wpb": "14021", "bsz": "32", "num_updates": "2290", "lr": "8.11579e-05", "gnorm": "5.112", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "38309"}
2022-12-16 06:08:59 | INFO | train_inner | {"epoch": 17, "update": 17.0, "loss": "1.058", "nll_loss": "1.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.08", "wps": "1605.4", "ups": "0.12", "wpb": "13048.6", "bsz": "29.6", "num_updates": "2300", "lr": "8.10526e-05", "gnorm": "4.891", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.9", "wall": "38390"}
2022-12-16 06:08:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 06:09:43 | INFO | absl | Using default tokenizer.
2022-12-16 06:10:11 | INFO | absl | Using default tokenizer.
2022-12-16 06:10:44 | INFO | absl | Using default tokenizer.
2022-12-16 06:11:21 | INFO | absl | Using default tokenizer.
2022-12-16 06:11:49 | INFO | absl | Using default tokenizer.
2022-12-16 06:12:24 | INFO | absl | Using default tokenizer.
2022-12-16 06:13:07 | INFO | absl | Using default tokenizer.
2022-12-16 06:13:40 | INFO | absl | Using default tokenizer.
2022-12-16 06:14:22 | INFO | absl | Using default tokenizer.
2022-12-16 06:14:54 | INFO | absl | Using default tokenizer.
2022-12-16 06:15:32 | INFO | absl | Using default tokenizer.
2022-12-16 06:16:09 | INFO | absl | Using default tokenizer.
2022-12-16 06:16:41 | INFO | absl | Using default tokenizer.
2022-12-16 06:17:19 | INFO | absl | Using default tokenizer.
2022-12-16 06:17:52 | INFO | absl | Using default tokenizer.
2022-12-16 06:18:25 | INFO | absl | Using default tokenizer.
2022-12-16 06:18:56 | INFO | absl | Using default tokenizer.
2022-12-16 06:19:30 | INFO | absl | Using default tokenizer.
2022-12-16 06:20:07 | INFO | absl | Using default tokenizer.
2022-12-16 06:20:41 | INFO | absl | Using default tokenizer.
2022-12-16 06:21:14 | INFO | absl | Using default tokenizer.
2022-12-16 06:21:54 | INFO | absl | Using default tokenizer.
2022-12-16 06:22:27 | INFO | absl | Using default tokenizer.
2022-12-16 06:23:01 | INFO | absl | Using default tokenizer.
2022-12-16 06:23:45 | INFO | absl | Using default tokenizer.
2022-12-16 06:24:21 | INFO | absl | Using default tokenizer.
2022-12-16 06:24:56 | INFO | absl | Using default tokenizer.
2022-12-16 06:25:34 | INFO | absl | Using default tokenizer.
2022-12-16 06:26:19 | INFO | absl | Using default tokenizer.
2022-12-16 06:26:59 | INFO | absl | Using default tokenizer.
2022-12-16 06:27:34 | INFO | absl | Using default tokenizer.
2022-12-16 06:28:02 | INFO | valid | {"epoch": 17, "valid_loss": "4.178", "valid_nll_loss": "4.178", "valid_rouge1": 0.5024238580254049, "valid_rouge2": 0.1467389966672778, "valid_rougel": 0.228663356197592, "valid_rouge_avg": 0.3245814273463413, "valid_ppl": "18.11", "valid_wps": "96.9", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2300", "valid_best_rouge_avg": 0.3245814273463413}
2022-12-16 06:28:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2300 updates
2022-12-16 06:28:02 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 06:28:22 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 06:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 17 @ 2300 updates, score 0.3245814273463413) (writing took 37.918061428004876 seconds)
2022-12-16 06:28:40 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-12-16 06:28:40 | INFO | train | {"epoch": 17, "train_loss": "1.026", "train_nll_loss": "1.026", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "2.04", "train_wps": "816.1", "train_ups": "0.06", "train_wpb": "13879.2", "train_bsz": "31.8", "train_num_updates": "2300", "train_lr": "8.10526e-05", "train_gnorm": "4.904", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1088", "train_gb_free": "62.9", "train_wall": "39571"}
2022-12-16 06:28:40 | INFO | fairseq.trainer | begin training epoch 18
2022-12-16 06:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 06:30:33 | INFO | train_inner | {"epoch": 18, "update": 17.074, "loss": "0.892", "nll_loss": "0.892", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.86", "wps": "106.7", "ups": "0.01", "wpb": "13809.7", "bsz": "32", "num_updates": "2310", "lr": "8.09474e-05", "gnorm": "5.21", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "39684"}
2022-12-16 06:31:56 | INFO | train_inner | {"epoch": 18, "update": 17.147, "loss": "0.906", "nll_loss": "0.906", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.87", "wps": "1706.8", "ups": "0.12", "wpb": "14036.8", "bsz": "32", "num_updates": "2320", "lr": "8.08421e-05", "gnorm": "3.386", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "39767"}
2022-12-16 06:33:17 | INFO | train_inner | {"epoch": 18, "update": 17.221, "loss": "0.913", "nll_loss": "0.913", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.88", "wps": "1735", "ups": "0.12", "wpb": "14082.5", "bsz": "32", "num_updates": "2330", "lr": "8.07368e-05", "gnorm": "2.192", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.9", "wall": "39848"}
2022-12-16 06:34:35 | INFO | train_inner | {"epoch": 18, "update": 17.294, "loss": "0.935", "nll_loss": "0.935", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.91", "wps": "1778.2", "ups": "0.13", "wpb": "13873.1", "bsz": "32", "num_updates": "2340", "lr": "8.06316e-05", "gnorm": "3.165", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "39926"}
2022-12-16 06:35:55 | INFO | train_inner | {"epoch": 18, "update": 17.368, "loss": "0.926", "nll_loss": "0.926", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.9", "wps": "1744", "ups": "0.12", "wpb": "14003.7", "bsz": "32", "num_updates": "2350", "lr": "8.05263e-05", "gnorm": "11.273", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "40006"}
2022-12-16 06:37:15 | INFO | train_inner | {"epoch": 18, "update": 17.441, "loss": "0.907", "nll_loss": "0.907", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.88", "wps": "1747.1", "ups": "0.12", "wpb": "14015.6", "bsz": "32", "num_updates": "2360", "lr": "8.04211e-05", "gnorm": "2.404", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "40086"}
2022-12-16 06:38:34 | INFO | train_inner | {"epoch": 18, "update": 17.515, "loss": "0.952", "nll_loss": "0.952", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.93", "wps": "1795.2", "ups": "0.13", "wpb": "14127.6", "bsz": "32", "num_updates": "2370", "lr": "8.03158e-05", "gnorm": "7.09", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "40165"}
2022-12-16 06:39:54 | INFO | train_inner | {"epoch": 18, "update": 17.588, "loss": "0.958", "nll_loss": "0.958", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.94", "wps": "1742.3", "ups": "0.12", "wpb": "14009.1", "bsz": "32", "num_updates": "2380", "lr": "8.02105e-05", "gnorm": "8.235", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "40246"}
2022-12-16 06:41:15 | INFO | train_inner | {"epoch": 18, "update": 17.662, "loss": "0.946", "nll_loss": "0.946", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.93", "wps": "1720.8", "ups": "0.12", "wpb": "13860.4", "bsz": "32", "num_updates": "2390", "lr": "8.01053e-05", "gnorm": "5.376", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "40326"}
2022-12-16 06:42:35 | INFO | train_inner | {"epoch": 18, "update": 17.735, "loss": "0.897", "nll_loss": "0.897", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.86", "wps": "1749.2", "ups": "0.13", "wpb": "13981", "bsz": "32", "num_updates": "2400", "lr": "8e-05", "gnorm": "4.048", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "40406"}
2022-12-16 06:43:56 | INFO | train_inner | {"epoch": 18, "update": 17.809, "loss": "0.943", "nll_loss": "0.943", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.92", "wps": "1683.1", "ups": "0.12", "wpb": "13698.1", "bsz": "32", "num_updates": "2410", "lr": "7.98947e-05", "gnorm": "3.684", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "40487"}
2022-12-16 06:45:17 | INFO | train_inner | {"epoch": 18, "update": 17.882, "loss": "0.987", "nll_loss": "0.987", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.98", "wps": "1733.3", "ups": "0.12", "wpb": "13970.8", "bsz": "32", "num_updates": "2420", "lr": "7.97895e-05", "gnorm": "3.254", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "40568"}
2022-12-16 06:46:38 | INFO | train_inner | {"epoch": 18, "update": 17.956, "loss": "1.008", "nll_loss": "1.008", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "2.01", "wps": "1727.5", "ups": "0.12", "wpb": "14049.7", "bsz": "32", "num_updates": "2430", "lr": "7.96842e-05", "gnorm": "3.378", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "40649"}
2022-12-16 06:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 06:48:12 | INFO | absl | Using default tokenizer.
2022-12-16 06:48:40 | INFO | absl | Using default tokenizer.
2022-12-16 06:49:08 | INFO | absl | Using default tokenizer.
2022-12-16 06:49:40 | INFO | absl | Using default tokenizer.
2022-12-16 06:50:09 | INFO | absl | Using default tokenizer.
2022-12-16 06:50:44 | INFO | absl | Using default tokenizer.
2022-12-16 06:51:15 | INFO | absl | Using default tokenizer.
2022-12-16 06:51:49 | INFO | absl | Using default tokenizer.
2022-12-16 06:52:21 | INFO | absl | Using default tokenizer.
2022-12-16 06:52:56 | INFO | absl | Using default tokenizer.
2022-12-16 06:53:28 | INFO | absl | Using default tokenizer.
2022-12-16 06:54:02 | INFO | absl | Using default tokenizer.
2022-12-16 06:54:46 | INFO | absl | Using default tokenizer.
2022-12-16 06:55:16 | INFO | absl | Using default tokenizer.
2022-12-16 06:55:46 | INFO | absl | Using default tokenizer.
2022-12-16 06:56:16 | INFO | absl | Using default tokenizer.
2022-12-16 06:56:45 | INFO | absl | Using default tokenizer.
2022-12-16 06:57:16 | INFO | absl | Using default tokenizer.
2022-12-16 06:57:49 | INFO | absl | Using default tokenizer.
2022-12-16 06:58:18 | INFO | absl | Using default tokenizer.
2022-12-16 06:58:52 | INFO | absl | Using default tokenizer.
2022-12-16 06:59:30 | INFO | absl | Using default tokenizer.
2022-12-16 06:59:59 | INFO | absl | Using default tokenizer.
2022-12-16 07:00:34 | INFO | absl | Using default tokenizer.
2022-12-16 07:01:05 | INFO | absl | Using default tokenizer.
2022-12-16 07:01:37 | INFO | absl | Using default tokenizer.
2022-12-16 07:02:17 | INFO | absl | Using default tokenizer.
2022-12-16 07:02:53 | INFO | absl | Using default tokenizer.
2022-12-16 07:03:30 | INFO | absl | Using default tokenizer.
2022-12-16 07:04:02 | INFO | absl | Using default tokenizer.
2022-12-16 07:04:35 | INFO | absl | Using default tokenizer.
2022-12-16 07:05:01 | INFO | valid | {"epoch": 18, "valid_loss": "4.301", "valid_nll_loss": "4.301", "valid_rouge1": 0.5086247829066876, "valid_rouge2": 0.1472313762723476, "valid_rougel": 0.22720944656297443, "valid_rouge_avg": 0.3279280795895176, "valid_ppl": "19.71", "valid_wps": "105.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2436", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 07:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2436 updates
2022-12-16 07:05:01 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 07:05:19 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 07:05:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 18 @ 2436 updates, score 0.3279280795895176) (writing took 51.69796625385061 seconds)
2022-12-16 07:05:52 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-12-16 07:05:52 | INFO | train | {"epoch": 18, "train_loss": "0.938", "train_nll_loss": "0.938", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.92", "train_wps": "845.5", "train_ups": "0.06", "train_wpb": "13879.6", "train_bsz": "31.8", "train_num_updates": "2436", "train_lr": "7.96211e-05", "train_gnorm": "4.857", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "41803"}
2022-12-16 07:05:52 | INFO | fairseq.trainer | begin training epoch 19
2022-12-16 07:05:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 07:06:55 | INFO | train_inner | {"epoch": 19, "update": 18.029, "loss": "0.909", "nll_loss": "0.909", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.88", "wps": "105.5", "ups": "0.01", "wpb": "12841.5", "bsz": "29.6", "num_updates": "2440", "lr": "7.95789e-05", "gnorm": "4.121", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "41866"}
2022-12-16 07:08:16 | INFO | train_inner | {"epoch": 19, "update": 18.103, "loss": "0.79", "nll_loss": "0.79", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.73", "wps": "1737.3", "ups": "0.12", "wpb": "14037.3", "bsz": "32", "num_updates": "2450", "lr": "7.94737e-05", "gnorm": "2.998", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "41947"}
2022-12-16 07:09:36 | INFO | train_inner | {"epoch": 19, "update": 18.176, "loss": "0.771", "nll_loss": "0.771", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.71", "wps": "1756.3", "ups": "0.13", "wpb": "14038.6", "bsz": "32", "num_updates": "2460", "lr": "7.93684e-05", "gnorm": "2.392", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "42027"}
2022-12-16 07:10:57 | INFO | train_inner | {"epoch": 19, "update": 18.25, "loss": "0.79", "nll_loss": "0.79", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.73", "wps": "1715.6", "ups": "0.12", "wpb": "13900.8", "bsz": "32", "num_updates": "2470", "lr": "7.92632e-05", "gnorm": "3.373", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "42108"}
2022-12-16 07:12:18 | INFO | train_inner | {"epoch": 19, "update": 18.324, "loss": "0.794", "nll_loss": "0.794", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.73", "wps": "1719.1", "ups": "0.12", "wpb": "13851", "bsz": "32", "num_updates": "2480", "lr": "7.91579e-05", "gnorm": "4.734", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "42189"}
2022-12-16 07:13:38 | INFO | train_inner | {"epoch": 19, "update": 18.397, "loss": "0.805", "nll_loss": "0.805", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.75", "wps": "1760.6", "ups": "0.12", "wpb": "14115.6", "bsz": "32", "num_updates": "2490", "lr": "7.90526e-05", "gnorm": "17.071", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "42269"}
2022-12-16 07:14:59 | INFO | train_inner | {"epoch": 19, "update": 18.471, "loss": "0.804", "nll_loss": "0.804", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.75", "wps": "1701.9", "ups": "0.12", "wpb": "13806.6", "bsz": "32", "num_updates": "2500", "lr": "7.89474e-05", "gnorm": "3.461", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "42350"}
2022-12-16 07:16:17 | INFO | train_inner | {"epoch": 19, "update": 18.544, "loss": "0.81", "nll_loss": "0.81", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.75", "wps": "1805.1", "ups": "0.13", "wpb": "14060", "bsz": "32", "num_updates": "2510", "lr": "7.88421e-05", "gnorm": "3.461", "clip": "100", "loss_scale": "0.0625", "train_wall": "77", "gb_free": "62.2", "wall": "42428"}
2022-12-16 07:17:36 | INFO | train_inner | {"epoch": 19, "update": 18.618, "loss": "0.821", "nll_loss": "0.821", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.77", "wps": "1783.3", "ups": "0.13", "wpb": "14074.2", "bsz": "32", "num_updates": "2520", "lr": "7.87368e-05", "gnorm": "3.5", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "42507"}
2022-12-16 07:18:57 | INFO | train_inner | {"epoch": 19, "update": 18.691, "loss": "0.844", "nll_loss": "0.844", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.8", "wps": "1695.3", "ups": "0.12", "wpb": "13819.8", "bsz": "32", "num_updates": "2530", "lr": "7.86316e-05", "gnorm": "12.517", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "42589"}
2022-12-16 07:20:17 | INFO | train_inner | {"epoch": 19, "update": 18.765, "loss": "0.843", "nll_loss": "0.843", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.79", "wps": "1741.3", "ups": "0.13", "wpb": "13891.7", "bsz": "32", "num_updates": "2540", "lr": "7.85263e-05", "gnorm": "2.646", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "42668"}
2022-12-16 07:21:37 | INFO | train_inner | {"epoch": 19, "update": 18.838, "loss": "0.872", "nll_loss": "0.872", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.83", "wps": "1740.5", "ups": "0.13", "wpb": "13845.4", "bsz": "32", "num_updates": "2550", "lr": "7.84211e-05", "gnorm": "4.51", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.9", "wall": "42748"}
2022-12-16 07:22:57 | INFO | train_inner | {"epoch": 19, "update": 18.912, "loss": "0.859", "nll_loss": "0.859", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.81", "wps": "1730.7", "ups": "0.12", "wpb": "13855.7", "bsz": "32", "num_updates": "2560", "lr": "7.83158e-05", "gnorm": "3.117", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "42828"}
2022-12-16 07:24:20 | INFO | train_inner | {"epoch": 19, "update": 18.985, "loss": "0.862", "nll_loss": "0.862", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.82", "wps": "1694.5", "ups": "0.12", "wpb": "14129", "bsz": "32", "num_updates": "2570", "lr": "7.82105e-05", "gnorm": "3.083", "clip": "100", "loss_scale": "0.0625", "train_wall": "83", "gb_free": "62.2", "wall": "42911"}
2022-12-16 07:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 07:25:26 | INFO | absl | Using default tokenizer.
2022-12-16 07:25:57 | INFO | absl | Using default tokenizer.
2022-12-16 07:26:32 | INFO | absl | Using default tokenizer.
2022-12-16 07:27:06 | INFO | absl | Using default tokenizer.
2022-12-16 07:27:37 | INFO | absl | Using default tokenizer.
2022-12-16 07:28:08 | INFO | absl | Using default tokenizer.
2022-12-16 07:28:38 | INFO | absl | Using default tokenizer.
2022-12-16 07:29:08 | INFO | absl | Using default tokenizer.
2022-12-16 07:29:52 | INFO | absl | Using default tokenizer.
2022-12-16 07:30:24 | INFO | absl | Using default tokenizer.
2022-12-16 07:30:54 | INFO | absl | Using default tokenizer.
2022-12-16 07:31:25 | INFO | absl | Using default tokenizer.
2022-12-16 07:31:57 | INFO | absl | Using default tokenizer.
2022-12-16 07:32:40 | INFO | absl | Using default tokenizer.
2022-12-16 07:33:11 | INFO | absl | Using default tokenizer.
2022-12-16 07:33:41 | INFO | absl | Using default tokenizer.
2022-12-16 07:34:25 | INFO | absl | Using default tokenizer.
2022-12-16 07:34:59 | INFO | absl | Using default tokenizer.
2022-12-16 07:35:43 | INFO | absl | Using default tokenizer.
2022-12-16 07:36:18 | INFO | absl | Using default tokenizer.
2022-12-16 07:36:52 | INFO | absl | Using default tokenizer.
2022-12-16 07:37:31 | INFO | absl | Using default tokenizer.
2022-12-16 07:38:08 | INFO | absl | Using default tokenizer.
2022-12-16 07:38:44 | INFO | absl | Using default tokenizer.
2022-12-16 07:39:17 | INFO | absl | Using default tokenizer.
2022-12-16 07:39:50 | INFO | absl | Using default tokenizer.
2022-12-16 07:40:23 | INFO | absl | Using default tokenizer.
2022-12-16 07:41:00 | INFO | absl | Using default tokenizer.
2022-12-16 07:41:40 | INFO | absl | Using default tokenizer.
2022-12-16 07:42:13 | INFO | absl | Using default tokenizer.
2022-12-16 07:42:51 | INFO | absl | Using default tokenizer.
2022-12-16 07:43:30 | INFO | valid | {"epoch": 19, "valid_loss": "4.448", "valid_nll_loss": "4.448", "valid_rouge1": 0.5002453833007868, "valid_rouge2": 0.14383072150129603, "valid_rougel": 0.22564641875332983, "valid_rouge_avg": 0.32203805240104144, "valid_ppl": "21.82", "valid_wps": "98.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2572", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 07:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2572 updates
2022-12-16 07:43:30 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 07:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 07:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 19 @ 2572 updates, score 0.32203805240104144) (writing took 19.93868362205103 seconds)
2022-12-16 07:43:50 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-12-16 07:43:50 | INFO | train | {"epoch": 19, "train_loss": "0.821", "train_nll_loss": "0.821", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.77", "train_wps": "829.1", "train_ups": "0.06", "train_wpb": "13882.5", "train_bsz": "31.8", "train_num_updates": "2572", "train_lr": "7.81895e-05", "train_gnorm": "5.064", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1086", "train_gb_free": "62.9", "train_wall": "44081"}
2022-12-16 07:43:50 | INFO | fairseq.trainer | begin training epoch 20
2022-12-16 07:43:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 07:45:31 | INFO | train_inner | {"epoch": 20, "update": 19.059, "loss": "0.712", "nll_loss": "0.712", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.64", "wps": "101.2", "ups": "0.01", "wpb": "12863.2", "bsz": "29.6", "num_updates": "2580", "lr": "7.81053e-05", "gnorm": "3.234", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "44182"}
2022-12-16 07:46:51 | INFO | train_inner | {"epoch": 20, "update": 19.132, "loss": "0.716", "nll_loss": "0.716", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.64", "wps": "1738", "ups": "0.12", "wpb": "13926.3", "bsz": "32", "num_updates": "2590", "lr": "7.8e-05", "gnorm": "2.338", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "44262"}
2022-12-16 07:48:12 | INFO | train_inner | {"epoch": 20, "update": 19.206, "loss": "0.692", "nll_loss": "0.692", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.62", "wps": "1725.8", "ups": "0.12", "wpb": "13905.1", "bsz": "32", "num_updates": "2600", "lr": "7.78947e-05", "gnorm": "3.407", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63.6", "wall": "44343"}
2022-12-16 07:49:32 | INFO | train_inner | {"epoch": 20, "update": 19.279, "loss": "0.699", "nll_loss": "0.699", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.62", "wps": "1733.9", "ups": "0.12", "wpb": "13966.1", "bsz": "32", "num_updates": "2610", "lr": "7.77895e-05", "gnorm": "4.781", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "44424"}
2022-12-16 07:50:52 | INFO | train_inner | {"epoch": 20, "update": 19.353, "loss": "0.745", "nll_loss": "0.745", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.68", "wps": "1732.7", "ups": "0.13", "wpb": "13844.4", "bsz": "32", "num_updates": "2620", "lr": "7.76842e-05", "gnorm": "8.868", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63", "wall": "44503"}
2022-12-16 07:52:12 | INFO | train_inner | {"epoch": 20, "update": 19.426, "loss": "0.745", "nll_loss": "0.745", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.68", "wps": "1766.3", "ups": "0.13", "wpb": "14046.2", "bsz": "32", "num_updates": "2630", "lr": "7.75789e-05", "gnorm": "14.918", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "44583"}
2022-12-16 07:53:32 | INFO | train_inner | {"epoch": 20, "update": 19.5, "loss": "0.719", "nll_loss": "0.719", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.65", "wps": "1720.3", "ups": "0.12", "wpb": "13804.2", "bsz": "32", "num_updates": "2640", "lr": "7.74737e-05", "gnorm": "4.015", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "44663"}
2022-12-16 07:54:54 | INFO | train_inner | {"epoch": 20, "update": 19.574, "loss": "0.72", "nll_loss": "0.72", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.65", "wps": "1727.1", "ups": "0.12", "wpb": "14044.7", "bsz": "32", "num_updates": "2650", "lr": "7.73684e-05", "gnorm": "3.959", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "44745"}
2022-12-16 07:56:13 | INFO | train_inner | {"epoch": 20, "update": 19.647, "loss": "0.761", "nll_loss": "0.761", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.7", "wps": "1780.5", "ups": "0.13", "wpb": "14202", "bsz": "32", "num_updates": "2660", "lr": "7.72632e-05", "gnorm": "3.1", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "44824"}
2022-12-16 07:57:34 | INFO | train_inner | {"epoch": 20, "update": 19.721, "loss": "0.753", "nll_loss": "0.753", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.69", "wps": "1746.6", "ups": "0.12", "wpb": "14114.8", "bsz": "32", "num_updates": "2670", "lr": "7.71579e-05", "gnorm": "7.203", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63.6", "wall": "44905"}
2022-12-16 07:58:55 | INFO | train_inner | {"epoch": 20, "update": 19.794, "loss": "0.758", "nll_loss": "0.758", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.69", "wps": "1734.4", "ups": "0.12", "wpb": "13997.5", "bsz": "32", "num_updates": "2680", "lr": "7.70526e-05", "gnorm": "3.755", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "44986"}
2022-12-16 08:00:15 | INFO | train_inner | {"epoch": 20, "update": 19.868, "loss": "0.75", "nll_loss": "0.75", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.68", "wps": "1758.2", "ups": "0.13", "wpb": "14020.6", "bsz": "32", "num_updates": "2690", "lr": "7.69474e-05", "gnorm": "6.91", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "45066"}
2022-12-16 08:01:37 | INFO | train_inner | {"epoch": 20, "update": 19.941, "loss": "0.738", "nll_loss": "0.738", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.67", "wps": "1699.4", "ups": "0.12", "wpb": "13917.4", "bsz": "32", "num_updates": "2700", "lr": "7.68421e-05", "gnorm": "2.134", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "45148"}
2022-12-16 08:02:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-12-16 08:03:25 | INFO | absl | Using default tokenizer.
2022-12-16 08:03:56 | INFO | absl | Using default tokenizer.
2022-12-16 08:04:27 | INFO | absl | Using default tokenizer.
2022-12-16 08:04:57 | INFO | absl | Using default tokenizer.
2022-12-16 08:05:26 | INFO | absl | Using default tokenizer.
2022-12-16 08:06:00 | INFO | absl | Using default tokenizer.
2022-12-16 08:06:28 | INFO | absl | Using default tokenizer.
2022-12-16 08:07:04 | INFO | absl | Using default tokenizer.
2022-12-16 08:07:38 | INFO | absl | Using default tokenizer.
2022-12-16 08:08:05 | INFO | absl | Using default tokenizer.
2022-12-16 08:08:37 | INFO | absl | Using default tokenizer.
2022-12-16 08:09:13 | INFO | absl | Using default tokenizer.
2022-12-16 08:09:47 | INFO | absl | Using default tokenizer.
2022-12-16 08:10:17 | INFO | absl | Using default tokenizer.
2022-12-16 08:10:47 | INFO | absl | Using default tokenizer.
2022-12-16 08:11:23 | INFO | absl | Using default tokenizer.
2022-12-16 08:11:53 | INFO | absl | Using default tokenizer.
2022-12-16 08:12:22 | INFO | absl | Using default tokenizer.
2022-12-16 08:13:00 | INFO | absl | Using default tokenizer.
2022-12-16 08:13:32 | INFO | absl | Using default tokenizer.
2022-12-16 08:14:05 | INFO | absl | Using default tokenizer.
2022-12-16 08:14:46 | INFO | absl | Using default tokenizer.
2022-12-16 08:15:17 | INFO | absl | Using default tokenizer.
2022-12-16 08:15:49 | INFO | absl | Using default tokenizer.
2022-12-16 08:16:31 | INFO | absl | Using default tokenizer.
2022-12-16 08:17:03 | INFO | absl | Using default tokenizer.
2022-12-16 08:17:48 | INFO | absl | Using default tokenizer.
2022-12-16 08:18:22 | INFO | absl | Using default tokenizer.
2022-12-16 08:19:01 | INFO | absl | Using default tokenizer.
2022-12-16 08:19:36 | INFO | absl | Using default tokenizer.
2022-12-16 08:20:15 | INFO | absl | Using default tokenizer.
2022-12-16 08:20:46 | INFO | valid | {"epoch": 20, "valid_loss": "4.627", "valid_nll_loss": "4.627", "valid_rouge1": 0.5080995436768786, "valid_rouge2": 0.14378570388654094, "valid_rougel": 0.22656220721813425, "valid_rouge_avg": 0.32594262378170985, "valid_ppl": "24.71", "valid_wps": "102.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2708", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 08:20:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2708 updates
2022-12-16 08:20:46 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 08:21:06 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 08:21:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 20 @ 2708 updates, score 0.32594262378170985) (writing took 19.897390116006136 seconds)
2022-12-16 08:21:06 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-12-16 08:21:06 | INFO | train | {"epoch": 20, "train_loss": "0.732", "train_nll_loss": "0.732", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.66", "train_wps": "843.9", "train_ups": "0.06", "train_wpb": "13876.2", "train_bsz": "31.8", "train_num_updates": "2708", "train_lr": "7.67579e-05", "train_gnorm": "5.18", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "46317"}
2022-12-16 08:21:06 | INFO | fairseq.trainer | begin training epoch 21
2022-12-16 08:21:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 08:21:57 | INFO | train_inner | {"epoch": 21, "update": 20.015, "loss": "0.738", "nll_loss": "0.738", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.67", "wps": "102.9", "ups": "0.01", "wpb": "12561.8", "bsz": "29.6", "num_updates": "2710", "lr": "7.67368e-05", "gnorm": "3.401", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "46369"}
2022-12-16 08:23:16 | INFO | train_inner | {"epoch": 21, "update": 20.088, "loss": "0.607", "nll_loss": "0.607", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.52", "wps": "1776.1", "ups": "0.13", "wpb": "13986.2", "bsz": "32", "num_updates": "2720", "lr": "7.66316e-05", "gnorm": "2.384", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "46447"}
2022-12-16 08:24:36 | INFO | train_inner | {"epoch": 21, "update": 20.162, "loss": "0.648", "nll_loss": "0.648", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.57", "wps": "1718.5", "ups": "0.12", "wpb": "13790.8", "bsz": "32", "num_updates": "2730", "lr": "7.65263e-05", "gnorm": "4.592", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "46528"}
2022-12-16 08:25:57 | INFO | train_inner | {"epoch": 21, "update": 20.235, "loss": "0.647", "nll_loss": "0.647", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.57", "wps": "1753.1", "ups": "0.12", "wpb": "14104.5", "bsz": "32", "num_updates": "2740", "lr": "7.64211e-05", "gnorm": "4.915", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "46608"}
2022-12-16 08:27:18 | INFO | train_inner | {"epoch": 21, "update": 20.309, "loss": "0.644", "nll_loss": "0.644", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.56", "wps": "1710.4", "ups": "0.12", "wpb": "13935.8", "bsz": "32", "num_updates": "2750", "lr": "7.63158e-05", "gnorm": "2.458", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63.6", "wall": "46690"}
2022-12-16 08:28:39 | INFO | train_inner | {"epoch": 21, "update": 20.382, "loss": "0.645", "nll_loss": "0.645", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.56", "wps": "1681", "ups": "0.12", "wpb": "13549.3", "bsz": "32", "num_updates": "2760", "lr": "7.62105e-05", "gnorm": "3.098", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "46770"}
2022-12-16 08:30:00 | INFO | train_inner | {"epoch": 21, "update": 20.456, "loss": "0.659", "nll_loss": "0.659", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.58", "wps": "1730.2", "ups": "0.12", "wpb": "13996.5", "bsz": "32", "num_updates": "2770", "lr": "7.61053e-05", "gnorm": "3.64", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "46851"}
2022-12-16 08:31:22 | INFO | train_inner | {"epoch": 21, "update": 20.529, "loss": "0.674", "nll_loss": "0.674", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.6", "wps": "1704.6", "ups": "0.12", "wpb": "13951.9", "bsz": "32", "num_updates": "2780", "lr": "7.6e-05", "gnorm": "5.817", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "46933"}
2022-12-16 08:32:43 | INFO | train_inner | {"epoch": 21, "update": 20.603, "loss": "0.649", "nll_loss": "0.649", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.57", "wps": "1709.8", "ups": "0.12", "wpb": "13943.9", "bsz": "32", "num_updates": "2790", "lr": "7.58947e-05", "gnorm": "3.453", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "47014"}
2022-12-16 08:34:02 | INFO | train_inner | {"epoch": 21, "update": 20.676, "loss": "0.666", "nll_loss": "0.666", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.59", "wps": "1786.8", "ups": "0.13", "wpb": "14103.5", "bsz": "32", "num_updates": "2800", "lr": "7.57895e-05", "gnorm": "5.722", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "47093"}
2022-12-16 08:35:22 | INFO | train_inner | {"epoch": 21, "update": 20.75, "loss": "0.675", "nll_loss": "0.675", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.6", "wps": "1778.5", "ups": "0.13", "wpb": "14144.9", "bsz": "32", "num_updates": "2810", "lr": "7.56842e-05", "gnorm": "3.397", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "47173"}
2022-12-16 08:36:43 | INFO | train_inner | {"epoch": 21, "update": 20.824, "loss": "0.668", "nll_loss": "0.668", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.59", "wps": "1720.7", "ups": "0.12", "wpb": "13917.2", "bsz": "32", "num_updates": "2820", "lr": "7.55789e-05", "gnorm": "4.506", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "65", "wall": "47254"}
2022-12-16 08:38:04 | INFO | train_inner | {"epoch": 21, "update": 20.897, "loss": "0.721", "nll_loss": "0.721", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.65", "wps": "1727.1", "ups": "0.12", "wpb": "14072.3", "bsz": "32", "num_updates": "2830", "lr": "7.54737e-05", "gnorm": "2.856", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "47335"}
2022-12-16 08:39:25 | INFO | train_inner | {"epoch": 21, "update": 20.971, "loss": "0.675", "nll_loss": "0.675", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.6", "wps": "1744.8", "ups": "0.12", "wpb": "13992.1", "bsz": "32", "num_updates": "2840", "lr": "7.53684e-05", "gnorm": "2.049", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "47416"}
2022-12-16 08:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 08:40:43 | INFO | absl | Using default tokenizer.
2022-12-16 08:41:10 | INFO | absl | Using default tokenizer.
2022-12-16 08:41:42 | INFO | absl | Using default tokenizer.
2022-12-16 08:42:13 | INFO | absl | Using default tokenizer.
2022-12-16 08:42:42 | INFO | absl | Using default tokenizer.
2022-12-16 08:43:08 | INFO | absl | Using default tokenizer.
2022-12-16 08:43:36 | INFO | absl | Using default tokenizer.
2022-12-16 08:44:04 | INFO | absl | Using default tokenizer.
2022-12-16 08:44:35 | INFO | absl | Using default tokenizer.
2022-12-16 08:45:05 | INFO | absl | Using default tokenizer.
2022-12-16 08:45:36 | INFO | absl | Using default tokenizer.
2022-12-16 08:46:07 | INFO | absl | Using default tokenizer.
2022-12-16 08:46:42 | INFO | absl | Using default tokenizer.
2022-12-16 08:47:16 | INFO | absl | Using default tokenizer.
2022-12-16 08:47:48 | INFO | absl | Using default tokenizer.
2022-12-16 08:48:20 | INFO | absl | Using default tokenizer.
2022-12-16 08:48:55 | INFO | absl | Using default tokenizer.
2022-12-16 08:49:24 | INFO | absl | Using default tokenizer.
2022-12-16 08:50:01 | INFO | absl | Using default tokenizer.
2022-12-16 08:50:30 | INFO | absl | Using default tokenizer.
2022-12-16 08:51:03 | INFO | absl | Using default tokenizer.
2022-12-16 08:51:38 | INFO | absl | Using default tokenizer.
2022-12-16 08:52:07 | INFO | absl | Using default tokenizer.
2022-12-16 08:52:39 | INFO | absl | Using default tokenizer.
2022-12-16 08:53:09 | INFO | absl | Using default tokenizer.
2022-12-16 08:53:39 | INFO | absl | Using default tokenizer.
2022-12-16 08:54:18 | INFO | absl | Using default tokenizer.
2022-12-16 08:54:52 | INFO | absl | Using default tokenizer.
2022-12-16 08:55:33 | INFO | absl | Using default tokenizer.
2022-12-16 08:56:07 | INFO | absl | Using default tokenizer.
2022-12-16 08:56:46 | INFO | absl | Using default tokenizer.
2022-12-16 08:57:14 | INFO | valid | {"epoch": 21, "valid_loss": "4.724", "valid_nll_loss": "4.724", "valid_rouge1": 0.5017711848065466, "valid_rouge2": 0.14395021331622893, "valid_rougel": 0.2250010434984457, "valid_rouge_avg": 0.3228606990613877, "valid_ppl": "26.44", "valid_wps": "107.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2844", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 08:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2844 updates
2022-12-16 08:57:14 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 08:57:33 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 08:57:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 21 @ 2844 updates, score 0.3228606990613877) (writing took 19.42918894602917 seconds)
2022-12-16 08:57:33 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-12-16 08:57:33 | INFO | train | {"epoch": 21, "train_loss": "0.66", "train_nll_loss": "0.66", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.58", "train_wps": "862.8", "train_ups": "0.06", "train_wpb": "13874.9", "train_bsz": "31.8", "train_num_updates": "2844", "train_lr": "7.53263e-05", "train_gnorm": "3.698", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "63.6", "train_wall": "48504"}
2022-12-16 08:57:33 | INFO | fairseq.trainer | begin training epoch 22
2022-12-16 08:57:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 08:58:59 | INFO | train_inner | {"epoch": 22, "update": 21.044, "loss": "0.627", "nll_loss": "0.627", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.54", "wps": "110.3", "ups": "0.01", "wpb": "12945.8", "bsz": "29.6", "num_updates": "2850", "lr": "7.52632e-05", "gnorm": "4.496", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "48590"}
2022-12-16 09:00:19 | INFO | train_inner | {"epoch": 22, "update": 21.118, "loss": "0.553", "nll_loss": "0.553", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.47", "wps": "1734.6", "ups": "0.12", "wpb": "13931.7", "bsz": "32", "num_updates": "2860", "lr": "7.51579e-05", "gnorm": "3.255", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "48670"}
2022-12-16 09:01:39 | INFO | train_inner | {"epoch": 22, "update": 21.191, "loss": "0.56", "nll_loss": "0.56", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.47", "wps": "1763.9", "ups": "0.13", "wpb": "14075.8", "bsz": "32", "num_updates": "2870", "lr": "7.50526e-05", "gnorm": "1.872", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "48750"}
2022-12-16 09:03:00 | INFO | train_inner | {"epoch": 22, "update": 21.265, "loss": "0.575", "nll_loss": "0.575", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.49", "wps": "1747.3", "ups": "0.12", "wpb": "14138.2", "bsz": "32", "num_updates": "2880", "lr": "7.49474e-05", "gnorm": "2.83", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "48831"}
2022-12-16 09:04:20 | INFO | train_inner | {"epoch": 22, "update": 21.338, "loss": "0.573", "nll_loss": "0.573", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.49", "wps": "1740.5", "ups": "0.13", "wpb": "13906.7", "bsz": "32", "num_updates": "2890", "lr": "7.48421e-05", "gnorm": "2.604", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "48911"}
2022-12-16 09:05:39 | INFO | train_inner | {"epoch": 22, "update": 21.412, "loss": "0.584", "nll_loss": "0.584", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.5", "wps": "1758.3", "ups": "0.13", "wpb": "13929.8", "bsz": "32", "num_updates": "2900", "lr": "7.47368e-05", "gnorm": "3.236", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "48990"}
2022-12-16 09:07:00 | INFO | train_inner | {"epoch": 22, "update": 21.485, "loss": "0.607", "nll_loss": "0.607", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.52", "wps": "1735.2", "ups": "0.12", "wpb": "14152.4", "bsz": "32", "num_updates": "2910", "lr": "7.46316e-05", "gnorm": "6.207", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "49072"}
2022-12-16 09:08:20 | INFO | train_inner | {"epoch": 22, "update": 21.559, "loss": "0.589", "nll_loss": "0.589", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.5", "wps": "1754.1", "ups": "0.13", "wpb": "14018.9", "bsz": "32", "num_updates": "2920", "lr": "7.45263e-05", "gnorm": "2.815", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63", "wall": "49151"}
2022-12-16 09:09:41 | INFO | train_inner | {"epoch": 22, "update": 21.632, "loss": "0.566", "nll_loss": "0.566", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.48", "wps": "1727.7", "ups": "0.12", "wpb": "13960.8", "bsz": "32", "num_updates": "2930", "lr": "7.44211e-05", "gnorm": "2.508", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63.6", "wall": "49232"}
2022-12-16 09:11:00 | INFO | train_inner | {"epoch": 22, "update": 21.706, "loss": "0.585", "nll_loss": "0.585", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.5", "wps": "1767.8", "ups": "0.13", "wpb": "13923.5", "bsz": "32", "num_updates": "2940", "lr": "7.43158e-05", "gnorm": "3.024", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "49311"}
2022-12-16 09:12:21 | INFO | train_inner | {"epoch": 22, "update": 21.779, "loss": "0.593", "nll_loss": "0.593", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.51", "wps": "1725.4", "ups": "0.12", "wpb": "13901.4", "bsz": "32", "num_updates": "2950", "lr": "7.42105e-05", "gnorm": "1.818", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "49392"}
2022-12-16 09:13:42 | INFO | train_inner | {"epoch": 22, "update": 21.853, "loss": "0.581", "nll_loss": "0.581", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.5", "wps": "1717.5", "ups": "0.12", "wpb": "13930", "bsz": "32", "num_updates": "2960", "lr": "7.41053e-05", "gnorm": "1.691", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "49473"}
2022-12-16 09:14:58 | INFO | train_inner | {"epoch": 22, "update": 21.926, "loss": "0.595", "nll_loss": "0.595", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.51", "wps": "1815.5", "ups": "0.13", "wpb": "13914.6", "bsz": "32", "num_updates": "2970", "lr": "7.4e-05", "gnorm": "1.909", "clip": "100", "loss_scale": "0.125", "train_wall": "76", "gb_free": "62.2", "wall": "49550"}
2022-12-16 09:16:19 | INFO | train_inner | {"epoch": 22, "update": 22.0, "loss": "0.603", "nll_loss": "0.603", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.52", "wps": "1560.3", "ups": "0.12", "wpb": "12555.2", "bsz": "29.6", "num_updates": "2980", "lr": "7.38947e-05", "gnorm": "2.784", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "49630"}
2022-12-16 09:16:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 09:17:03 | INFO | absl | Using default tokenizer.
2022-12-16 09:17:34 | INFO | absl | Using default tokenizer.
2022-12-16 09:18:00 | INFO | absl | Using default tokenizer.
2022-12-16 09:18:31 | INFO | absl | Using default tokenizer.
2022-12-16 09:18:59 | INFO | absl | Using default tokenizer.
2022-12-16 09:19:28 | INFO | absl | Using default tokenizer.
2022-12-16 09:20:01 | INFO | absl | Using default tokenizer.
2022-12-16 09:20:33 | INFO | absl | Using default tokenizer.
2022-12-16 09:21:07 | INFO | absl | Using default tokenizer.
2022-12-16 09:21:37 | INFO | absl | Using default tokenizer.
2022-12-16 09:22:13 | INFO | absl | Using default tokenizer.
2022-12-16 09:22:47 | INFO | absl | Using default tokenizer.
2022-12-16 09:23:18 | INFO | absl | Using default tokenizer.
2022-12-16 09:24:01 | INFO | absl | Using default tokenizer.
2022-12-16 09:24:36 | INFO | absl | Using default tokenizer.
2022-12-16 09:25:10 | INFO | absl | Using default tokenizer.
2022-12-16 09:25:47 | INFO | absl | Using default tokenizer.
2022-12-16 09:26:20 | INFO | absl | Using default tokenizer.
2022-12-16 09:26:54 | INFO | absl | Using default tokenizer.
2022-12-16 09:27:29 | INFO | absl | Using default tokenizer.
2022-12-16 09:28:00 | INFO | absl | Using default tokenizer.
2022-12-16 09:28:39 | INFO | absl | Using default tokenizer.
2022-12-16 09:29:07 | INFO | absl | Using default tokenizer.
2022-12-16 09:29:42 | INFO | absl | Using default tokenizer.
2022-12-16 09:30:15 | INFO | absl | Using default tokenizer.
2022-12-16 09:30:48 | INFO | absl | Using default tokenizer.
2022-12-16 09:31:32 | INFO | absl | Using default tokenizer.
2022-12-16 09:32:10 | INFO | absl | Using default tokenizer.
2022-12-16 09:32:52 | INFO | absl | Using default tokenizer.
2022-12-16 09:33:27 | INFO | absl | Using default tokenizer.
2022-12-16 09:34:02 | INFO | absl | Using default tokenizer.
2022-12-16 09:34:31 | INFO | valid | {"epoch": 22, "valid_loss": "4.848", "valid_nll_loss": "4.848", "valid_rouge1": 0.5093717762319073, "valid_rouge2": 0.14385487128096394, "valid_rougel": 0.22447428721654605, "valid_rouge_avg": 0.32661332375643565, "valid_ppl": "28.81", "valid_wps": "101.9", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "2980", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 09:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2980 updates
2022-12-16 09:34:31 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 09:34:49 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 09:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 22 @ 2980 updates, score 0.32661332375643565) (writing took 17.73159749386832 seconds)
2022-12-16 09:34:49 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-12-16 09:34:49 | INFO | train | {"epoch": 22, "train_loss": "0.582", "train_nll_loss": "0.582", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.5", "train_wps": "844.3", "train_ups": "0.06", "train_wpb": "13881.8", "train_bsz": "31.8", "train_num_updates": "2980", "train_lr": "7.38947e-05", "train_gnorm": "2.938", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1083", "train_gb_free": "62.2", "train_wall": "50740"}
2022-12-16 09:34:49 | INFO | fairseq.trainer | begin training epoch 23
2022-12-16 09:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 09:36:48 | INFO | train_inner | {"epoch": 23, "update": 22.074, "loss": "0.465", "nll_loss": "0.465", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.38", "wps": "113.8", "ups": "0.01", "wpb": "13986", "bsz": "32", "num_updates": "2990", "lr": "7.37895e-05", "gnorm": "1.422", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "50859"}
2022-12-16 09:38:08 | INFO | train_inner | {"epoch": 23, "update": 22.147, "loss": "0.488", "nll_loss": "0.488", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.4", "wps": "1699.5", "ups": "0.12", "wpb": "13604", "bsz": "32", "num_updates": "3000", "lr": "7.36842e-05", "gnorm": "1.596", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "50939"}
2022-12-16 09:39:30 | INFO | train_inner | {"epoch": 23, "update": 22.221, "loss": "0.476", "nll_loss": "0.476", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.39", "wps": "1711.8", "ups": "0.12", "wpb": "14009.6", "bsz": "32", "num_updates": "3010", "lr": "7.35789e-05", "gnorm": "1.637", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "51021"}
2022-12-16 09:40:50 | INFO | train_inner | {"epoch": 23, "update": 22.294, "loss": "0.501", "nll_loss": "0.501", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.41", "wps": "1746.4", "ups": "0.13", "wpb": "13896.4", "bsz": "32", "num_updates": "3020", "lr": "7.34737e-05", "gnorm": "2.573", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "51101"}
2022-12-16 09:42:09 | INFO | train_inner | {"epoch": 23, "update": 22.368, "loss": "0.501", "nll_loss": "0.501", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.42", "wps": "1787.5", "ups": "0.13", "wpb": "14117.8", "bsz": "32", "num_updates": "3030", "lr": "7.33684e-05", "gnorm": "3.465", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.9", "wall": "51180"}
2022-12-16 09:43:28 | INFO | train_inner | {"epoch": 23, "update": 22.441, "loss": "0.506", "nll_loss": "0.506", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.42", "wps": "1783.7", "ups": "0.13", "wpb": "14052.9", "bsz": "32", "num_updates": "3040", "lr": "7.32632e-05", "gnorm": "1.923", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "51259"}
2022-12-16 09:44:48 | INFO | train_inner | {"epoch": 23, "update": 22.515, "loss": "0.51", "nll_loss": "0.51", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.42", "wps": "1748.4", "ups": "0.12", "wpb": "14077.8", "bsz": "32", "num_updates": "3050", "lr": "7.31579e-05", "gnorm": "8.465", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "51339"}
2022-12-16 09:46:09 | INFO | train_inner | {"epoch": 23, "update": 22.588, "loss": "0.515", "nll_loss": "0.515", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.43", "wps": "1723", "ups": "0.12", "wpb": "13876.3", "bsz": "32", "num_updates": "3060", "lr": "7.30526e-05", "gnorm": "2.376", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "51420"}
2022-12-16 09:47:27 | INFO | train_inner | {"epoch": 23, "update": 22.662, "loss": "0.498", "nll_loss": "0.498", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.41", "wps": "1813.9", "ups": "0.13", "wpb": "14250.6", "bsz": "32", "num_updates": "3070", "lr": "7.29474e-05", "gnorm": "1.978", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "51498"}
2022-12-16 09:48:48 | INFO | train_inner | {"epoch": 23, "update": 22.735, "loss": "0.524", "nll_loss": "0.524", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.44", "wps": "1719.1", "ups": "0.12", "wpb": "13850", "bsz": "32", "num_updates": "3080", "lr": "7.28421e-05", "gnorm": "5.214", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "51579"}
2022-12-16 09:50:08 | INFO | train_inner | {"epoch": 23, "update": 22.809, "loss": "0.518", "nll_loss": "0.518", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.43", "wps": "1722.9", "ups": "0.13", "wpb": "13773.3", "bsz": "32", "num_updates": "3090", "lr": "7.27368e-05", "gnorm": "6.688", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "51659"}
2022-12-16 09:51:28 | INFO | train_inner | {"epoch": 23, "update": 22.882, "loss": "0.521", "nll_loss": "0.521", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.44", "wps": "1773", "ups": "0.12", "wpb": "14186.6", "bsz": "32", "num_updates": "3100", "lr": "7.26316e-05", "gnorm": "1.984", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.9", "wall": "51739"}
2022-12-16 09:52:47 | INFO | train_inner | {"epoch": 23, "update": 22.956, "loss": "0.522", "nll_loss": "0.522", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.44", "wps": "1759.3", "ups": "0.13", "wpb": "13874.7", "bsz": "32", "num_updates": "3110", "lr": "7.25263e-05", "gnorm": "2.495", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "51818"}
2022-12-16 09:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 09:54:21 | INFO | absl | Using default tokenizer.
2022-12-16 09:54:47 | INFO | absl | Using default tokenizer.
2022-12-16 09:55:13 | INFO | absl | Using default tokenizer.
2022-12-16 09:55:42 | INFO | absl | Using default tokenizer.
2022-12-16 09:56:11 | INFO | absl | Using default tokenizer.
2022-12-16 09:56:48 | INFO | absl | Using default tokenizer.
2022-12-16 09:57:21 | INFO | absl | Using default tokenizer.
2022-12-16 09:57:54 | INFO | absl | Using default tokenizer.
2022-12-16 09:58:26 | INFO | absl | Using default tokenizer.
2022-12-16 09:58:59 | INFO | absl | Using default tokenizer.
2022-12-16 09:59:32 | INFO | absl | Using default tokenizer.
2022-12-16 10:00:06 | INFO | absl | Using default tokenizer.
2022-12-16 10:00:36 | INFO | absl | Using default tokenizer.
2022-12-16 10:01:04 | INFO | absl | Using default tokenizer.
2022-12-16 10:01:35 | INFO | absl | Using default tokenizer.
2022-12-16 10:02:08 | INFO | absl | Using default tokenizer.
2022-12-16 10:02:39 | INFO | absl | Using default tokenizer.
2022-12-16 10:03:09 | INFO | absl | Using default tokenizer.
2022-12-16 10:03:40 | INFO | absl | Using default tokenizer.
2022-12-16 10:04:09 | INFO | absl | Using default tokenizer.
2022-12-16 10:04:44 | INFO | absl | Using default tokenizer.
2022-12-16 10:05:33 | INFO | absl | Using default tokenizer.
2022-12-16 10:06:11 | INFO | absl | Using default tokenizer.
2022-12-16 10:06:42 | INFO | absl | Using default tokenizer.
2022-12-16 10:07:14 | INFO | absl | Using default tokenizer.
2022-12-16 10:07:44 | INFO | absl | Using default tokenizer.
2022-12-16 10:08:17 | INFO | absl | Using default tokenizer.
2022-12-16 10:08:51 | INFO | absl | Using default tokenizer.
2022-12-16 10:09:34 | INFO | absl | Using default tokenizer.
2022-12-16 10:10:14 | INFO | absl | Using default tokenizer.
2022-12-16 10:10:49 | INFO | absl | Using default tokenizer.
2022-12-16 10:11:16 | INFO | valid | {"epoch": 23, "valid_loss": "5", "valid_nll_loss": "5", "valid_rouge1": 0.5031324310230828, "valid_rouge2": 0.14337811339504394, "valid_rougel": 0.22492209004892244, "valid_rouge_avg": 0.3232552722090634, "valid_ppl": "31.99", "valid_wps": "104.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3116", "valid_best_rouge_avg": 0.3279280795895176}
2022-12-16 10:11:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3116 updates
2022-12-16 10:11:16 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 10:11:34 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 10:11:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 23 @ 3116 updates, score 0.3232552722090634) (writing took 18.07691148784943 seconds)
2022-12-16 10:11:34 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-12-16 10:11:34 | INFO | train | {"epoch": 23, "train_loss": "0.505", "train_nll_loss": "0.505", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.42", "train_wps": "856.2", "train_ups": "0.06", "train_wpb": "13881.8", "train_bsz": "31.8", "train_num_updates": "3116", "train_lr": "7.24632e-05", "train_gnorm": "3.432", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "52945"}
2022-12-16 10:11:34 | INFO | fairseq.trainer | begin training epoch 24
2022-12-16 10:11:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 10:12:43 | INFO | train_inner | {"epoch": 24, "update": 23.029, "loss": "0.497", "nll_loss": "0.497", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.41", "wps": "106.9", "ups": "0.01", "wpb": "12791.7", "bsz": "29.6", "num_updates": "3120", "lr": "7.24211e-05", "gnorm": "6.778", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "53014"}
2022-12-16 10:14:02 | INFO | train_inner | {"epoch": 24, "update": 23.103, "loss": "0.423", "nll_loss": "0.423", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.34", "wps": "1770.3", "ups": "0.13", "wpb": "14026.5", "bsz": "32", "num_updates": "3130", "lr": "7.23158e-05", "gnorm": "6.79", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "53093"}
2022-12-16 10:15:22 | INFO | train_inner | {"epoch": 24, "update": 23.176, "loss": "0.403", "nll_loss": "0.403", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "1775.5", "ups": "0.13", "wpb": "14112.6", "bsz": "32", "num_updates": "3140", "lr": "7.22105e-05", "gnorm": "5.437", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "53173"}
2022-12-16 10:16:40 | INFO | train_inner | {"epoch": 24, "update": 23.25, "loss": "0.428", "nll_loss": "0.428", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.35", "wps": "1793.9", "ups": "0.13", "wpb": "14049.4", "bsz": "32", "num_updates": "3150", "lr": "7.21053e-05", "gnorm": "4.03", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "53251"}
2022-12-16 10:18:01 | INFO | train_inner | {"epoch": 24, "update": 23.324, "loss": "0.455", "nll_loss": "0.455", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1741", "ups": "0.12", "wpb": "14045.5", "bsz": "32", "num_updates": "3160", "lr": "7.2e-05", "gnorm": "6.608", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "53332"}
2022-12-16 10:19:23 | INFO | train_inner | {"epoch": 24, "update": 23.397, "loss": "0.445", "nll_loss": "0.445", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.36", "wps": "1700.2", "ups": "0.12", "wpb": "13952.5", "bsz": "32", "num_updates": "3170", "lr": "7.18947e-05", "gnorm": "9.143", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "53414"}
2022-12-16 10:20:44 | INFO | train_inner | {"epoch": 24, "update": 23.471, "loss": "0.445", "nll_loss": "0.445", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.36", "wps": "1718.1", "ups": "0.12", "wpb": "13883.5", "bsz": "32", "num_updates": "3180", "lr": "7.17895e-05", "gnorm": "3.854", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "53495"}
2022-12-16 10:22:04 | INFO | train_inner | {"epoch": 24, "update": 23.544, "loss": "0.46", "nll_loss": "0.46", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.38", "wps": "1761.5", "ups": "0.12", "wpb": "14149.9", "bsz": "32", "num_updates": "3190", "lr": "7.16842e-05", "gnorm": "3.273", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "53575"}
2022-12-16 10:22:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2022-12-16 10:23:32 | INFO | train_inner | {"epoch": 24, "update": 23.625, "loss": "0.458", "nll_loss": "0.458", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1569.6", "ups": "0.11", "wpb": "13861.2", "bsz": "32", "num_updates": "3200", "lr": "7.15789e-05", "gnorm": "6.17", "clip": "100", "loss_scale": "0.0625", "train_wall": "88", "gb_free": "62.2", "wall": "53664"}
2022-12-16 10:24:54 | INFO | train_inner | {"epoch": 24, "update": 23.699, "loss": "0.479", "nll_loss": "0.479", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.39", "wps": "1707.9", "ups": "0.12", "wpb": "13923.2", "bsz": "32", "num_updates": "3210", "lr": "7.14737e-05", "gnorm": "5.494", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "53745"}
2022-12-16 10:26:15 | INFO | train_inner | {"epoch": 24, "update": 23.772, "loss": "0.498", "nll_loss": "0.498", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.41", "wps": "1756.4", "ups": "0.12", "wpb": "14182.2", "bsz": "32", "num_updates": "3220", "lr": "7.13684e-05", "gnorm": "12.659", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "53826"}
2022-12-16 10:27:34 | INFO | train_inner | {"epoch": 24, "update": 23.846, "loss": "0.48", "nll_loss": "0.48", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.39", "wps": "1739.4", "ups": "0.13", "wpb": "13744.7", "bsz": "32", "num_updates": "3230", "lr": "7.12632e-05", "gnorm": "9.295", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "53905"}
2022-12-16 10:28:54 | INFO | train_inner | {"epoch": 24, "update": 23.919, "loss": "0.447", "nll_loss": "0.447", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.36", "wps": "1730.4", "ups": "0.12", "wpb": "13887.2", "bsz": "32", "num_updates": "3240", "lr": "7.11579e-05", "gnorm": "4.161", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "53985"}
2022-12-16 10:30:15 | INFO | train_inner | {"epoch": 24, "update": 23.993, "loss": "0.502", "nll_loss": "0.502", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.42", "wps": "1681.6", "ups": "0.12", "wpb": "13674.2", "bsz": "32", "num_updates": "3250", "lr": "7.10526e-05", "gnorm": "3.095", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "54067"}
2022-12-16 10:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 10:31:07 | INFO | absl | Using default tokenizer.
2022-12-16 10:31:36 | INFO | absl | Using default tokenizer.
2022-12-16 10:32:06 | INFO | absl | Using default tokenizer.
2022-12-16 10:32:42 | INFO | absl | Using default tokenizer.
2022-12-16 10:33:13 | INFO | absl | Using default tokenizer.
2022-12-16 10:33:43 | INFO | absl | Using default tokenizer.
2022-12-16 10:34:15 | INFO | absl | Using default tokenizer.
2022-12-16 10:34:49 | INFO | absl | Using default tokenizer.
2022-12-16 10:35:32 | INFO | absl | Using default tokenizer.
2022-12-16 10:36:06 | INFO | absl | Using default tokenizer.
2022-12-16 10:36:37 | INFO | absl | Using default tokenizer.
2022-12-16 10:37:16 | INFO | absl | Using default tokenizer.
2022-12-16 10:37:52 | INFO | absl | Using default tokenizer.
2022-12-16 10:38:28 | INFO | absl | Using default tokenizer.
2022-12-16 10:39:03 | INFO | absl | Using default tokenizer.
2022-12-16 10:39:37 | INFO | absl | Using default tokenizer.
2022-12-16 10:40:08 | INFO | absl | Using default tokenizer.
2022-12-16 10:40:42 | INFO | absl | Using default tokenizer.
2022-12-16 10:41:14 | INFO | absl | Using default tokenizer.
2022-12-16 10:41:49 | INFO | absl | Using default tokenizer.
2022-12-16 10:42:33 | INFO | absl | Using default tokenizer.
2022-12-16 10:43:12 | INFO | absl | Using default tokenizer.
2022-12-16 10:43:50 | INFO | absl | Using default tokenizer.
2022-12-16 10:44:27 | INFO | absl | Using default tokenizer.
2022-12-16 10:45:11 | INFO | absl | Using default tokenizer.
2022-12-16 10:45:45 | INFO | absl | Using default tokenizer.
2022-12-16 10:46:19 | INFO | absl | Using default tokenizer.
2022-12-16 10:46:54 | INFO | absl | Using default tokenizer.
2022-12-16 10:47:34 | INFO | absl | Using default tokenizer.
2022-12-16 10:48:14 | INFO | absl | Using default tokenizer.
2022-12-16 10:48:53 | INFO | absl | Using default tokenizer.
2022-12-16 10:49:17 | INFO | valid | {"epoch": 24, "valid_loss": "5.088", "valid_nll_loss": "5.088", "valid_rouge1": 0.5091487532851836, "valid_rouge2": 0.1475289979746721, "valid_rougel": 0.22661902140281778, "valid_rouge_avg": 0.32833887562992775, "valid_ppl": "34", "valid_wps": "97.4", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3251", "valid_best_rouge_avg": 0.32833887562992775}
2022-12-16 10:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3251 updates
2022-12-16 10:49:17 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 10:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 10:50:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 24 @ 3251 updates, score 0.32833887562992775) (writing took 57.973418874898925 seconds)
2022-12-16 10:50:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-12-16 10:50:15 | INFO | train | {"epoch": 24, "train_loss": "0.455", "train_nll_loss": "0.455", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.37", "train_wps": "807.5", "train_ups": "0.06", "train_wpb": "13882.2", "train_bsz": "31.8", "train_num_updates": "3251", "train_lr": "7.10421e-05", "train_gnorm": "6.111", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1085", "train_gb_free": "62.9", "train_wall": "55266"}
2022-12-16 10:50:15 | INFO | fairseq.trainer | begin training epoch 25
2022-12-16 10:50:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 10:51:56 | INFO | train_inner | {"epoch": 25, "update": 24.066, "loss": "0.395", "nll_loss": "0.395", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "98.7", "ups": "0.01", "wpb": "12834.5", "bsz": "29.6", "num_updates": "3260", "lr": "7.09474e-05", "gnorm": "2.227", "clip": "100", "loss_scale": "0.0625", "train_wall": "77", "gb_free": "62.2", "wall": "55367"}
2022-12-16 10:53:17 | INFO | train_inner | {"epoch": 25, "update": 24.14, "loss": "0.41", "nll_loss": "0.41", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.33", "wps": "1698.6", "ups": "0.12", "wpb": "13729.3", "bsz": "32", "num_updates": "3270", "lr": "7.08421e-05", "gnorm": "2.085", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "55448"}
2022-12-16 10:54:35 | INFO | train_inner | {"epoch": 25, "update": 24.213, "loss": "0.41", "nll_loss": "0.41", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.33", "wps": "1808.6", "ups": "0.13", "wpb": "14113.2", "bsz": "32", "num_updates": "3280", "lr": "7.07368e-05", "gnorm": "1.995", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "55526"}
2022-12-16 10:55:55 | INFO | train_inner | {"epoch": 25, "update": 24.287, "loss": "0.424", "nll_loss": "0.424", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.34", "wps": "1715.2", "ups": "0.12", "wpb": "13729.8", "bsz": "32", "num_updates": "3290", "lr": "7.06316e-05", "gnorm": "3.389", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "55606"}
2022-12-16 10:57:14 | INFO | train_inner | {"epoch": 25, "update": 24.36, "loss": "0.42", "nll_loss": "0.42", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.34", "wps": "1766.9", "ups": "0.13", "wpb": "13989.2", "bsz": "32", "num_updates": "3300", "lr": "7.05263e-05", "gnorm": "5.374", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "63.6", "wall": "55685"}
2022-12-16 10:58:35 | INFO | train_inner | {"epoch": 25, "update": 24.434, "loss": "0.463", "nll_loss": "0.463", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.38", "wps": "1726.4", "ups": "0.12", "wpb": "13988.9", "bsz": "32", "num_updates": "3310", "lr": "7.04211e-05", "gnorm": "7.357", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "55766"}
2022-12-16 10:59:55 | INFO | train_inner | {"epoch": 25, "update": 24.507, "loss": "0.452", "nll_loss": "0.452", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1763.2", "ups": "0.13", "wpb": "14019.9", "bsz": "32", "num_updates": "3320", "lr": "7.03158e-05", "gnorm": "3.493", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "55846"}
2022-12-16 11:01:13 | INFO | train_inner | {"epoch": 25, "update": 24.581, "loss": "0.453", "nll_loss": "0.453", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1790.6", "ups": "0.13", "wpb": "13924.9", "bsz": "32", "num_updates": "3330", "lr": "7.02105e-05", "gnorm": "7.896", "clip": "100", "loss_scale": "0.0625", "train_wall": "77", "gb_free": "62.2", "wall": "55924"}
2022-12-16 11:02:34 | INFO | train_inner | {"epoch": 25, "update": 24.654, "loss": "0.436", "nll_loss": "0.436", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.35", "wps": "1688.4", "ups": "0.12", "wpb": "13721.6", "bsz": "32", "num_updates": "3340", "lr": "7.01053e-05", "gnorm": "7.832", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "56005"}
2022-12-16 11:03:54 | INFO | train_inner | {"epoch": 25, "update": 24.728, "loss": "0.453", "nll_loss": "0.453", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1771.5", "ups": "0.12", "wpb": "14250.7", "bsz": "32", "num_updates": "3350", "lr": "7e-05", "gnorm": "6.861", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "56085"}
2022-12-16 11:05:16 | INFO | train_inner | {"epoch": 25, "update": 24.801, "loss": "0.458", "nll_loss": "0.458", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1727.4", "ups": "0.12", "wpb": "14020.8", "bsz": "32", "num_updates": "3360", "lr": "6.98947e-05", "gnorm": "3.984", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "56167"}
2022-12-16 11:06:36 | INFO | train_inner | {"epoch": 25, "update": 24.875, "loss": "0.452", "nll_loss": "0.452", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.37", "wps": "1733.4", "ups": "0.12", "wpb": "13953.2", "bsz": "32", "num_updates": "3370", "lr": "6.97895e-05", "gnorm": "6.325", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "56247"}
2022-12-16 11:07:57 | INFO | train_inner | {"epoch": 25, "update": 24.949, "loss": "0.47", "nll_loss": "0.47", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.39", "wps": "1753.4", "ups": "0.12", "wpb": "14167.2", "bsz": "32", "num_updates": "3380", "lr": "6.96842e-05", "gnorm": "21.093", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "56328"}
2022-12-16 11:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:09:36 | INFO | absl | Using default tokenizer.
2022-12-16 11:10:04 | INFO | absl | Using default tokenizer.
2022-12-16 11:10:32 | INFO | absl | Using default tokenizer.
2022-12-16 11:11:01 | INFO | absl | Using default tokenizer.
2022-12-16 11:11:37 | INFO | absl | Using default tokenizer.
2022-12-16 11:12:06 | INFO | absl | Using default tokenizer.
2022-12-16 11:12:35 | INFO | absl | Using default tokenizer.
2022-12-16 11:13:03 | INFO | absl | Using default tokenizer.
2022-12-16 11:13:35 | INFO | absl | Using default tokenizer.
2022-12-16 11:14:08 | INFO | absl | Using default tokenizer.
2022-12-16 11:14:37 | INFO | absl | Using default tokenizer.
2022-12-16 11:15:14 | INFO | absl | Using default tokenizer.
2022-12-16 11:15:43 | INFO | absl | Using default tokenizer.
2022-12-16 11:16:15 | INFO | absl | Using default tokenizer.
2022-12-16 11:16:46 | INFO | absl | Using default tokenizer.
2022-12-16 11:17:19 | INFO | absl | Using default tokenizer.
2022-12-16 11:17:53 | INFO | absl | Using default tokenizer.
2022-12-16 11:18:24 | INFO | absl | Using default tokenizer.
2022-12-16 11:18:56 | INFO | absl | Using default tokenizer.
2022-12-16 11:19:29 | INFO | absl | Using default tokenizer.
2022-12-16 11:20:02 | INFO | absl | Using default tokenizer.
2022-12-16 11:20:41 | INFO | absl | Using default tokenizer.
2022-12-16 11:21:19 | INFO | absl | Using default tokenizer.
2022-12-16 11:21:50 | INFO | absl | Using default tokenizer.
2022-12-16 11:22:24 | INFO | absl | Using default tokenizer.
2022-12-16 11:23:01 | INFO | absl | Using default tokenizer.
2022-12-16 11:23:32 | INFO | absl | Using default tokenizer.
2022-12-16 11:24:14 | INFO | absl | Using default tokenizer.
2022-12-16 11:24:48 | INFO | absl | Using default tokenizer.
2022-12-16 11:25:22 | INFO | absl | Using default tokenizer.
2022-12-16 11:25:56 | INFO | absl | Using default tokenizer.
2022-12-16 11:26:23 | INFO | valid | {"epoch": 25, "valid_loss": "5.156", "valid_nll_loss": "5.156", "valid_rouge1": 0.509502046213467, "valid_rouge2": 0.14416825664612673, "valid_rougel": 0.22601009714919493, "valid_rouge_avg": 0.32683515142979674, "valid_ppl": "35.64", "valid_wps": "106", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3387", "valid_best_rouge_avg": 0.32833887562992775}
2022-12-16 11:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3387 updates
2022-12-16 11:26:23 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 11:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 11:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 25 @ 3387 updates, score 0.32683515142979674) (writing took 18.011143791023642 seconds)
2022-12-16 11:26:41 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-12-16 11:26:41 | INFO | train | {"epoch": 25, "train_loss": "0.439", "train_nll_loss": "0.439", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.36", "train_wps": "863.2", "train_ups": "0.06", "train_wpb": "13877", "train_bsz": "31.8", "train_num_updates": "3387", "train_lr": "6.96105e-05", "train_gnorm": "5.987", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1081", "train_gb_free": "62.2", "train_wall": "57452"}
2022-12-16 11:26:41 | INFO | fairseq.trainer | begin training epoch 26
2022-12-16 11:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 11:28:08 | INFO | train_inner | {"epoch": 26, "update": 25.022, "loss": "0.419", "nll_loss": "0.419", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.34", "wps": "106.2", "ups": "0.01", "wpb": "12862.2", "bsz": "29.6", "num_updates": "3390", "lr": "6.95789e-05", "gnorm": "2.471", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "57539"}
2022-12-16 11:29:29 | INFO | train_inner | {"epoch": 26, "update": 25.096, "loss": "0.35", "nll_loss": "0.35", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.27", "wps": "1728", "ups": "0.12", "wpb": "14050.5", "bsz": "32", "num_updates": "3400", "lr": "6.94737e-05", "gnorm": "1.672", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "57620"}
2022-12-16 11:30:48 | INFO | train_inner | {"epoch": 26, "update": 25.169, "loss": "0.409", "nll_loss": "0.409", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.33", "wps": "1763.9", "ups": "0.13", "wpb": "13969", "bsz": "32", "num_updates": "3410", "lr": "6.93684e-05", "gnorm": "15.296", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "57699"}
2022-12-16 11:32:08 | INFO | train_inner | {"epoch": 26, "update": 25.243, "loss": "0.379", "nll_loss": "0.379", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.3", "wps": "1759.9", "ups": "0.13", "wpb": "13992.3", "bsz": "32", "num_updates": "3420", "lr": "6.92632e-05", "gnorm": "2.87", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "57779"}
2022-12-16 11:33:28 | INFO | train_inner | {"epoch": 26, "update": 25.316, "loss": "0.391", "nll_loss": "0.391", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1708.2", "ups": "0.13", "wpb": "13640.8", "bsz": "32", "num_updates": "3430", "lr": "6.91579e-05", "gnorm": "2.346", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "63", "wall": "57859"}
2022-12-16 11:34:49 | INFO | train_inner | {"epoch": 26, "update": 25.39, "loss": "0.4", "nll_loss": "0.4", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "1742.8", "ups": "0.12", "wpb": "14057.3", "bsz": "32", "num_updates": "3440", "lr": "6.90526e-05", "gnorm": "2.773", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "57940"}
2022-12-16 11:36:09 | INFO | train_inner | {"epoch": 26, "update": 25.463, "loss": "0.395", "nll_loss": "0.395", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1735.9", "ups": "0.12", "wpb": "13917.5", "bsz": "32", "num_updates": "3450", "lr": "6.89474e-05", "gnorm": "4.087", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "58020"}
2022-12-16 11:37:30 | INFO | train_inner | {"epoch": 26, "update": 25.537, "loss": "0.4", "nll_loss": "0.4", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "1730.6", "ups": "0.12", "wpb": "14101", "bsz": "32", "num_updates": "3460", "lr": "6.88421e-05", "gnorm": "29.943", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "58101"}
2022-12-16 11:38:50 | INFO | train_inner | {"epoch": 26, "update": 25.61, "loss": "0.389", "nll_loss": "0.389", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1745.4", "ups": "0.12", "wpb": "13970.1", "bsz": "32", "num_updates": "3470", "lr": "6.87368e-05", "gnorm": "2.255", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "58181"}
2022-12-16 11:40:10 | INFO | train_inner | {"epoch": 26, "update": 25.684, "loss": "0.388", "nll_loss": "0.388", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1737.9", "ups": "0.12", "wpb": "13930.2", "bsz": "32", "num_updates": "3480", "lr": "6.86316e-05", "gnorm": "2.197", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "58262"}
2022-12-16 11:41:31 | INFO | train_inner | {"epoch": 26, "update": 25.757, "loss": "0.389", "nll_loss": "0.389", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1727.4", "ups": "0.12", "wpb": "13893.3", "bsz": "32", "num_updates": "3490", "lr": "6.85263e-05", "gnorm": "2.456", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "58342"}
2022-12-16 11:42:51 | INFO | train_inner | {"epoch": 26, "update": 25.831, "loss": "0.399", "nll_loss": "0.399", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "1726.3", "ups": "0.13", "wpb": "13784.8", "bsz": "32", "num_updates": "3500", "lr": "6.84211e-05", "gnorm": "3.985", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "58422"}
2022-12-16 11:44:11 | INFO | train_inner | {"epoch": 26, "update": 25.904, "loss": "0.392", "nll_loss": "0.392", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.31", "wps": "1750.2", "ups": "0.13", "wpb": "13960.9", "bsz": "32", "num_updates": "3510", "lr": "6.83158e-05", "gnorm": "1.4", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "58502"}
2022-12-16 11:45:32 | INFO | train_inner | {"epoch": 26, "update": 25.978, "loss": "0.402", "nll_loss": "0.402", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.32", "wps": "1728.3", "ups": "0.12", "wpb": "14015", "bsz": "32", "num_updates": "3520", "lr": "6.82105e-05", "gnorm": "1.475", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "58583"}
2022-12-16 11:45:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 11:46:42 | INFO | absl | Using default tokenizer.
2022-12-16 11:47:09 | INFO | absl | Using default tokenizer.
2022-12-16 11:47:41 | INFO | absl | Using default tokenizer.
2022-12-16 11:48:09 | INFO | absl | Using default tokenizer.
2022-12-16 11:48:39 | INFO | absl | Using default tokenizer.
2022-12-16 11:49:11 | INFO | absl | Using default tokenizer.
2022-12-16 11:49:44 | INFO | absl | Using default tokenizer.
2022-12-16 11:50:18 | INFO | absl | Using default tokenizer.
2022-12-16 11:50:50 | INFO | absl | Using default tokenizer.
2022-12-16 11:51:21 | INFO | absl | Using default tokenizer.
2022-12-16 11:51:58 | INFO | absl | Using default tokenizer.
2022-12-16 11:52:28 | INFO | absl | Using default tokenizer.
2022-12-16 11:53:01 | INFO | absl | Using default tokenizer.
2022-12-16 11:53:33 | INFO | absl | Using default tokenizer.
2022-12-16 11:54:05 | INFO | absl | Using default tokenizer.
2022-12-16 11:54:34 | INFO | absl | Using default tokenizer.
2022-12-16 11:55:05 | INFO | absl | Using default tokenizer.
2022-12-16 11:55:35 | INFO | absl | Using default tokenizer.
2022-12-16 11:56:08 | INFO | absl | Using default tokenizer.
2022-12-16 11:56:42 | INFO | absl | Using default tokenizer.
2022-12-16 11:57:16 | INFO | absl | Using default tokenizer.
2022-12-16 11:57:57 | INFO | absl | Using default tokenizer.
2022-12-16 11:58:31 | INFO | absl | Using default tokenizer.
2022-12-16 11:59:02 | INFO | absl | Using default tokenizer.
2022-12-16 11:59:36 | INFO | absl | Using default tokenizer.
2022-12-16 12:00:09 | INFO | absl | Using default tokenizer.
2022-12-16 12:00:46 | INFO | absl | Using default tokenizer.
2022-12-16 12:01:20 | INFO | absl | Using default tokenizer.
2022-12-16 12:01:53 | INFO | absl | Using default tokenizer.
2022-12-16 12:02:27 | INFO | absl | Using default tokenizer.
2022-12-16 12:03:03 | INFO | absl | Using default tokenizer.
2022-12-16 12:03:28 | INFO | valid | {"epoch": 26, "valid_loss": "5.24", "valid_nll_loss": "5.24", "valid_rouge1": 0.5082509675058715, "valid_rouge2": 0.1421064272079473, "valid_rougel": 0.22389334281763665, "valid_rouge_avg": 0.3251786973569095, "valid_ppl": "37.79", "valid_wps": "105.9", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3523", "valid_best_rouge_avg": 0.32833887562992775}
2022-12-16 12:03:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 3523 updates
2022-12-16 12:03:28 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 12:03:45 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 12:03:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 26 @ 3523 updates, score 0.3251786973569095) (writing took 17.086154389893636 seconds)
2022-12-16 12:03:45 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-12-16 12:03:45 | INFO | train | {"epoch": 26, "train_loss": "0.391", "train_nll_loss": "0.391", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.31", "train_wps": "848.8", "train_ups": "0.06", "train_wpb": "13876.2", "train_bsz": "31.8", "train_num_updates": "3523", "train_lr": "6.81789e-05", "train_gnorm": "5.431", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "59676"}
2022-12-16 12:03:45 | INFO | fairseq.trainer | begin training epoch 27
2022-12-16 12:03:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 12:05:20 | INFO | train_inner | {"epoch": 27, "update": 26.051, "loss": "0.35", "nll_loss": "0.35", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.27", "wps": "110.7", "ups": "0.01", "wpb": "13157", "bsz": "29.6", "num_updates": "3530", "lr": "6.81053e-05", "gnorm": "1.675", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "59772"}
2022-12-16 12:06:40 | INFO | train_inner | {"epoch": 27, "update": 26.125, "loss": "0.309", "nll_loss": "0.309", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.24", "wps": "1734.9", "ups": "0.13", "wpb": "13835.3", "bsz": "32", "num_updates": "3540", "lr": "6.8e-05", "gnorm": "1.498", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "59851"}
2022-12-16 12:08:01 | INFO | train_inner | {"epoch": 27, "update": 26.199, "loss": "0.323", "nll_loss": "0.323", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.25", "wps": "1691.7", "ups": "0.12", "wpb": "13580.9", "bsz": "32", "num_updates": "3550", "lr": "6.78947e-05", "gnorm": "1.393", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "59932"}
2022-12-16 12:09:21 | INFO | train_inner | {"epoch": 27, "update": 26.272, "loss": "0.318", "nll_loss": "0.318", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.25", "wps": "1762", "ups": "0.12", "wpb": "14111.2", "bsz": "32", "num_updates": "3560", "lr": "6.77895e-05", "gnorm": "3.744", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "60012"}
2022-12-16 12:10:40 | INFO | train_inner | {"epoch": 27, "update": 26.346, "loss": "0.333", "nll_loss": "0.333", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "1741.7", "ups": "0.13", "wpb": "13798.6", "bsz": "32", "num_updates": "3570", "lr": "6.76842e-05", "gnorm": "2.661", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "60091"}
2022-12-16 12:12:01 | INFO | train_inner | {"epoch": 27, "update": 26.419, "loss": "0.328", "nll_loss": "0.328", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "1716", "ups": "0.12", "wpb": "13890.3", "bsz": "32", "num_updates": "3580", "lr": "6.75789e-05", "gnorm": "1.536", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "60172"}
2022-12-16 12:13:20 | INFO | train_inner | {"epoch": 27, "update": 26.493, "loss": "0.358", "nll_loss": "0.358", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.28", "wps": "1730.9", "ups": "0.13", "wpb": "13745.3", "bsz": "32", "num_updates": "3590", "lr": "6.74737e-05", "gnorm": "3.827", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "60251"}
2022-12-16 12:14:41 | INFO | train_inner | {"epoch": 27, "update": 26.566, "loss": "0.338", "nll_loss": "0.338", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "1703.2", "ups": "0.12", "wpb": "13829.9", "bsz": "32", "num_updates": "3600", "lr": "6.73684e-05", "gnorm": "2.773", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "60333"}
2022-12-16 12:16:00 | INFO | train_inner | {"epoch": 27, "update": 26.64, "loss": "0.327", "nll_loss": "0.327", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.25", "wps": "1790.6", "ups": "0.13", "wpb": "14119", "bsz": "32", "num_updates": "3610", "lr": "6.72632e-05", "gnorm": "3.145", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "60411"}
2022-12-16 12:17:18 | INFO | train_inner | {"epoch": 27, "update": 26.713, "loss": "0.319", "nll_loss": "0.319", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.25", "wps": "1794.7", "ups": "0.13", "wpb": "14014.4", "bsz": "32", "num_updates": "3620", "lr": "6.71579e-05", "gnorm": "4.056", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "60490"}
2022-12-16 12:18:37 | INFO | train_inner | {"epoch": 27, "update": 26.787, "loss": "0.33", "nll_loss": "0.33", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "1778.7", "ups": "0.13", "wpb": "14013.3", "bsz": "32", "num_updates": "3630", "lr": "6.70526e-05", "gnorm": "1.506", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "60568"}
2022-12-16 12:19:58 | INFO | train_inner | {"epoch": 27, "update": 26.86, "loss": "0.332", "nll_loss": "0.332", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "1753.3", "ups": "0.12", "wpb": "14132.7", "bsz": "32", "num_updates": "3640", "lr": "6.69474e-05", "gnorm": "1.87", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "60649"}
2022-12-16 12:21:18 | INFO | train_inner | {"epoch": 27, "update": 26.934, "loss": "0.343", "nll_loss": "0.343", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.27", "wps": "1763.5", "ups": "0.12", "wpb": "14193.9", "bsz": "32", "num_updates": "3650", "lr": "6.68421e-05", "gnorm": "2.249", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "60729"}
2022-12-16 12:22:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 12:23:15 | INFO | absl | Using default tokenizer.
2022-12-16 12:23:43 | INFO | absl | Using default tokenizer.
2022-12-16 12:24:10 | INFO | absl | Using default tokenizer.
2022-12-16 12:24:43 | INFO | absl | Using default tokenizer.
2022-12-16 12:25:12 | INFO | absl | Using default tokenizer.
2022-12-16 12:25:41 | INFO | absl | Using default tokenizer.
2022-12-16 12:26:08 | INFO | absl | Using default tokenizer.
2022-12-16 12:26:39 | INFO | absl | Using default tokenizer.
2022-12-16 12:27:12 | INFO | absl | Using default tokenizer.
2022-12-16 12:27:45 | INFO | absl | Using default tokenizer.
2022-12-16 12:28:18 | INFO | absl | Using default tokenizer.
2022-12-16 12:28:48 | INFO | absl | Using default tokenizer.
2022-12-16 12:29:17 | INFO | absl | Using default tokenizer.
2022-12-16 12:29:56 | INFO | absl | Using default tokenizer.
2022-12-16 12:30:28 | INFO | absl | Using default tokenizer.
2022-12-16 12:31:02 | INFO | absl | Using default tokenizer.
2022-12-16 12:31:33 | INFO | absl | Using default tokenizer.
2022-12-16 12:32:05 | INFO | absl | Using default tokenizer.
2022-12-16 12:32:40 | INFO | absl | Using default tokenizer.
2022-12-16 12:33:09 | INFO | absl | Using default tokenizer.
2022-12-16 12:33:42 | INFO | absl | Using default tokenizer.
2022-12-16 12:34:17 | INFO | absl | Using default tokenizer.
2022-12-16 12:34:50 | INFO | absl | Using default tokenizer.
2022-12-16 12:35:28 | INFO | absl | Using default tokenizer.
2022-12-16 12:36:01 | INFO | absl | Using default tokenizer.
2022-12-16 12:36:34 | INFO | absl | Using default tokenizer.
2022-12-16 12:37:08 | INFO | absl | Using default tokenizer.
2022-12-16 12:37:39 | INFO | absl | Using default tokenizer.
2022-12-16 12:38:15 | INFO | absl | Using default tokenizer.
2022-12-16 12:38:51 | INFO | absl | Using default tokenizer.
2022-12-16 12:39:27 | INFO | absl | Using default tokenizer.
2022-12-16 12:39:53 | INFO | valid | {"epoch": 27, "valid_loss": "5.353", "valid_nll_loss": "5.353", "valid_rouge1": 0.5067917548252664, "valid_rouge2": 0.14568063619718918, "valid_rougel": 0.22573914887114024, "valid_rouge_avg": 0.32623619551122773, "valid_ppl": "40.86", "valid_wps": "106.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3659", "valid_best_rouge_avg": 0.32833887562992775}
2022-12-16 12:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 3659 updates
2022-12-16 12:39:53 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 12:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 12:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 27 @ 3659 updates, score 0.32623619551122773) (writing took 19.252031872980297 seconds)
2022-12-16 12:40:13 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-12-16 12:40:13 | INFO | train | {"epoch": 27, "train_loss": "0.331", "train_nll_loss": "0.331", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.26", "train_wps": "862.7", "train_ups": "0.06", "train_wpb": "13879.1", "train_bsz": "31.8", "train_num_updates": "3659", "train_lr": "6.67474e-05", "train_gnorm": "2.514", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1083", "train_gb_free": "62.2", "train_wall": "61864"}
2022-12-16 12:40:13 | INFO | fairseq.trainer | begin training epoch 28
2022-12-16 12:40:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 12:40:57 | INFO | train_inner | {"epoch": 28, "update": 27.007, "loss": "0.339", "nll_loss": "0.339", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.26", "wps": "109.7", "ups": "0.01", "wpb": "12930.8", "bsz": "29.6", "num_updates": "3660", "lr": "6.67368e-05", "gnorm": "3.103", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "61908"}
2022-12-16 12:42:17 | INFO | train_inner | {"epoch": 28, "update": 27.081, "loss": "0.265", "nll_loss": "0.265", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1706", "ups": "0.13", "wpb": "13647.7", "bsz": "32", "num_updates": "3670", "lr": "6.66316e-05", "gnorm": "2.002", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "61988"}
2022-12-16 12:43:36 | INFO | train_inner | {"epoch": 28, "update": 27.154, "loss": "0.264", "nll_loss": "0.264", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1776.1", "ups": "0.13", "wpb": "14134.1", "bsz": "32", "num_updates": "3680", "lr": "6.65263e-05", "gnorm": "1.921", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "62067"}
2022-12-16 12:44:58 | INFO | train_inner | {"epoch": 28, "update": 27.228, "loss": "0.273", "nll_loss": "0.273", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.21", "wps": "1735", "ups": "0.12", "wpb": "14167.1", "bsz": "32", "num_updates": "3690", "lr": "6.64211e-05", "gnorm": "1.873", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "62149"}
2022-12-16 12:46:18 | INFO | train_inner | {"epoch": 28, "update": 27.301, "loss": "0.285", "nll_loss": "0.285", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.22", "wps": "1775.4", "ups": "0.12", "wpb": "14264.3", "bsz": "32", "num_updates": "3700", "lr": "6.63158e-05", "gnorm": "2.342", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "62229"}
2022-12-16 12:47:38 | INFO | train_inner | {"epoch": 28, "update": 27.375, "loss": "0.291", "nll_loss": "0.291", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.22", "wps": "1758.1", "ups": "0.13", "wpb": "14045.6", "bsz": "32", "num_updates": "3710", "lr": "6.62105e-05", "gnorm": "2.538", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "62309"}
2022-12-16 12:48:58 | INFO | train_inner | {"epoch": 28, "update": 27.449, "loss": "0.296", "nll_loss": "0.296", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.23", "wps": "1760.5", "ups": "0.13", "wpb": "13975", "bsz": "32", "num_updates": "3720", "lr": "6.61053e-05", "gnorm": "1.707", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "62389"}
2022-12-16 12:50:19 | INFO | train_inner | {"epoch": 28, "update": 27.522, "loss": "0.298", "nll_loss": "0.298", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.23", "wps": "1709.9", "ups": "0.12", "wpb": "13979.1", "bsz": "32", "num_updates": "3730", "lr": "6.6e-05", "gnorm": "3.306", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "62470"}
2022-12-16 12:51:39 | INFO | train_inner | {"epoch": 28, "update": 27.596, "loss": "0.301", "nll_loss": "0.301", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.23", "wps": "1728.4", "ups": "0.13", "wpb": "13771", "bsz": "32", "num_updates": "3740", "lr": "6.58947e-05", "gnorm": "3.725", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "62550"}
2022-12-16 12:52:58 | INFO | train_inner | {"epoch": 28, "update": 27.669, "loss": "0.291", "nll_loss": "0.291", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.22", "wps": "1766.5", "ups": "0.13", "wpb": "13939.8", "bsz": "32", "num_updates": "3750", "lr": "6.57895e-05", "gnorm": "1.813", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "62629"}
2022-12-16 12:54:18 | INFO | train_inner | {"epoch": 28, "update": 27.743, "loss": "0.289", "nll_loss": "0.289", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.22", "wps": "1720.1", "ups": "0.12", "wpb": "13783.3", "bsz": "32", "num_updates": "3760", "lr": "6.56842e-05", "gnorm": "1.836", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "62709"}
2022-12-16 12:55:39 | INFO | train_inner | {"epoch": 28, "update": 27.816, "loss": "0.304", "nll_loss": "0.304", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.23", "wps": "1719.5", "ups": "0.12", "wpb": "13915.7", "bsz": "32", "num_updates": "3770", "lr": "6.55789e-05", "gnorm": "2.211", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "62790"}
2022-12-16 12:57:00 | INFO | train_inner | {"epoch": 28, "update": 27.89, "loss": "0.298", "nll_loss": "0.298", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.23", "wps": "1727.2", "ups": "0.12", "wpb": "13934.6", "bsz": "32", "num_updates": "3780", "lr": "6.54737e-05", "gnorm": "2.138", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "62871"}
2022-12-16 12:58:21 | INFO | train_inner | {"epoch": 28, "update": 27.963, "loss": "0.317", "nll_loss": "0.317", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.25", "wps": "1697", "ups": "0.12", "wpb": "13836.2", "bsz": "32", "num_updates": "3790", "lr": "6.53684e-05", "gnorm": "1.973", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "62953"}
2022-12-16 12:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 12:59:50 | INFO | absl | Using default tokenizer.
2022-12-16 13:00:23 | INFO | absl | Using default tokenizer.
2022-12-16 13:00:56 | INFO | absl | Using default tokenizer.
2022-12-16 13:01:29 | INFO | absl | Using default tokenizer.
2022-12-16 13:01:59 | INFO | absl | Using default tokenizer.
2022-12-16 13:02:25 | INFO | absl | Using default tokenizer.
2022-12-16 13:02:56 | INFO | absl | Using default tokenizer.
2022-12-16 13:03:25 | INFO | absl | Using default tokenizer.
2022-12-16 13:03:55 | INFO | absl | Using default tokenizer.
2022-12-16 13:04:23 | INFO | absl | Using default tokenizer.
2022-12-16 13:04:56 | INFO | absl | Using default tokenizer.
2022-12-16 13:05:32 | INFO | absl | Using default tokenizer.
2022-12-16 13:06:03 | INFO | absl | Using default tokenizer.
2022-12-16 13:06:34 | INFO | absl | Using default tokenizer.
2022-12-16 13:07:06 | INFO | absl | Using default tokenizer.
2022-12-16 13:07:43 | INFO | absl | Using default tokenizer.
2022-12-16 13:08:13 | INFO | absl | Using default tokenizer.
2022-12-16 13:08:46 | INFO | absl | Using default tokenizer.
2022-12-16 13:09:15 | INFO | absl | Using default tokenizer.
2022-12-16 13:09:46 | INFO | absl | Using default tokenizer.
2022-12-16 13:10:17 | INFO | absl | Using default tokenizer.
2022-12-16 13:10:56 | INFO | absl | Using default tokenizer.
2022-12-16 13:11:31 | INFO | absl | Using default tokenizer.
2022-12-16 13:12:02 | INFO | absl | Using default tokenizer.
2022-12-16 13:12:33 | INFO | absl | Using default tokenizer.
2022-12-16 13:13:11 | INFO | absl | Using default tokenizer.
2022-12-16 13:13:46 | INFO | absl | Using default tokenizer.
2022-12-16 13:14:19 | INFO | absl | Using default tokenizer.
2022-12-16 13:14:59 | INFO | absl | Using default tokenizer.
2022-12-16 13:15:33 | INFO | absl | Using default tokenizer.
2022-12-16 13:16:06 | INFO | absl | Using default tokenizer.
2022-12-16 13:16:30 | INFO | valid | {"epoch": 28, "valid_loss": "5.45", "valid_nll_loss": "5.45", "valid_rouge1": 0.5117770712900971, "valid_rouge2": 0.1451574570423822, "valid_rougel": 0.22494282402968774, "valid_rouge_avg": 0.32846726416623967, "valid_ppl": "43.71", "valid_wps": "106.3", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3795", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 13:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 3795 updates
2022-12-16 13:16:30 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 13:16:56 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 13:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 28 @ 3795 updates, score 0.32846726416623967) (writing took 683.0057565970346 seconds)
2022-12-16 13:27:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-12-16 13:27:53 | INFO | train | {"epoch": 28, "train_loss": "0.291", "train_nll_loss": "0.291", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.22", "train_wps": "659.8", "train_ups": "0.05", "train_wpb": "13876.1", "train_bsz": "31.8", "train_num_updates": "3795", "train_lr": "6.53158e-05", "train_gnorm": "2.248", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1088", "train_gb_free": "62.2", "train_wall": "64724"}
2022-12-16 13:27:53 | INFO | fairseq.trainer | begin training epoch 29
2022-12-16 13:27:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 13:29:04 | INFO | train_inner | {"epoch": 29, "update": 28.037, "loss": "0.269", "nll_loss": "0.269", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "69.7", "ups": "0.01", "wpb": "12845.7", "bsz": "29.6", "num_updates": "3800", "lr": "6.52632e-05", "gnorm": "2.177", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.9", "wall": "64795"}
2022-12-16 13:30:23 | INFO | train_inner | {"epoch": 29, "update": 28.11, "loss": "0.242", "nll_loss": "0.242", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1780.8", "ups": "0.13", "wpb": "13950.6", "bsz": "32", "num_updates": "3810", "lr": "6.51579e-05", "gnorm": "1.626", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "64874"}
2022-12-16 13:31:45 | INFO | train_inner | {"epoch": 29, "update": 28.184, "loss": "0.248", "nll_loss": "0.248", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1691.9", "ups": "0.12", "wpb": "13911.4", "bsz": "32", "num_updates": "3820", "lr": "6.50526e-05", "gnorm": "2.181", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "64956"}
2022-12-16 13:33:06 | INFO | train_inner | {"epoch": 29, "update": 28.257, "loss": "0.248", "nll_loss": "0.248", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1717", "ups": "0.12", "wpb": "13939.9", "bsz": "32", "num_updates": "3830", "lr": "6.49474e-05", "gnorm": "1.968", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "65037"}
2022-12-16 13:34:25 | INFO | train_inner | {"epoch": 29, "update": 28.331, "loss": "0.26", "nll_loss": "0.26", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1762.8", "ups": "0.13", "wpb": "13992.4", "bsz": "32", "num_updates": "3840", "lr": "6.48421e-05", "gnorm": "2.478", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "65117"}
2022-12-16 13:35:45 | INFO | train_inner | {"epoch": 29, "update": 28.404, "loss": "0.254", "nll_loss": "0.254", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1732.3", "ups": "0.13", "wpb": "13798.2", "bsz": "32", "num_updates": "3850", "lr": "6.47368e-05", "gnorm": "2.064", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "65196"}
2022-12-16 13:37:05 | INFO | train_inner | {"epoch": 29, "update": 28.478, "loss": "0.259", "nll_loss": "0.259", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1761.9", "ups": "0.12", "wpb": "14144", "bsz": "32", "num_updates": "3860", "lr": "6.46316e-05", "gnorm": "3.058", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "65276"}
2022-12-16 13:38:27 | INFO | train_inner | {"epoch": 29, "update": 28.551, "loss": "0.257", "nll_loss": "0.257", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1714.5", "ups": "0.12", "wpb": "14005.5", "bsz": "32", "num_updates": "3870", "lr": "6.45263e-05", "gnorm": "1.909", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "65358"}
2022-12-16 13:39:46 | INFO | train_inner | {"epoch": 29, "update": 28.625, "loss": "0.259", "nll_loss": "0.259", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.2", "wps": "1775", "ups": "0.13", "wpb": "14047.3", "bsz": "32", "num_updates": "3880", "lr": "6.44211e-05", "gnorm": "2.774", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "65437"}
2022-12-16 13:41:08 | INFO | train_inner | {"epoch": 29, "update": 28.699, "loss": "0.256", "nll_loss": "0.256", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1754.3", "ups": "0.12", "wpb": "14295.2", "bsz": "32", "num_updates": "3890", "lr": "6.43158e-05", "gnorm": "1.783", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "65519"}
2022-12-16 13:42:28 | INFO | train_inner | {"epoch": 29, "update": 28.772, "loss": "0.272", "nll_loss": "0.272", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.21", "wps": "1716.6", "ups": "0.12", "wpb": "13760.5", "bsz": "32", "num_updates": "3900", "lr": "6.42105e-05", "gnorm": "6.514", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "65599"}
2022-12-16 13:43:47 | INFO | train_inner | {"epoch": 29, "update": 28.846, "loss": "0.276", "nll_loss": "0.276", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.21", "wps": "1751.1", "ups": "0.13", "wpb": "13902.3", "bsz": "32", "num_updates": "3910", "lr": "6.41053e-05", "gnorm": "1.728", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "65678"}
2022-12-16 13:45:09 | INFO | train_inner | {"epoch": 29, "update": 28.919, "loss": "0.285", "nll_loss": "0.285", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.22", "wps": "1717.5", "ups": "0.12", "wpb": "14033.1", "bsz": "32", "num_updates": "3920", "lr": "6.4e-05", "gnorm": "2.833", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "65760"}
2022-12-16 13:46:31 | INFO | train_inner | {"epoch": 29, "update": 28.993, "loss": "0.276", "nll_loss": "0.276", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.21", "wps": "1681", "ups": "0.12", "wpb": "13805.9", "bsz": "32", "num_updates": "3930", "lr": "6.38947e-05", "gnorm": "1.878", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "65842"}
2022-12-16 13:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-12-16 13:47:24 | INFO | absl | Using default tokenizer.
2022-12-16 13:47:58 | INFO | absl | Using default tokenizer.
2022-12-16 13:48:24 | INFO | absl | Using default tokenizer.
2022-12-16 13:48:53 | INFO | absl | Using default tokenizer.
2022-12-16 13:49:24 | INFO | absl | Using default tokenizer.
2022-12-16 13:49:51 | INFO | absl | Using default tokenizer.
2022-12-16 13:50:21 | INFO | absl | Using default tokenizer.
2022-12-16 13:50:49 | INFO | absl | Using default tokenizer.
2022-12-16 13:51:26 | INFO | absl | Using default tokenizer.
2022-12-16 13:51:58 | INFO | absl | Using default tokenizer.
2022-12-16 13:52:33 | INFO | absl | Using default tokenizer.
2022-12-16 13:53:08 | INFO | absl | Using default tokenizer.
2022-12-16 13:53:42 | INFO | absl | Using default tokenizer.
2022-12-16 13:54:17 | INFO | absl | Using default tokenizer.
2022-12-16 13:54:45 | INFO | absl | Using default tokenizer.
2022-12-16 13:55:18 | INFO | absl | Using default tokenizer.
2022-12-16 13:55:49 | INFO | absl | Using default tokenizer.
2022-12-16 13:56:20 | INFO | absl | Using default tokenizer.
2022-12-16 13:56:53 | INFO | absl | Using default tokenizer.
2022-12-16 13:57:24 | INFO | absl | Using default tokenizer.
2022-12-16 13:57:56 | INFO | absl | Using default tokenizer.
2022-12-16 13:58:38 | INFO | absl | Using default tokenizer.
2022-12-16 13:59:10 | INFO | absl | Using default tokenizer.
2022-12-16 13:59:42 | INFO | absl | Using default tokenizer.
2022-12-16 14:00:18 | INFO | absl | Using default tokenizer.
2022-12-16 14:00:52 | INFO | absl | Using default tokenizer.
2022-12-16 14:01:30 | INFO | absl | Using default tokenizer.
2022-12-16 14:02:04 | INFO | absl | Using default tokenizer.
2022-12-16 14:02:38 | INFO | absl | Using default tokenizer.
2022-12-16 14:03:18 | INFO | absl | Using default tokenizer.
2022-12-16 14:03:51 | INFO | absl | Using default tokenizer.
2022-12-16 14:04:16 | INFO | valid | {"epoch": 29, "valid_loss": "5.52", "valid_nll_loss": "5.52", "valid_rouge1": 0.5082422387022083, "valid_rouge2": 0.14374669915138322, "valid_rougel": 0.22455211411536996, "valid_rouge_avg": 0.32599446892679573, "valid_ppl": "45.9", "valid_wps": "105.2", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "3931", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 14:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 3931 updates
2022-12-16 14:04:16 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 14:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 14:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 29 @ 3931 updates, score 0.32599446892679573) (writing took 23.53869557613507 seconds)
2022-12-16 14:04:40 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-12-16 14:04:40 | INFO | train | {"epoch": 29, "train_loss": "0.26", "train_nll_loss": "0.26", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.2", "train_wps": "855.4", "train_ups": "0.06", "train_wpb": "13881.3", "train_bsz": "31.8", "train_num_updates": "3931", "train_lr": "6.38842e-05", "train_gnorm": "2.528", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1090", "train_gb_free": "62.2", "train_wall": "66931"}
2022-12-16 14:04:40 | INFO | fairseq.trainer | begin training epoch 30
2022-12-16 14:04:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 14:06:32 | INFO | train_inner | {"epoch": 30, "update": 29.066, "loss": "0.223", "nll_loss": "0.223", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.17", "wps": "106.3", "ups": "0.01", "wpb": "12760.5", "bsz": "29.6", "num_updates": "3940", "lr": "6.37895e-05", "gnorm": "1.581", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "67043"}
2022-12-16 14:07:53 | INFO | train_inner | {"epoch": 30, "update": 29.14, "loss": "0.224", "nll_loss": "0.224", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.17", "wps": "1721.2", "ups": "0.12", "wpb": "13877", "bsz": "32", "num_updates": "3950", "lr": "6.36842e-05", "gnorm": "1.203", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "67124"}
2022-12-16 14:09:12 | INFO | train_inner | {"epoch": 30, "update": 29.213, "loss": "0.233", "nll_loss": "0.233", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.17", "wps": "1788.4", "ups": "0.13", "wpb": "14137.1", "bsz": "32", "num_updates": "3960", "lr": "6.35789e-05", "gnorm": "1.19", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "67203"}
2022-12-16 14:10:30 | INFO | train_inner | {"epoch": 30, "update": 29.287, "loss": "0.229", "nll_loss": "0.229", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.17", "wps": "1760.2", "ups": "0.13", "wpb": "13721.4", "bsz": "32", "num_updates": "3970", "lr": "6.34737e-05", "gnorm": "1.183", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "67281"}
2022-12-16 14:11:50 | INFO | train_inner | {"epoch": 30, "update": 29.36, "loss": "0.243", "nll_loss": "0.243", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1743.3", "ups": "0.13", "wpb": "13935.1", "bsz": "32", "num_updates": "3980", "lr": "6.33684e-05", "gnorm": "1.285", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "67361"}
2022-12-16 14:13:09 | INFO | train_inner | {"epoch": 30, "update": 29.434, "loss": "0.24", "nll_loss": "0.24", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1740.4", "ups": "0.13", "wpb": "13897.9", "bsz": "32", "num_updates": "3990", "lr": "6.32632e-05", "gnorm": "1.279", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "67441"}
2022-12-16 14:14:30 | INFO | train_inner | {"epoch": 30, "update": 29.507, "loss": "0.248", "nll_loss": "0.248", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1715.1", "ups": "0.12", "wpb": "13845", "bsz": "32", "num_updates": "4000", "lr": "6.31579e-05", "gnorm": "1.257", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "67521"}
2022-12-16 14:15:51 | INFO | train_inner | {"epoch": 30, "update": 29.581, "loss": "0.247", "nll_loss": "0.247", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1741.5", "ups": "0.12", "wpb": "14144.6", "bsz": "32", "num_updates": "4010", "lr": "6.30526e-05", "gnorm": "1.26", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "67603"}
2022-12-16 14:17:11 | INFO | train_inner | {"epoch": 30, "update": 29.654, "loss": "0.243", "nll_loss": "0.243", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1770", "ups": "0.13", "wpb": "14146.3", "bsz": "32", "num_updates": "4020", "lr": "6.29474e-05", "gnorm": "1.23", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.9", "wall": "67682"}
2022-12-16 14:18:33 | INFO | train_inner | {"epoch": 30, "update": 29.728, "loss": "0.244", "nll_loss": "0.244", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1717.1", "ups": "0.12", "wpb": "13954.1", "bsz": "32", "num_updates": "4030", "lr": "6.28421e-05", "gnorm": "1.191", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "67764"}
2022-12-16 14:19:54 | INFO | train_inner | {"epoch": 30, "update": 29.801, "loss": "0.248", "nll_loss": "0.248", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1718.2", "ups": "0.12", "wpb": "13936.3", "bsz": "32", "num_updates": "4040", "lr": "6.27368e-05", "gnorm": "1.265", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "67845"}
2022-12-16 14:21:13 | INFO | train_inner | {"epoch": 30, "update": 29.875, "loss": "0.243", "nll_loss": "0.243", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.18", "wps": "1733.8", "ups": "0.13", "wpb": "13794.3", "bsz": "32", "num_updates": "4050", "lr": "6.26316e-05", "gnorm": "1.194", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "67925"}
2022-12-16 14:22:33 | INFO | train_inner | {"epoch": 30, "update": 29.949, "loss": "0.245", "nll_loss": "0.245", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.19", "wps": "1786.3", "ups": "0.13", "wpb": "14239.5", "bsz": "32", "num_updates": "4060", "lr": "6.25263e-05", "gnorm": "1.602", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "68004"}
2022-12-16 14:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 14:24:16 | INFO | absl | Using default tokenizer.
2022-12-16 14:24:44 | INFO | absl | Using default tokenizer.
2022-12-16 14:25:11 | INFO | absl | Using default tokenizer.
2022-12-16 14:25:47 | INFO | absl | Using default tokenizer.
2022-12-16 14:26:18 | INFO | absl | Using default tokenizer.
2022-12-16 14:26:48 | INFO | absl | Using default tokenizer.
2022-12-16 14:27:17 | INFO | absl | Using default tokenizer.
2022-12-16 14:27:47 | INFO | absl | Using default tokenizer.
2022-12-16 14:28:23 | INFO | absl | Using default tokenizer.
2022-12-16 14:28:53 | INFO | absl | Using default tokenizer.
2022-12-16 14:29:23 | INFO | absl | Using default tokenizer.
2022-12-16 14:29:58 | INFO | absl | Using default tokenizer.
2022-12-16 14:30:31 | INFO | absl | Using default tokenizer.
2022-12-16 14:31:05 | INFO | absl | Using default tokenizer.
2022-12-16 14:31:35 | INFO | absl | Using default tokenizer.
2022-12-16 14:32:11 | INFO | absl | Using default tokenizer.
2022-12-16 14:32:42 | INFO | absl | Using default tokenizer.
2022-12-16 14:33:15 | INFO | absl | Using default tokenizer.
2022-12-16 14:33:50 | INFO | absl | Using default tokenizer.
2022-12-16 14:34:27 | INFO | absl | Using default tokenizer.
2022-12-16 14:35:01 | INFO | absl | Using default tokenizer.
2022-12-16 14:35:38 | INFO | absl | Using default tokenizer.
2022-12-16 14:36:15 | INFO | absl | Using default tokenizer.
2022-12-16 14:36:50 | INFO | absl | Using default tokenizer.
2022-12-16 14:37:25 | INFO | absl | Using default tokenizer.
2022-12-16 14:37:57 | INFO | absl | Using default tokenizer.
2022-12-16 14:38:41 | INFO | absl | Using default tokenizer.
2022-12-16 14:39:11 | INFO | absl | Using default tokenizer.
2022-12-16 14:39:47 | INFO | absl | Using default tokenizer.
2022-12-16 14:40:27 | INFO | absl | Using default tokenizer.
2022-12-16 14:41:06 | INFO | absl | Using default tokenizer.
2022-12-16 14:41:34 | INFO | valid | {"epoch": 30, "valid_loss": "5.608", "valid_nll_loss": "5.608", "valid_rouge1": 0.5101697324027454, "valid_rouge2": 0.14394082864460916, "valid_rougel": 0.22449130306801215, "valid_rouge_avg": 0.3270552805236773, "valid_ppl": "48.77", "valid_wps": "102.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4067", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 14:41:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4067 updates
2022-12-16 14:41:34 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 14:42:00 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 14:42:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 30 @ 4067 updates, score 0.3270552805236773) (writing took 25.630094073014334 seconds)
2022-12-16 14:42:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-12-16 14:42:00 | INFO | train | {"epoch": 30, "train_loss": "0.24", "train_nll_loss": "0.24", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.18", "train_wps": "842.7", "train_ups": "0.06", "train_wpb": "13881.2", "train_bsz": "31.8", "train_num_updates": "4067", "train_lr": "6.24526e-05", "train_gnorm": "1.283", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "69171"}
2022-12-16 14:42:00 | INFO | fairseq.trainer | begin training epoch 31
2022-12-16 14:42:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 14:43:04 | INFO | train_inner | {"epoch": 31, "update": 30.022, "loss": "0.23", "nll_loss": "0.23", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.17", "wps": "104.6", "ups": "0.01", "wpb": "12878.8", "bsz": "29.6", "num_updates": "4070", "lr": "6.24211e-05", "gnorm": "1.515", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "69235"}
2022-12-16 14:44:24 | INFO | train_inner | {"epoch": 31, "update": 30.096, "loss": "0.196", "nll_loss": "0.196", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1745", "ups": "0.12", "wpb": "14040.9", "bsz": "32", "num_updates": "4080", "lr": "6.23158e-05", "gnorm": "1.517", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "69316"}
2022-12-16 14:45:44 | INFO | train_inner | {"epoch": 31, "update": 30.169, "loss": "0.194", "nll_loss": "0.194", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1732.4", "ups": "0.13", "wpb": "13802.5", "bsz": "32", "num_updates": "4090", "lr": "6.22105e-05", "gnorm": "1.339", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "69395"}
2022-12-16 14:47:03 | INFO | train_inner | {"epoch": 31, "update": 30.243, "loss": "0.202", "nll_loss": "0.202", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1785", "ups": "0.13", "wpb": "13999.8", "bsz": "32", "num_updates": "4100", "lr": "6.21053e-05", "gnorm": "1.544", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "69474"}
2022-12-16 14:48:22 | INFO | train_inner | {"epoch": 31, "update": 30.316, "loss": "0.202", "nll_loss": "0.202", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1724.3", "ups": "0.13", "wpb": "13676.7", "bsz": "32", "num_updates": "4110", "lr": "6.2e-05", "gnorm": "1.148", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "69553"}
2022-12-16 14:49:43 | INFO | train_inner | {"epoch": 31, "update": 30.39, "loss": "0.205", "nll_loss": "0.205", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1674.8", "ups": "0.12", "wpb": "13644.2", "bsz": "32", "num_updates": "4120", "lr": "6.18947e-05", "gnorm": "1.361", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "69634"}
2022-12-16 14:51:02 | INFO | train_inner | {"epoch": 31, "update": 30.463, "loss": "0.201", "nll_loss": "0.201", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1811.6", "ups": "0.13", "wpb": "14277.1", "bsz": "32", "num_updates": "4130", "lr": "6.17895e-05", "gnorm": "1.456", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "69713"}
2022-12-16 14:52:22 | INFO | train_inner | {"epoch": 31, "update": 30.537, "loss": "0.207", "nll_loss": "0.207", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1721.3", "ups": "0.12", "wpb": "13815.8", "bsz": "32", "num_updates": "4140", "lr": "6.16842e-05", "gnorm": "2.536", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "69794"}
2022-12-16 14:53:45 | INFO | train_inner | {"epoch": 31, "update": 30.61, "loss": "0.199", "nll_loss": "0.199", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1701.1", "ups": "0.12", "wpb": "14001.6", "bsz": "32", "num_updates": "4150", "lr": "6.15789e-05", "gnorm": "2.854", "clip": "100", "loss_scale": "0.0625", "train_wall": "82", "gb_free": "62.2", "wall": "69876"}
2022-12-16 14:55:05 | INFO | train_inner | {"epoch": 31, "update": 30.684, "loss": "0.208", "nll_loss": "0.208", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.16", "wps": "1730.3", "ups": "0.12", "wpb": "13915", "bsz": "32", "num_updates": "4160", "lr": "6.14737e-05", "gnorm": "5.097", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "69956"}
2022-12-16 14:56:26 | INFO | train_inner | {"epoch": 31, "update": 30.757, "loss": "0.204", "nll_loss": "0.204", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1758.4", "ups": "0.12", "wpb": "14200.2", "bsz": "32", "num_updates": "4170", "lr": "6.13684e-05", "gnorm": "1.678", "clip": "100", "loss_scale": "0.0625", "train_wall": "80", "gb_free": "62.2", "wall": "70037"}
2022-12-16 14:57:47 | INFO | train_inner | {"epoch": 31, "update": 30.831, "loss": "0.198", "nll_loss": "0.198", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1763.1", "ups": "0.12", "wpb": "14285", "bsz": "32", "num_updates": "4180", "lr": "6.12632e-05", "gnorm": "1.41", "clip": "100", "loss_scale": "0.0625", "train_wall": "81", "gb_free": "62.2", "wall": "70118"}
2022-12-16 14:59:06 | INFO | train_inner | {"epoch": 31, "update": 30.904, "loss": "0.21", "nll_loss": "0.21", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.16", "wps": "1793.6", "ups": "0.13", "wpb": "14125.7", "bsz": "32", "num_updates": "4190", "lr": "6.11579e-05", "gnorm": "1.565", "clip": "100", "loss_scale": "0.0625", "train_wall": "78", "gb_free": "62.2", "wall": "70197"}
2022-12-16 15:00:25 | INFO | train_inner | {"epoch": 31, "update": 30.978, "loss": "0.21", "nll_loss": "0.21", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.16", "wps": "1719.7", "ups": "0.13", "wpb": "13648.1", "bsz": "32", "num_updates": "4200", "lr": "6.10526e-05", "gnorm": "1.359", "clip": "100", "loss_scale": "0.0625", "train_wall": "79", "gb_free": "62.2", "wall": "70276"}
2022-12-16 15:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 15:01:36 | INFO | absl | Using default tokenizer.
2022-12-16 15:02:08 | INFO | absl | Using default tokenizer.
2022-12-16 15:02:36 | INFO | absl | Using default tokenizer.
2022-12-16 15:03:04 | INFO | absl | Using default tokenizer.
2022-12-16 15:03:34 | INFO | absl | Using default tokenizer.
2022-12-16 15:04:03 | INFO | absl | Using default tokenizer.
2022-12-16 15:04:33 | INFO | absl | Using default tokenizer.
2022-12-16 15:05:06 | INFO | absl | Using default tokenizer.
2022-12-16 15:05:40 | INFO | absl | Using default tokenizer.
2022-12-16 15:06:10 | INFO | absl | Using default tokenizer.
2022-12-16 15:06:42 | INFO | absl | Using default tokenizer.
2022-12-16 15:07:20 | INFO | absl | Using default tokenizer.
2022-12-16 15:07:48 | INFO | absl | Using default tokenizer.
2022-12-16 15:08:25 | INFO | absl | Using default tokenizer.
2022-12-16 15:08:57 | INFO | absl | Using default tokenizer.
2022-12-16 15:09:26 | INFO | absl | Using default tokenizer.
2022-12-16 15:09:55 | INFO | absl | Using default tokenizer.
2022-12-16 15:10:25 | INFO | absl | Using default tokenizer.
2022-12-16 15:11:05 | INFO | absl | Using default tokenizer.
2022-12-16 15:11:40 | INFO | absl | Using default tokenizer.
2022-12-16 15:12:15 | INFO | absl | Using default tokenizer.
2022-12-16 15:12:51 | INFO | absl | Using default tokenizer.
2022-12-16 15:13:18 | INFO | absl | Using default tokenizer.
2022-12-16 15:13:49 | INFO | absl | Using default tokenizer.
2022-12-16 15:14:25 | INFO | absl | Using default tokenizer.
2022-12-16 15:14:59 | INFO | absl | Using default tokenizer.
2022-12-16 15:15:33 | INFO | absl | Using default tokenizer.
2022-12-16 15:16:03 | INFO | absl | Using default tokenizer.
2022-12-16 15:16:49 | INFO | absl | Using default tokenizer.
2022-12-16 15:17:19 | INFO | absl | Using default tokenizer.
2022-12-16 15:17:54 | INFO | absl | Using default tokenizer.
2022-12-16 15:18:25 | INFO | valid | {"epoch": 31, "valid_loss": "5.712", "valid_nll_loss": "5.712", "valid_rouge1": 0.5105961410664843, "valid_rouge2": 0.1459874184217942, "valid_rougel": 0.2264305761472251, "valid_rouge_avg": 0.32829177974413926, "valid_ppl": "52.43", "valid_wps": "106.1", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4203", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 15:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4203 updates
2022-12-16 15:18:25 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 15:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 15:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 31 @ 4203 updates, score 0.32829177974413926) (writing took 26.436060992069542 seconds)
2022-12-16 15:18:51 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-12-16 15:18:51 | INFO | train | {"epoch": 31, "train_loss": "0.203", "train_nll_loss": "0.203", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.15", "train_wps": "853.7", "train_ups": "0.06", "train_wpb": "13881.3", "train_bsz": "31.8", "train_num_updates": "4203", "train_lr": "6.10211e-05", "train_gnorm": "1.897", "train_clip": "100", "train_loss_scale": "0.0625", "train_train_wall": "1087", "train_gb_free": "62.2", "train_wall": "71382"}
2022-12-16 15:18:51 | INFO | fairseq.trainer | begin training epoch 32
2022-12-16 15:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 15:20:28 | INFO | train_inner | {"epoch": 32, "update": 31.051, "loss": "0.185", "nll_loss": "0.185", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "107.7", "ups": "0.01", "wpb": "12959.9", "bsz": "29.6", "num_updates": "4210", "lr": "6.09474e-05", "gnorm": "1.356", "clip": "100", "loss_scale": "0.0625", "train_wall": "83", "gb_free": "62.2", "wall": "71479"}
2022-12-16 15:21:47 | INFO | train_inner | {"epoch": 32, "update": 31.125, "loss": "0.178", "nll_loss": "0.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1769.3", "ups": "0.13", "wpb": "13951.8", "bsz": "32", "num_updates": "4220", "lr": "6.08421e-05", "gnorm": "1.213", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "71558"}
2022-12-16 15:23:06 | INFO | train_inner | {"epoch": 32, "update": 31.199, "loss": "0.18", "nll_loss": "0.18", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1753.2", "ups": "0.13", "wpb": "13891.3", "bsz": "32", "num_updates": "4230", "lr": "6.07368e-05", "gnorm": "1.525", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63.6", "wall": "71637"}
2022-12-16 15:24:28 | INFO | train_inner | {"epoch": 32, "update": 31.272, "loss": "0.188", "nll_loss": "0.188", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1740.5", "ups": "0.12", "wpb": "14148.7", "bsz": "32", "num_updates": "4240", "lr": "6.06316e-05", "gnorm": "12.516", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "71719"}
2022-12-16 15:25:46 | INFO | train_inner | {"epoch": 32, "update": 31.346, "loss": "0.178", "nll_loss": "0.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1738.1", "ups": "0.13", "wpb": "13629.8", "bsz": "32", "num_updates": "4250", "lr": "6.05263e-05", "gnorm": "1.563", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "71797"}
2022-12-16 15:27:07 | INFO | train_inner | {"epoch": 32, "update": 31.419, "loss": "0.192", "nll_loss": "0.192", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1752.7", "ups": "0.12", "wpb": "14219.5", "bsz": "32", "num_updates": "4260", "lr": "6.04211e-05", "gnorm": "3.197", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "71878"}
2022-12-16 15:28:26 | INFO | train_inner | {"epoch": 32, "update": 31.493, "loss": "0.181", "nll_loss": "0.181", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1795.5", "ups": "0.13", "wpb": "14229", "bsz": "32", "num_updates": "4270", "lr": "6.03158e-05", "gnorm": "1.349", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "71957"}
2022-12-16 15:29:47 | INFO | train_inner | {"epoch": 32, "update": 31.566, "loss": "0.191", "nll_loss": "0.191", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1689.3", "ups": "0.12", "wpb": "13681.5", "bsz": "32", "num_updates": "4280", "lr": "6.02105e-05", "gnorm": "1.243", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "72038"}
2022-12-16 15:31:08 | INFO | train_inner | {"epoch": 32, "update": 31.64, "loss": "0.194", "nll_loss": "0.194", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1724.7", "ups": "0.12", "wpb": "13942.7", "bsz": "32", "num_updates": "4290", "lr": "6.01053e-05", "gnorm": "3.309", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "72119"}
2022-12-16 15:32:28 | INFO | train_inner | {"epoch": 32, "update": 31.713, "loss": "0.207", "nll_loss": "0.207", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1731.5", "ups": "0.13", "wpb": "13837.7", "bsz": "32", "num_updates": "4300", "lr": "6e-05", "gnorm": "2.686", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "72199"}
2022-12-16 15:33:49 | INFO | train_inner | {"epoch": 32, "update": 31.787, "loss": "0.201", "nll_loss": "0.201", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1708.2", "ups": "0.12", "wpb": "13832.8", "bsz": "32", "num_updates": "4310", "lr": "5.98947e-05", "gnorm": "2.576", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "72280"}
2022-12-16 15:35:09 | INFO | train_inner | {"epoch": 32, "update": 31.86, "loss": "0.199", "nll_loss": "0.199", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.15", "wps": "1747.8", "ups": "0.12", "wpb": "14008.8", "bsz": "32", "num_updates": "4320", "lr": "5.97895e-05", "gnorm": "4.736", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "72360"}
2022-12-16 15:36:30 | INFO | train_inner | {"epoch": 32, "update": 31.934, "loss": "0.195", "nll_loss": "0.195", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1741.9", "ups": "0.12", "wpb": "14111.6", "bsz": "32", "num_updates": "4330", "lr": "5.96842e-05", "gnorm": "1.819", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "72441"}
2022-12-16 15:37:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 15:38:30 | INFO | absl | Using default tokenizer.
2022-12-16 15:39:03 | INFO | absl | Using default tokenizer.
2022-12-16 15:39:32 | INFO | absl | Using default tokenizer.
2022-12-16 15:40:01 | INFO | absl | Using default tokenizer.
2022-12-16 15:40:29 | INFO | absl | Using default tokenizer.
2022-12-16 15:40:59 | INFO | absl | Using default tokenizer.
2022-12-16 15:41:29 | INFO | absl | Using default tokenizer.
2022-12-16 15:42:00 | INFO | absl | Using default tokenizer.
2022-12-16 15:42:37 | INFO | absl | Using default tokenizer.
2022-12-16 15:43:08 | INFO | absl | Using default tokenizer.
2022-12-16 15:43:42 | INFO | absl | Using default tokenizer.
2022-12-16 15:44:14 | INFO | absl | Using default tokenizer.
2022-12-16 15:44:47 | INFO | absl | Using default tokenizer.
2022-12-16 15:45:23 | INFO | absl | Using default tokenizer.
2022-12-16 15:45:57 | INFO | absl | Using default tokenizer.
2022-12-16 15:46:28 | INFO | absl | Using default tokenizer.
2022-12-16 15:47:03 | INFO | absl | Using default tokenizer.
2022-12-16 15:47:33 | INFO | absl | Using default tokenizer.
2022-12-16 15:48:13 | INFO | absl | Using default tokenizer.
2022-12-16 15:48:50 | INFO | absl | Using default tokenizer.
2022-12-16 15:49:22 | INFO | absl | Using default tokenizer.
2022-12-16 15:49:59 | INFO | absl | Using default tokenizer.
2022-12-16 15:50:29 | INFO | absl | Using default tokenizer.
2022-12-16 15:51:03 | INFO | absl | Using default tokenizer.
2022-12-16 15:51:37 | INFO | absl | Using default tokenizer.
2022-12-16 15:52:10 | INFO | absl | Using default tokenizer.
2022-12-16 15:52:41 | INFO | absl | Using default tokenizer.
2022-12-16 15:53:14 | INFO | absl | Using default tokenizer.
2022-12-16 15:53:50 | INFO | absl | Using default tokenizer.
2022-12-16 15:54:22 | INFO | absl | Using default tokenizer.
2022-12-16 15:54:54 | INFO | absl | Using default tokenizer.
2022-12-16 15:55:25 | INFO | valid | {"epoch": 32, "valid_loss": "5.784", "valid_nll_loss": "5.784", "valid_rouge1": 0.5105560141394883, "valid_rouge2": 0.14588394164423324, "valid_rougel": 0.22700509026410887, "valid_rouge_avg": 0.32821997789186086, "valid_ppl": "55.09", "valid_wps": "105.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4339", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 15:55:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 4339 updates
2022-12-16 15:55:25 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 15:55:47 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 15:55:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 32 @ 4339 updates, score 0.32821997789186086) (writing took 21.856967637082562 seconds)
2022-12-16 15:55:47 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-12-16 15:55:47 | INFO | train | {"epoch": 32, "train_loss": "0.19", "train_nll_loss": "0.19", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.14", "train_wps": "851.9", "train_ups": "0.06", "train_wpb": "13879.7", "train_bsz": "31.8", "train_num_updates": "4339", "train_lr": "5.95895e-05", "train_gnorm": "3.055", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "73598"}
2022-12-16 15:55:47 | INFO | fairseq.trainer | begin training epoch 33
2022-12-16 15:55:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 15:56:34 | INFO | train_inner | {"epoch": 33, "update": 32.007, "loss": "0.192", "nll_loss": "0.192", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "107.6", "ups": "0.01", "wpb": "12945.1", "bsz": "29.6", "num_updates": "4340", "lr": "5.95789e-05", "gnorm": "3.176", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "73645"}
2022-12-16 15:57:54 | INFO | train_inner | {"epoch": 33, "update": 32.081, "loss": "0.162", "nll_loss": "0.162", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1723.1", "ups": "0.12", "wpb": "13818.5", "bsz": "32", "num_updates": "4350", "lr": "5.94737e-05", "gnorm": "1.197", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "73725"}
2022-12-16 15:59:13 | INFO | train_inner | {"epoch": 33, "update": 32.154, "loss": "0.165", "nll_loss": "0.165", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1749.6", "ups": "0.13", "wpb": "13846.2", "bsz": "32", "num_updates": "4360", "lr": "5.93684e-05", "gnorm": "1.039", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63", "wall": "73804"}
2022-12-16 16:00:33 | INFO | train_inner | {"epoch": 33, "update": 32.228, "loss": "0.172", "nll_loss": "0.172", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1705.4", "ups": "0.12", "wpb": "13699.8", "bsz": "32", "num_updates": "4370", "lr": "5.92632e-05", "gnorm": "2.215", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "64.3", "wall": "73885"}
2022-12-16 16:01:52 | INFO | train_inner | {"epoch": 33, "update": 32.301, "loss": "0.171", "nll_loss": "0.171", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1784.2", "ups": "0.13", "wpb": "14083.7", "bsz": "32", "num_updates": "4380", "lr": "5.91579e-05", "gnorm": "1.513", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "73963"}
2022-12-16 16:03:13 | INFO | train_inner | {"epoch": 33, "update": 32.375, "loss": "0.181", "nll_loss": "0.181", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1777.7", "ups": "0.12", "wpb": "14239.7", "bsz": "32", "num_updates": "4390", "lr": "5.90526e-05", "gnorm": "6.545", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "74044"}
2022-12-16 16:04:33 | INFO | train_inner | {"epoch": 33, "update": 32.449, "loss": "0.178", "nll_loss": "0.178", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1717.4", "ups": "0.12", "wpb": "13768.3", "bsz": "32", "num_updates": "4400", "lr": "5.89474e-05", "gnorm": "1.813", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "64.3", "wall": "74124"}
2022-12-16 16:05:52 | INFO | train_inner | {"epoch": 33, "update": 32.522, "loss": "0.185", "nll_loss": "0.185", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1786.2", "ups": "0.13", "wpb": "14097.1", "bsz": "32", "num_updates": "4410", "lr": "5.88421e-05", "gnorm": "3.065", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.9", "wall": "74203"}
2022-12-16 16:07:13 | INFO | train_inner | {"epoch": 33, "update": 32.596, "loss": "0.184", "nll_loss": "0.184", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.14", "wps": "1727.3", "ups": "0.12", "wpb": "14016.8", "bsz": "32", "num_updates": "4420", "lr": "5.87368e-05", "gnorm": "1.654", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "74284"}
2022-12-16 16:08:34 | INFO | train_inner | {"epoch": 33, "update": 32.669, "loss": "0.176", "nll_loss": "0.176", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1738.5", "ups": "0.12", "wpb": "14046.8", "bsz": "32", "num_updates": "4430", "lr": "5.86316e-05", "gnorm": "3.612", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "74365"}
2022-12-16 16:09:54 | INFO | train_inner | {"epoch": 33, "update": 32.743, "loss": "0.179", "nll_loss": "0.179", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1727.5", "ups": "0.12", "wpb": "13882.1", "bsz": "32", "num_updates": "4440", "lr": "5.85263e-05", "gnorm": "7.38", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63", "wall": "74445"}
2022-12-16 16:11:14 | INFO | train_inner | {"epoch": 33, "update": 32.816, "loss": "0.181", "nll_loss": "0.181", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1725.7", "ups": "0.12", "wpb": "13844", "bsz": "32", "num_updates": "4450", "lr": "5.84211e-05", "gnorm": "2.512", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "74525"}
2022-12-16 16:12:35 | INFO | train_inner | {"epoch": 33, "update": 32.89, "loss": "0.183", "nll_loss": "0.183", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1722.4", "ups": "0.12", "wpb": "13956.1", "bsz": "32", "num_updates": "4460", "lr": "5.83158e-05", "gnorm": "2.852", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "74606"}
2022-12-16 16:13:55 | INFO | train_inner | {"epoch": 33, "update": 32.963, "loss": "0.181", "nll_loss": "0.181", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1772", "ups": "0.13", "wpb": "14111.5", "bsz": "32", "num_updates": "4470", "lr": "5.82105e-05", "gnorm": "2.025", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "74686"}
2022-12-16 16:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 16:15:22 | INFO | absl | Using default tokenizer.
2022-12-16 16:15:50 | INFO | absl | Using default tokenizer.
2022-12-16 16:16:15 | INFO | absl | Using default tokenizer.
2022-12-16 16:16:44 | INFO | absl | Using default tokenizer.
2022-12-16 16:17:09 | INFO | absl | Using default tokenizer.
2022-12-16 16:17:38 | INFO | absl | Using default tokenizer.
2022-12-16 16:18:06 | INFO | absl | Using default tokenizer.
2022-12-16 16:18:36 | INFO | absl | Using default tokenizer.
2022-12-16 16:19:08 | INFO | absl | Using default tokenizer.
2022-12-16 16:19:39 | INFO | absl | Using default tokenizer.
2022-12-16 16:20:07 | INFO | absl | Using default tokenizer.
2022-12-16 16:20:38 | INFO | absl | Using default tokenizer.
2022-12-16 16:21:10 | INFO | absl | Using default tokenizer.
2022-12-16 16:21:37 | INFO | absl | Using default tokenizer.
2022-12-16 16:22:22 | INFO | absl | Using default tokenizer.
2022-12-16 16:22:52 | INFO | absl | Using default tokenizer.
2022-12-16 16:23:20 | INFO | absl | Using default tokenizer.
2022-12-16 16:23:50 | INFO | absl | Using default tokenizer.
2022-12-16 16:24:19 | INFO | absl | Using default tokenizer.
2022-12-16 16:24:52 | INFO | absl | Using default tokenizer.
2022-12-16 16:25:22 | INFO | absl | Using default tokenizer.
2022-12-16 16:26:00 | INFO | absl | Using default tokenizer.
2022-12-16 16:26:32 | INFO | absl | Using default tokenizer.
2022-12-16 16:27:07 | INFO | absl | Using default tokenizer.
2022-12-16 16:27:39 | INFO | absl | Using default tokenizer.
2022-12-16 16:28:11 | INFO | absl | Using default tokenizer.
2022-12-16 16:28:49 | INFO | absl | Using default tokenizer.
2022-12-16 16:29:22 | INFO | absl | Using default tokenizer.
2022-12-16 16:29:55 | INFO | absl | Using default tokenizer.
2022-12-16 16:30:27 | INFO | absl | Using default tokenizer.
2022-12-16 16:30:59 | INFO | absl | Using default tokenizer.
2022-12-16 16:31:21 | INFO | valid | {"epoch": 33, "valid_loss": "5.809", "valid_nll_loss": "5.809", "valid_rouge1": 0.5041584578641916, "valid_rouge2": 0.14332735942536268, "valid_rougel": 0.2239585341892294, "valid_rouge_avg": 0.32374290864477706, "valid_ppl": "56.08", "valid_wps": "110.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4475", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 16:31:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 4475 updates
2022-12-16 16:31:21 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 16:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 16:31:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 33 @ 4475 updates, score 0.32374290864477706) (writing took 23.787770695984364 seconds)
2022-12-16 16:31:45 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-12-16 16:31:45 | INFO | train | {"epoch": 33, "train_loss": "0.177", "train_nll_loss": "0.177", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.13", "train_wps": "874.5", "train_ups": "0.06", "train_wpb": "13877.4", "train_bsz": "31.8", "train_num_updates": "4475", "train_lr": "5.81579e-05", "train_gnorm": "2.834", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1084", "train_gb_free": "62.2", "train_wall": "75756"}
2022-12-16 16:31:45 | INFO | fairseq.trainer | begin training epoch 34
2022-12-16 16:31:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 16:33:04 | INFO | train_inner | {"epoch": 34, "update": 33.037, "loss": "0.168", "nll_loss": "0.168", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "112.8", "ups": "0.01", "wpb": "12964.1", "bsz": "29.6", "num_updates": "4480", "lr": "5.81053e-05", "gnorm": "2.312", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "75836"}
2022-12-16 16:34:24 | INFO | train_inner | {"epoch": 34, "update": 33.11, "loss": "0.156", "nll_loss": "0.156", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1750.4", "ups": "0.13", "wpb": "13975.1", "bsz": "32", "num_updates": "4490", "lr": "5.8e-05", "gnorm": "17.342", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "75915"}
2022-12-16 16:35:45 | INFO | train_inner | {"epoch": 34, "update": 33.184, "loss": "0.16", "nll_loss": "0.16", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1725.8", "ups": "0.12", "wpb": "13870.7", "bsz": "32", "num_updates": "4500", "lr": "5.78947e-05", "gnorm": "2.023", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "75996"}
2022-12-16 16:37:06 | INFO | train_inner | {"epoch": 34, "update": 33.257, "loss": "0.165", "nll_loss": "0.165", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1700.6", "ups": "0.12", "wpb": "13833.1", "bsz": "32", "num_updates": "4510", "lr": "5.77895e-05", "gnorm": "2.207", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "76077"}
2022-12-16 16:38:27 | INFO | train_inner | {"epoch": 34, "update": 33.331, "loss": "0.157", "nll_loss": "0.157", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1759.8", "ups": "0.12", "wpb": "14185.5", "bsz": "32", "num_updates": "4520", "lr": "5.76842e-05", "gnorm": "4.226", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "76158"}
2022-12-16 16:39:48 | INFO | train_inner | {"epoch": 34, "update": 33.404, "loss": "0.163", "nll_loss": "0.163", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1722.4", "ups": "0.12", "wpb": "14031.9", "bsz": "32", "num_updates": "4530", "lr": "5.75789e-05", "gnorm": "3.623", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "76239"}
2022-12-16 16:41:09 | INFO | train_inner | {"epoch": 34, "update": 33.478, "loss": "0.166", "nll_loss": "0.166", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1756.7", "ups": "0.12", "wpb": "14117.9", "bsz": "32", "num_updates": "4540", "lr": "5.74737e-05", "gnorm": "2.545", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "76320"}
2022-12-16 16:42:30 | INFO | train_inner | {"epoch": 34, "update": 33.551, "loss": "0.157", "nll_loss": "0.157", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1717.4", "ups": "0.12", "wpb": "13896.2", "bsz": "32", "num_updates": "4550", "lr": "5.73684e-05", "gnorm": "1.427", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "76401"}
2022-12-16 16:43:49 | INFO | train_inner | {"epoch": 34, "update": 33.625, "loss": "0.171", "nll_loss": "0.171", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1763.2", "ups": "0.13", "wpb": "13914.9", "bsz": "32", "num_updates": "4560", "lr": "5.72632e-05", "gnorm": "4.771", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "76480"}
2022-12-16 16:45:09 | INFO | train_inner | {"epoch": 34, "update": 33.699, "loss": "0.165", "nll_loss": "0.165", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1725.2", "ups": "0.12", "wpb": "13928.7", "bsz": "32", "num_updates": "4570", "lr": "5.71579e-05", "gnorm": "1.807", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "76560"}
2022-12-16 16:46:28 | INFO | train_inner | {"epoch": 34, "update": 33.772, "loss": "0.176", "nll_loss": "0.176", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1767.7", "ups": "0.13", "wpb": "13881.1", "bsz": "32", "num_updates": "4580", "lr": "5.70526e-05", "gnorm": "2.301", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "76639"}
2022-12-16 16:47:48 | INFO | train_inner | {"epoch": 34, "update": 33.846, "loss": "0.174", "nll_loss": "0.174", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1747.6", "ups": "0.12", "wpb": "14095.6", "bsz": "32", "num_updates": "4590", "lr": "5.69474e-05", "gnorm": "1.167", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "76720"}
2022-12-16 16:49:08 | INFO | train_inner | {"epoch": 34, "update": 33.919, "loss": "0.167", "nll_loss": "0.167", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.12", "wps": "1695.2", "ups": "0.13", "wpb": "13510.2", "bsz": "32", "num_updates": "4600", "lr": "5.68421e-05", "gnorm": "1.224", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "76799"}
2022-12-16 16:50:31 | INFO | train_inner | {"epoch": 34, "update": 33.993, "loss": "0.172", "nll_loss": "0.172", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.13", "wps": "1703.7", "ups": "0.12", "wpb": "14112.7", "bsz": "32", "num_updates": "4610", "lr": "5.67368e-05", "gnorm": "1.396", "clip": "100", "loss_scale": "0.125", "train_wall": "83", "gb_free": "62.2", "wall": "76882"}
2022-12-16 16:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 16:51:23 | INFO | absl | Using default tokenizer.
2022-12-16 16:51:54 | INFO | absl | Using default tokenizer.
2022-12-16 16:52:23 | INFO | absl | Using default tokenizer.
2022-12-16 16:52:54 | INFO | absl | Using default tokenizer.
2022-12-16 16:53:29 | INFO | absl | Using default tokenizer.
2022-12-16 16:53:58 | INFO | absl | Using default tokenizer.
2022-12-16 16:54:28 | INFO | absl | Using default tokenizer.
2022-12-16 16:55:01 | INFO | absl | Using default tokenizer.
2022-12-16 16:55:34 | INFO | absl | Using default tokenizer.
2022-12-16 16:56:02 | INFO | absl | Using default tokenizer.
2022-12-16 16:56:34 | INFO | absl | Using default tokenizer.
2022-12-16 16:57:09 | INFO | absl | Using default tokenizer.
2022-12-16 16:57:40 | INFO | absl | Using default tokenizer.
2022-12-16 16:58:11 | INFO | absl | Using default tokenizer.
2022-12-16 16:58:43 | INFO | absl | Using default tokenizer.
2022-12-16 16:59:16 | INFO | absl | Using default tokenizer.
2022-12-16 16:59:47 | INFO | absl | Using default tokenizer.
2022-12-16 17:00:21 | INFO | absl | Using default tokenizer.
2022-12-16 17:00:52 | INFO | absl | Using default tokenizer.
2022-12-16 17:01:27 | INFO | absl | Using default tokenizer.
2022-12-16 17:02:01 | INFO | absl | Using default tokenizer.
2022-12-16 17:02:39 | INFO | absl | Using default tokenizer.
2022-12-16 17:03:09 | INFO | absl | Using default tokenizer.
2022-12-16 17:03:43 | INFO | absl | Using default tokenizer.
2022-12-16 17:04:19 | INFO | absl | Using default tokenizer.
2022-12-16 17:04:52 | INFO | absl | Using default tokenizer.
2022-12-16 17:05:25 | INFO | absl | Using default tokenizer.
2022-12-16 17:05:57 | INFO | absl | Using default tokenizer.
2022-12-16 17:06:35 | INFO | absl | Using default tokenizer.
2022-12-16 17:07:05 | INFO | absl | Using default tokenizer.
2022-12-16 17:07:43 | INFO | absl | Using default tokenizer.
2022-12-16 17:08:14 | INFO | valid | {"epoch": 34, "valid_loss": "5.87", "valid_nll_loss": "5.87", "valid_rouge1": 0.5070937854249, "valid_rouge2": 0.1437811184273739, "valid_rougel": 0.2249446040937498, "valid_rouge_avg": 0.32543745192613693, "valid_ppl": "58.5", "valid_wps": "105.8", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4611", "valid_best_rouge_avg": 0.32846726416623967}
2022-12-16 17:08:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 4611 updates
2022-12-16 17:08:15 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 17:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 17:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 34 @ 4611 updates, score 0.32543745192613693) (writing took 22.151685199001804 seconds)
2022-12-16 17:08:37 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-12-16 17:08:37 | INFO | train | {"epoch": 34, "train_loss": "0.165", "train_nll_loss": "0.165", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.12", "train_wps": "853.7", "train_ups": "0.06", "train_wpb": "13882", "train_bsz": "31.8", "train_num_updates": "4611", "train_lr": "5.67263e-05", "train_gnorm": "3.509", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "77968"}
2022-12-16 17:08:37 | INFO | fairseq.trainer | begin training epoch 35
2022-12-16 17:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 17:10:27 | INFO | train_inner | {"epoch": 35, "update": 34.066, "loss": "0.14", "nll_loss": "0.14", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "107.1", "ups": "0.01", "wpb": "12806.1", "bsz": "29.6", "num_updates": "4620", "lr": "5.66316e-05", "gnorm": "1.896", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "78078"}
2022-12-16 17:11:47 | INFO | train_inner | {"epoch": 35, "update": 34.14, "loss": "0.148", "nll_loss": "0.148", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1754.4", "ups": "0.13", "wpb": "13955.6", "bsz": "32", "num_updates": "4630", "lr": "5.65263e-05", "gnorm": "1.764", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "78158"}
2022-12-16 17:13:08 | INFO | train_inner | {"epoch": 35, "update": 34.213, "loss": "0.144", "nll_loss": "0.144", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1701.7", "ups": "0.12", "wpb": "13779.9", "bsz": "32", "num_updates": "4640", "lr": "5.64211e-05", "gnorm": "1.26", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "63", "wall": "78239"}
2022-12-16 17:14:27 | INFO | train_inner | {"epoch": 35, "update": 34.287, "loss": "0.145", "nll_loss": "0.145", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1777", "ups": "0.13", "wpb": "14075.1", "bsz": "32", "num_updates": "4650", "lr": "5.63158e-05", "gnorm": "1.341", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "78318"}
2022-12-16 17:15:48 | INFO | train_inner | {"epoch": 35, "update": 34.36, "loss": "0.142", "nll_loss": "0.142", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1735.6", "ups": "0.12", "wpb": "14110.4", "bsz": "32", "num_updates": "4660", "lr": "5.62105e-05", "gnorm": "1.039", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "78399"}
2022-12-16 17:17:08 | INFO | train_inner | {"epoch": 35, "update": 34.434, "loss": "0.147", "nll_loss": "0.147", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1742.4", "ups": "0.12", "wpb": "13954.6", "bsz": "32", "num_updates": "4670", "lr": "5.61053e-05", "gnorm": "1.372", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "78479"}
2022-12-16 17:18:30 | INFO | train_inner | {"epoch": 35, "update": 34.507, "loss": "0.144", "nll_loss": "0.144", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1677.5", "ups": "0.12", "wpb": "13726.3", "bsz": "32", "num_updates": "4680", "lr": "5.6e-05", "gnorm": "1.117", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "78561"}
2022-12-16 17:19:50 | INFO | train_inner | {"epoch": 35, "update": 34.581, "loss": "0.145", "nll_loss": "0.145", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1744.5", "ups": "0.12", "wpb": "13972.2", "bsz": "32", "num_updates": "4690", "lr": "5.58947e-05", "gnorm": "1.146", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "78641"}
2022-12-16 17:21:11 | INFO | train_inner | {"epoch": 35, "update": 34.654, "loss": "0.147", "nll_loss": "0.147", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1725.8", "ups": "0.12", "wpb": "13957", "bsz": "32", "num_updates": "4700", "lr": "5.57895e-05", "gnorm": "1.948", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "78722"}
2022-12-16 17:22:32 | INFO | train_inner | {"epoch": 35, "update": 34.728, "loss": "0.148", "nll_loss": "0.148", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1773.4", "ups": "0.12", "wpb": "14344.5", "bsz": "32", "num_updates": "4710", "lr": "5.56842e-05", "gnorm": "1.02", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "78803"}
2022-12-16 17:23:53 | INFO | train_inner | {"epoch": 35, "update": 34.801, "loss": "0.154", "nll_loss": "0.154", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1729.3", "ups": "0.12", "wpb": "14031.3", "bsz": "32", "num_updates": "4720", "lr": "5.55789e-05", "gnorm": "1.243", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "78884"}
2022-12-16 17:25:14 | INFO | train_inner | {"epoch": 35, "update": 34.875, "loss": "0.15", "nll_loss": "0.15", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1684.6", "ups": "0.12", "wpb": "13605", "bsz": "32", "num_updates": "4730", "lr": "5.54737e-05", "gnorm": "1.408", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "78965"}
2022-12-16 17:26:35 | INFO | train_inner | {"epoch": 35, "update": 34.949, "loss": "0.151", "nll_loss": "0.151", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "1731.9", "ups": "0.12", "wpb": "14023.2", "bsz": "32", "num_updates": "4740", "lr": "5.53684e-05", "gnorm": "1.035", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.9", "wall": "79046"}
2022-12-16 17:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 17:28:16 | INFO | absl | Using default tokenizer.
2022-12-16 17:28:45 | INFO | absl | Using default tokenizer.
2022-12-16 17:29:24 | INFO | absl | Using default tokenizer.
2022-12-16 17:29:54 | INFO | absl | Using default tokenizer.
2022-12-16 17:30:22 | INFO | absl | Using default tokenizer.
2022-12-16 17:30:51 | INFO | absl | Using default tokenizer.
2022-12-16 17:31:18 | INFO | absl | Using default tokenizer.
2022-12-16 17:31:47 | INFO | absl | Using default tokenizer.
2022-12-16 17:32:22 | INFO | absl | Using default tokenizer.
2022-12-16 17:32:50 | INFO | absl | Using default tokenizer.
2022-12-16 17:33:22 | INFO | absl | Using default tokenizer.
2022-12-16 17:34:01 | INFO | absl | Using default tokenizer.
2022-12-16 17:34:40 | INFO | absl | Using default tokenizer.
2022-12-16 17:35:21 | INFO | absl | Using default tokenizer.
2022-12-16 17:35:53 | INFO | absl | Using default tokenizer.
2022-12-16 17:36:24 | INFO | absl | Using default tokenizer.
2022-12-16 17:36:58 | INFO | absl | Using default tokenizer.
2022-12-16 17:37:33 | INFO | absl | Using default tokenizer.
2022-12-16 17:38:09 | INFO | absl | Using default tokenizer.
2022-12-16 17:38:45 | INFO | absl | Using default tokenizer.
2022-12-16 17:39:19 | INFO | absl | Using default tokenizer.
2022-12-16 17:39:58 | INFO | absl | Using default tokenizer.
2022-12-16 17:40:30 | INFO | absl | Using default tokenizer.
2022-12-16 17:41:01 | INFO | absl | Using default tokenizer.
2022-12-16 17:41:36 | INFO | absl | Using default tokenizer.
2022-12-16 17:42:12 | INFO | absl | Using default tokenizer.
2022-12-16 17:42:44 | INFO | absl | Using default tokenizer.
2022-12-16 17:43:21 | INFO | absl | Using default tokenizer.
2022-12-16 17:43:58 | INFO | absl | Using default tokenizer.
2022-12-16 17:44:35 | INFO | absl | Using default tokenizer.
2022-12-16 17:45:16 | INFO | absl | Using default tokenizer.
2022-12-16 17:45:48 | INFO | valid | {"epoch": 35, "valid_loss": "5.92", "valid_nll_loss": "5.92", "valid_rouge1": 0.5109985473373481, "valid_rouge2": 0.14617373485762436, "valid_rougel": 0.2257227774357937, "valid_rouge_avg": 0.32858614109748624, "valid_ppl": "60.57", "valid_wps": "101.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4747", "valid_best_rouge_avg": 0.32858614109748624}
2022-12-16 17:45:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 4747 updates
2022-12-16 17:45:48 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 17:46:10 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 17:52:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 35 @ 4747 updates, score 0.32858614109748624) (writing took 417.4789509179536 seconds)
2022-12-16 17:52:45 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-12-16 17:52:45 | INFO | train | {"epoch": 35, "train_loss": "0.147", "train_nll_loss": "0.147", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.11", "train_wps": "712.6", "train_ups": "0.05", "train_wpb": "13879.1", "train_bsz": "31.8", "train_num_updates": "4747", "train_lr": "5.52947e-05", "train_gnorm": "1.348", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1090", "train_gb_free": "62.2", "train_wall": "80617"}
2022-12-16 17:52:45 | INFO | fairseq.trainer | begin training epoch 36
2022-12-16 17:52:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 17:53:41 | INFO | train_inner | {"epoch": 36, "update": 35.022, "loss": "0.145", "nll_loss": "0.145", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.11", "wps": "80.2", "ups": "0.01", "wpb": "13031.4", "bsz": "29.6", "num_updates": "4750", "lr": "5.52632e-05", "gnorm": "1.633", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "80672"}
2022-12-16 17:55:02 | INFO | train_inner | {"epoch": 36, "update": 35.096, "loss": "0.123", "nll_loss": "0.123", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1694.6", "ups": "0.12", "wpb": "13844.4", "bsz": "32", "num_updates": "4760", "lr": "5.51579e-05", "gnorm": "0.931", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "80754"}
2022-12-16 17:56:23 | INFO | train_inner | {"epoch": 36, "update": 35.169, "loss": "0.128", "nll_loss": "0.128", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1721", "ups": "0.12", "wpb": "13911.5", "bsz": "32", "num_updates": "4770", "lr": "5.50526e-05", "gnorm": "1.096", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "80834"}
2022-12-16 17:57:44 | INFO | train_inner | {"epoch": 36, "update": 35.243, "loss": "0.129", "nll_loss": "0.129", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1743.8", "ups": "0.12", "wpb": "14001.2", "bsz": "32", "num_updates": "4780", "lr": "5.49474e-05", "gnorm": "1.788", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "80915"}
2022-12-16 17:59:04 | INFO | train_inner | {"epoch": 36, "update": 35.316, "loss": "0.134", "nll_loss": "0.134", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1739.6", "ups": "0.13", "wpb": "13909.2", "bsz": "32", "num_updates": "4790", "lr": "5.48421e-05", "gnorm": "1.76", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "80995"}
2022-12-16 18:00:24 | INFO | train_inner | {"epoch": 36, "update": 35.39, "loss": "0.132", "nll_loss": "0.132", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1707.7", "ups": "0.12", "wpb": "13801.3", "bsz": "32", "num_updates": "4800", "lr": "5.47368e-05", "gnorm": "0.986", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "81075"}
2022-12-16 18:01:46 | INFO | train_inner | {"epoch": 36, "update": 35.463, "loss": "0.134", "nll_loss": "0.134", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1737.6", "ups": "0.12", "wpb": "14150.8", "bsz": "32", "num_updates": "4810", "lr": "5.46316e-05", "gnorm": "0.955", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "81157"}
2022-12-16 18:03:05 | INFO | train_inner | {"epoch": 36, "update": 35.537, "loss": "0.131", "nll_loss": "0.131", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1781.2", "ups": "0.13", "wpb": "14144.9", "bsz": "32", "num_updates": "4820", "lr": "5.45263e-05", "gnorm": "0.955", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "81236"}
2022-12-16 18:04:26 | INFO | train_inner | {"epoch": 36, "update": 35.61, "loss": "0.134", "nll_loss": "0.134", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1731", "ups": "0.12", "wpb": "13883.9", "bsz": "32", "num_updates": "4830", "lr": "5.44211e-05", "gnorm": "1.464", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "81317"}
2022-12-16 18:05:46 | INFO | train_inner | {"epoch": 36, "update": 35.684, "loss": "0.138", "nll_loss": "0.138", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1717.2", "ups": "0.12", "wpb": "13824.5", "bsz": "32", "num_updates": "4840", "lr": "5.43158e-05", "gnorm": "0.974", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "81397"}
2022-12-16 18:07:06 | INFO | train_inner | {"epoch": 36, "update": 35.757, "loss": "0.136", "nll_loss": "0.136", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1764.9", "ups": "0.13", "wpb": "14062.7", "bsz": "32", "num_updates": "4850", "lr": "5.42105e-05", "gnorm": "1.412", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "81477"}
2022-12-16 18:08:25 | INFO | train_inner | {"epoch": 36, "update": 35.831, "loss": "0.136", "nll_loss": "0.136", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1761.3", "ups": "0.13", "wpb": "13924.8", "bsz": "32", "num_updates": "4860", "lr": "5.41053e-05", "gnorm": "1.13", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "81556"}
2022-12-16 18:09:45 | INFO | train_inner | {"epoch": 36, "update": 35.904, "loss": "0.134", "nll_loss": "0.134", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1719.3", "ups": "0.13", "wpb": "13707.9", "bsz": "32", "num_updates": "4870", "lr": "5.4e-05", "gnorm": "1.058", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "81636"}
2022-12-16 18:11:04 | INFO | train_inner | {"epoch": 36, "update": 35.978, "loss": "0.137", "nll_loss": "0.137", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.1", "wps": "1787.8", "ups": "0.13", "wpb": "14183.2", "bsz": "32", "num_updates": "4880", "lr": "5.38947e-05", "gnorm": "0.985", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "81715"}
2022-12-16 18:11:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 18:12:13 | INFO | absl | Using default tokenizer.
2022-12-16 18:12:46 | INFO | absl | Using default tokenizer.
2022-12-16 18:13:16 | INFO | absl | Using default tokenizer.
2022-12-16 18:13:48 | INFO | absl | Using default tokenizer.
2022-12-16 18:14:14 | INFO | absl | Using default tokenizer.
2022-12-16 18:14:44 | INFO | absl | Using default tokenizer.
2022-12-16 18:15:17 | INFO | absl | Using default tokenizer.
2022-12-16 18:15:50 | INFO | absl | Using default tokenizer.
2022-12-16 18:16:26 | INFO | absl | Using default tokenizer.
2022-12-16 18:17:00 | INFO | absl | Using default tokenizer.
2022-12-16 18:17:34 | INFO | absl | Using default tokenizer.
2022-12-16 18:18:09 | INFO | absl | Using default tokenizer.
2022-12-16 18:18:39 | INFO | absl | Using default tokenizer.
2022-12-16 18:19:13 | INFO | absl | Using default tokenizer.
2022-12-16 18:19:44 | INFO | absl | Using default tokenizer.
2022-12-16 18:20:18 | INFO | absl | Using default tokenizer.
2022-12-16 18:20:50 | INFO | absl | Using default tokenizer.
2022-12-16 18:21:22 | INFO | absl | Using default tokenizer.
2022-12-16 18:21:55 | INFO | absl | Using default tokenizer.
2022-12-16 18:22:29 | INFO | absl | Using default tokenizer.
2022-12-16 18:22:57 | INFO | absl | Using default tokenizer.
2022-12-16 18:23:38 | INFO | absl | Using default tokenizer.
2022-12-16 18:24:13 | INFO | absl | Using default tokenizer.
2022-12-16 18:24:44 | INFO | absl | Using default tokenizer.
2022-12-16 18:25:20 | INFO | absl | Using default tokenizer.
2022-12-16 18:25:52 | INFO | absl | Using default tokenizer.
2022-12-16 18:26:29 | INFO | absl | Using default tokenizer.
2022-12-16 18:27:03 | INFO | absl | Using default tokenizer.
2022-12-16 18:27:39 | INFO | absl | Using default tokenizer.
2022-12-16 18:28:21 | INFO | absl | Using default tokenizer.
2022-12-16 18:29:06 | INFO | absl | Using default tokenizer.
2022-12-16 18:29:36 | INFO | valid | {"epoch": 36, "valid_loss": "6.017", "valid_nll_loss": "6.017", "valid_rouge1": 0.5106962069583929, "valid_rouge2": 0.14390101805961228, "valid_rougel": 0.22468751158264783, "valid_rouge_avg": 0.3272986125090026, "valid_ppl": "64.78", "valid_wps": "102.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "4883", "valid_best_rouge_avg": 0.32858614109748624}
2022-12-16 18:29:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 4883 updates
2022-12-16 18:29:36 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 18:30:01 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 18:30:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 36 @ 4883 updates, score 0.3272986125090026) (writing took 24.731100564124063 seconds)
2022-12-16 18:30:01 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-12-16 18:30:01 | INFO | train | {"epoch": 36, "train_loss": "0.133", "train_nll_loss": "0.133", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.1", "train_wps": "844.3", "train_ups": "0.06", "train_wpb": "13879.8", "train_bsz": "31.8", "train_num_updates": "4883", "train_lr": "5.38632e-05", "train_gnorm": "1.219", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1087", "train_gb_free": "63.6", "train_wall": "82852"}
2022-12-16 18:30:01 | INFO | fairseq.trainer | begin training epoch 37
2022-12-16 18:30:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 18:31:37 | INFO | train_inner | {"epoch": 37, "update": 36.051, "loss": "0.119", "nll_loss": "0.119", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "103.1", "ups": "0.01", "wpb": "12709.5", "bsz": "29.6", "num_updates": "4890", "lr": "5.37895e-05", "gnorm": "1.574", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "82948"}
2022-12-16 18:32:58 | INFO | train_inner | {"epoch": 37, "update": 36.125, "loss": "0.115", "nll_loss": "0.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1718", "ups": "0.12", "wpb": "13786", "bsz": "32", "num_updates": "4900", "lr": "5.36842e-05", "gnorm": "1.257", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "63.6", "wall": "83029"}
2022-12-16 18:34:17 | INFO | train_inner | {"epoch": 37, "update": 36.199, "loss": "0.115", "nll_loss": "0.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1721", "ups": "0.13", "wpb": "13730.6", "bsz": "32", "num_updates": "4910", "lr": "5.35789e-05", "gnorm": "1.302", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "83108"}
2022-12-16 18:35:38 | INFO | train_inner | {"epoch": 37, "update": 36.272, "loss": "0.122", "nll_loss": "0.122", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1740", "ups": "0.12", "wpb": "14056.7", "bsz": "32", "num_updates": "4920", "lr": "5.34737e-05", "gnorm": "2.446", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83189"}
2022-12-16 18:36:59 | INFO | train_inner | {"epoch": 37, "update": 36.346, "loss": "0.11", "nll_loss": "0.11", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1756.1", "ups": "0.12", "wpb": "14121.2", "bsz": "32", "num_updates": "4930", "lr": "5.33684e-05", "gnorm": "1.657", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83270"}
2022-12-16 18:38:19 | INFO | train_inner | {"epoch": 37, "update": 36.419, "loss": "0.11", "nll_loss": "0.11", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1740.4", "ups": "0.12", "wpb": "14048.1", "bsz": "32", "num_updates": "4940", "lr": "5.32632e-05", "gnorm": "1.151", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83350"}
2022-12-16 18:39:37 | INFO | train_inner | {"epoch": 37, "update": 36.493, "loss": "0.113", "nll_loss": "0.113", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1807", "ups": "0.13", "wpb": "13997.8", "bsz": "32", "num_updates": "4950", "lr": "5.31579e-05", "gnorm": "4.365", "clip": "100", "loss_scale": "0.125", "train_wall": "77", "gb_free": "62.2", "wall": "83428"}
2022-12-16 18:40:59 | INFO | train_inner | {"epoch": 37, "update": 36.566, "loss": "0.113", "nll_loss": "0.113", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1696.3", "ups": "0.12", "wpb": "13839.9", "bsz": "32", "num_updates": "4960", "lr": "5.30526e-05", "gnorm": "1.199", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "83510"}
2022-12-16 18:42:20 | INFO | train_inner | {"epoch": 37, "update": 36.64, "loss": "0.122", "nll_loss": "0.122", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1714.3", "ups": "0.12", "wpb": "13887.3", "bsz": "32", "num_updates": "4970", "lr": "5.29474e-05", "gnorm": "1.475", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "83591"}
2022-12-16 18:43:40 | INFO | train_inner | {"epoch": 37, "update": 36.713, "loss": "0.118", "nll_loss": "0.118", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1738", "ups": "0.12", "wpb": "14023.2", "bsz": "32", "num_updates": "4980", "lr": "5.28421e-05", "gnorm": "1.359", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83671"}
2022-12-16 18:45:00 | INFO | train_inner | {"epoch": 37, "update": 36.787, "loss": "0.124", "nll_loss": "0.124", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1785.9", "ups": "0.13", "wpb": "14171.4", "bsz": "32", "num_updates": "4990", "lr": "5.27368e-05", "gnorm": "1.437", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "64.3", "wall": "83751"}
2022-12-16 18:46:20 | INFO | train_inner | {"epoch": 37, "update": 36.86, "loss": "0.12", "nll_loss": "0.12", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1729.2", "ups": "0.13", "wpb": "13816.1", "bsz": "32", "num_updates": "5000", "lr": "5.26316e-05", "gnorm": "1.409", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83831"}
2022-12-16 18:47:40 | INFO | train_inner | {"epoch": 37, "update": 36.934, "loss": "0.119", "nll_loss": "0.119", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1756.8", "ups": "0.12", "wpb": "14179.7", "bsz": "32", "num_updates": "5010", "lr": "5.25263e-05", "gnorm": "1.227", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "83911"}
2022-12-16 18:48:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/home/acp20tg/.conda/envs/bart-ls/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-12-16 18:49:39 | INFO | absl | Using default tokenizer.
2022-12-16 18:50:07 | INFO | absl | Using default tokenizer.
2022-12-16 18:50:34 | INFO | absl | Using default tokenizer.
2022-12-16 18:51:06 | INFO | absl | Using default tokenizer.
2022-12-16 18:51:37 | INFO | absl | Using default tokenizer.
2022-12-16 18:52:09 | INFO | absl | Using default tokenizer.
2022-12-16 18:52:38 | INFO | absl | Using default tokenizer.
2022-12-16 18:53:09 | INFO | absl | Using default tokenizer.
2022-12-16 18:53:43 | INFO | absl | Using default tokenizer.
2022-12-16 18:54:16 | INFO | absl | Using default tokenizer.
2022-12-16 18:54:49 | INFO | absl | Using default tokenizer.
2022-12-16 18:55:23 | INFO | absl | Using default tokenizer.
2022-12-16 18:55:53 | INFO | absl | Using default tokenizer.
2022-12-16 18:56:23 | INFO | absl | Using default tokenizer.
2022-12-16 18:56:54 | INFO | absl | Using default tokenizer.
2022-12-16 18:57:30 | INFO | absl | Using default tokenizer.
2022-12-16 18:58:04 | INFO | absl | Using default tokenizer.
2022-12-16 18:58:35 | INFO | absl | Using default tokenizer.
2022-12-16 18:59:05 | INFO | absl | Using default tokenizer.
2022-12-16 18:59:44 | INFO | absl | Using default tokenizer.
2022-12-16 19:00:18 | INFO | absl | Using default tokenizer.
2022-12-16 19:00:56 | INFO | absl | Using default tokenizer.
2022-12-16 19:01:27 | INFO | absl | Using default tokenizer.
2022-12-16 19:02:04 | INFO | absl | Using default tokenizer.
2022-12-16 19:02:39 | INFO | absl | Using default tokenizer.
2022-12-16 19:03:16 | INFO | absl | Using default tokenizer.
2022-12-16 19:03:50 | INFO | absl | Using default tokenizer.
2022-12-16 19:04:31 | INFO | absl | Using default tokenizer.
2022-12-16 19:05:12 | INFO | absl | Using default tokenizer.
2022-12-16 19:05:43 | INFO | absl | Using default tokenizer.
2022-12-16 19:06:15 | INFO | absl | Using default tokenizer.
2022-12-16 19:06:38 | INFO | valid | {"epoch": 37, "valid_loss": "6.064", "valid_nll_loss": "6.064", "valid_rouge1": 0.5091175615898227, "valid_rouge2": 0.1462617040507614, "valid_rougel": 0.22653233268749745, "valid_rouge_avg": 0.327689632820292, "valid_ppl": "66.89", "valid_wps": "104.2", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5019", "valid_best_rouge_avg": 0.32858614109748624}
2022-12-16 19:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 5019 updates
2022-12-16 19:06:38 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 19:07:00 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 19:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 37 @ 5019 updates, score 0.327689632820292) (writing took 22.332107477122918 seconds)
2022-12-16 19:07:00 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-12-16 19:07:00 | INFO | train | {"epoch": 37, "train_loss": "0.117", "train_nll_loss": "0.117", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.08", "train_wps": "850.4", "train_ups": "0.06", "train_wpb": "13876.7", "train_bsz": "31.8", "train_num_updates": "5019", "train_lr": "5.24316e-05", "train_gnorm": "1.727", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "85071"}
2022-12-16 19:07:00 | INFO | fairseq.trainer | begin training epoch 38
2022-12-16 19:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 19:07:47 | INFO | train_inner | {"epoch": 38, "update": 37.007, "loss": "0.123", "nll_loss": "0.123", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "107.2", "ups": "0.01", "wpb": "12935.6", "bsz": "29.6", "num_updates": "5020", "lr": "5.24211e-05", "gnorm": "2.305", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "85118"}
2022-12-16 19:09:08 | INFO | train_inner | {"epoch": 38, "update": 37.081, "loss": "0.1", "nll_loss": "0.1", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1721.1", "ups": "0.12", "wpb": "13886.5", "bsz": "32", "num_updates": "5030", "lr": "5.23158e-05", "gnorm": "1.495", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "85199"}
2022-12-16 19:10:29 | INFO | train_inner | {"epoch": 38, "update": 37.154, "loss": "0.103", "nll_loss": "0.103", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1719.8", "ups": "0.12", "wpb": "13928.8", "bsz": "32", "num_updates": "5040", "lr": "5.22105e-05", "gnorm": "1.19", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "85280"}
2022-12-16 19:11:51 | INFO | train_inner | {"epoch": 38, "update": 37.228, "loss": "0.108", "nll_loss": "0.108", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1694.5", "ups": "0.12", "wpb": "13944.7", "bsz": "32", "num_updates": "5050", "lr": "5.21053e-05", "gnorm": "1.184", "clip": "100", "loss_scale": "0.125", "train_wall": "82", "gb_free": "62.2", "wall": "85362"}
2022-12-16 19:13:13 | INFO | train_inner | {"epoch": 38, "update": 37.301, "loss": "0.107", "nll_loss": "0.107", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1749.3", "ups": "0.12", "wpb": "14265.8", "bsz": "32", "num_updates": "5060", "lr": "5.2e-05", "gnorm": "1.228", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "85444"}
2022-12-16 19:14:33 | INFO | train_inner | {"epoch": 38, "update": 37.375, "loss": "0.116", "nll_loss": "0.116", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1703.6", "ups": "0.12", "wpb": "13745.1", "bsz": "32", "num_updates": "5070", "lr": "5.18947e-05", "gnorm": "1.11", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "85524"}
2022-12-16 19:15:52 | INFO | train_inner | {"epoch": 38, "update": 37.449, "loss": "0.115", "nll_loss": "0.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1764.1", "ups": "0.13", "wpb": "13962.1", "bsz": "32", "num_updates": "5080", "lr": "5.17895e-05", "gnorm": "1.656", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "85604"}
2022-12-16 19:17:13 | INFO | train_inner | {"epoch": 38, "update": 37.522, "loss": "0.115", "nll_loss": "0.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1731.5", "ups": "0.12", "wpb": "13863.9", "bsz": "32", "num_updates": "5090", "lr": "5.16842e-05", "gnorm": "1.357", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "85684"}
2022-12-16 19:18:33 | INFO | train_inner | {"epoch": 38, "update": 37.596, "loss": "0.113", "nll_loss": "0.113", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1718.4", "ups": "0.12", "wpb": "13811.8", "bsz": "32", "num_updates": "5100", "lr": "5.15789e-05", "gnorm": "1.234", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "85764"}
2022-12-16 19:19:53 | INFO | train_inner | {"epoch": 38, "update": 37.669, "loss": "0.12", "nll_loss": "0.12", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.09", "wps": "1754.1", "ups": "0.13", "wpb": "13997.1", "bsz": "32", "num_updates": "5110", "lr": "5.14737e-05", "gnorm": "4.793", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "85844"}
2022-12-16 19:21:13 | INFO | train_inner | {"epoch": 38, "update": 37.743, "loss": "0.115", "nll_loss": "0.115", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1786.2", "ups": "0.12", "wpb": "14364.3", "bsz": "32", "num_updates": "5120", "lr": "5.13684e-05", "gnorm": "1.38", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "85924"}
2022-12-16 19:22:33 | INFO | train_inner | {"epoch": 38, "update": 37.816, "loss": "0.117", "nll_loss": "0.117", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1712.8", "ups": "0.13", "wpb": "13634", "bsz": "32", "num_updates": "5130", "lr": "5.12632e-05", "gnorm": "1.409", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "63.6", "wall": "86004"}
2022-12-16 19:23:52 | INFO | train_inner | {"epoch": 38, "update": 37.89, "loss": "0.117", "nll_loss": "0.117", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1764.7", "ups": "0.13", "wpb": "14036.8", "bsz": "32", "num_updates": "5140", "lr": "5.11579e-05", "gnorm": "4.825", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "86083"}
2022-12-16 19:25:12 | INFO | train_inner | {"epoch": 38, "update": 37.963, "loss": "0.114", "nll_loss": "0.114", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1761.6", "ups": "0.13", "wpb": "14057.1", "bsz": "32", "num_updates": "5150", "lr": "5.10526e-05", "gnorm": "1.25", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "86163"}
2022-12-16 19:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 19:26:39 | INFO | absl | Using default tokenizer.
2022-12-16 19:27:09 | INFO | absl | Using default tokenizer.
2022-12-16 19:27:38 | INFO | absl | Using default tokenizer.
2022-12-16 19:28:09 | INFO | absl | Using default tokenizer.
2022-12-16 19:28:37 | INFO | absl | Using default tokenizer.
2022-12-16 19:29:06 | INFO | absl | Using default tokenizer.
2022-12-16 19:29:38 | INFO | absl | Using default tokenizer.
2022-12-16 19:30:08 | INFO | absl | Using default tokenizer.
2022-12-16 19:30:37 | INFO | absl | Using default tokenizer.
2022-12-16 19:31:09 | INFO | absl | Using default tokenizer.
2022-12-16 19:31:37 | INFO | absl | Using default tokenizer.
2022-12-16 19:32:08 | INFO | absl | Using default tokenizer.
2022-12-16 19:32:42 | INFO | absl | Using default tokenizer.
2022-12-16 19:33:10 | INFO | absl | Using default tokenizer.
2022-12-16 19:33:41 | INFO | absl | Using default tokenizer.
2022-12-16 19:34:11 | INFO | absl | Using default tokenizer.
2022-12-16 19:34:43 | INFO | absl | Using default tokenizer.
2022-12-16 19:35:17 | INFO | absl | Using default tokenizer.
2022-12-16 19:35:52 | INFO | absl | Using default tokenizer.
2022-12-16 19:36:23 | INFO | absl | Using default tokenizer.
2022-12-16 19:36:55 | INFO | absl | Using default tokenizer.
2022-12-16 19:37:32 | INFO | absl | Using default tokenizer.
2022-12-16 19:38:05 | INFO | absl | Using default tokenizer.
2022-12-16 19:38:35 | INFO | absl | Using default tokenizer.
2022-12-16 19:39:08 | INFO | absl | Using default tokenizer.
2022-12-16 19:39:42 | INFO | absl | Using default tokenizer.
2022-12-16 19:40:27 | INFO | absl | Using default tokenizer.
2022-12-16 19:41:00 | INFO | absl | Using default tokenizer.
2022-12-16 19:41:43 | INFO | absl | Using default tokenizer.
2022-12-16 19:42:15 | INFO | absl | Using default tokenizer.
2022-12-16 19:42:48 | INFO | absl | Using default tokenizer.
2022-12-16 19:43:18 | INFO | valid | {"epoch": 38, "valid_loss": "6.112", "valid_nll_loss": "6.112", "valid_rouge1": 0.5068592328541535, "valid_rouge2": 0.14601537895854244, "valid_rougel": 0.2266462683815353, "valid_rouge_avg": 0.32643730590634806, "valid_ppl": "69.16", "valid_wps": "107.1", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5155", "valid_best_rouge_avg": 0.32858614109748624}
2022-12-16 19:43:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 5155 updates
2022-12-16 19:43:18 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 19:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 19:43:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 38 @ 5155 updates, score 0.32643730590634806) (writing took 22.38685366907157 seconds)
2022-12-16 19:43:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-12-16 19:43:40 | INFO | train | {"epoch": 38, "train_loss": "0.112", "train_nll_loss": "0.112", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.08", "train_wps": "858", "train_ups": "0.06", "train_wpb": "13881.4", "train_bsz": "31.8", "train_num_updates": "5155", "train_lr": "5.1e-05", "train_gnorm": "1.851", "train_clip": "100", "train_loss_scale": "0.125", "train_train_wall": "1089", "train_gb_free": "62.2", "train_wall": "87272"}
2022-12-16 19:43:41 | INFO | fairseq.trainer | begin training epoch 39
2022-12-16 19:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 19:44:58 | INFO | train_inner | {"epoch": 39, "update": 38.037, "loss": "0.104", "nll_loss": "0.104", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "107.8", "ups": "0.01", "wpb": "12783.5", "bsz": "29.6", "num_updates": "5160", "lr": "5.09474e-05", "gnorm": "1.451", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "87349"}
2022-12-16 19:46:19 | INFO | train_inner | {"epoch": 39, "update": 38.11, "loss": "0.113", "nll_loss": "0.113", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1735.9", "ups": "0.12", "wpb": "14066.5", "bsz": "32", "num_updates": "5170", "lr": "5.08421e-05", "gnorm": "2.065", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "87430"}
2022-12-16 19:47:39 | INFO | train_inner | {"epoch": 39, "update": 38.184, "loss": "0.101", "nll_loss": "0.101", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1798.7", "ups": "0.12", "wpb": "14391.4", "bsz": "32", "num_updates": "5180", "lr": "5.07368e-05", "gnorm": "1.11", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "87510"}
2022-12-16 19:48:58 | INFO | train_inner | {"epoch": 39, "update": 38.257, "loss": "0.104", "nll_loss": "0.104", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1748.3", "ups": "0.13", "wpb": "13720.3", "bsz": "32", "num_updates": "5190", "lr": "5.06316e-05", "gnorm": "2.087", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.9", "wall": "87589"}
2022-12-16 19:50:16 | INFO | train_inner | {"epoch": 39, "update": 38.331, "loss": "0.107", "nll_loss": "0.107", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1788.7", "ups": "0.13", "wpb": "13956.3", "bsz": "32", "num_updates": "5200", "lr": "5.05263e-05", "gnorm": "1.945", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "87667"}
2022-12-16 19:51:37 | INFO | train_inner | {"epoch": 39, "update": 38.404, "loss": "0.114", "nll_loss": "0.114", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1684.5", "ups": "0.12", "wpb": "13707.8", "bsz": "32", "num_updates": "5210", "lr": "5.04211e-05", "gnorm": "1.561", "clip": "100", "loss_scale": "0.125", "train_wall": "81", "gb_free": "62.2", "wall": "87748"}
2022-12-16 19:52:58 | INFO | train_inner | {"epoch": 39, "update": 38.478, "loss": "0.106", "nll_loss": "0.106", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1732.3", "ups": "0.12", "wpb": "14002.4", "bsz": "32", "num_updates": "5220", "lr": "5.03158e-05", "gnorm": "1.257", "clip": "100", "loss_scale": "0.125", "train_wall": "80", "gb_free": "62.2", "wall": "87829"}
2022-12-16 19:54:18 | INFO | train_inner | {"epoch": 39, "update": 38.551, "loss": "0.106", "nll_loss": "0.106", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1748.1", "ups": "0.13", "wpb": "13953.9", "bsz": "32", "num_updates": "5230", "lr": "5.02105e-05", "gnorm": "1.194", "clip": "100", "loss_scale": "0.125", "train_wall": "79", "gb_free": "62.2", "wall": "87909"}
2022-12-16 19:55:36 | INFO | train_inner | {"epoch": 39, "update": 38.625, "loss": "0.105", "nll_loss": "0.105", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1755.9", "ups": "0.13", "wpb": "13676.9", "bsz": "32", "num_updates": "5240", "lr": "5.01053e-05", "gnorm": "0.936", "clip": "100", "loss_scale": "0.125", "train_wall": "78", "gb_free": "62.2", "wall": "87987"}
2022-12-16 19:56:56 | INFO | train_inner | {"epoch": 39, "update": 38.699, "loss": "0.112", "nll_loss": "0.112", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1713.7", "ups": "0.13", "wpb": "13706", "bsz": "32", "num_updates": "5250", "lr": "5e-05", "gnorm": "1.557", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "88067"}
2022-12-16 19:58:17 | INFO | train_inner | {"epoch": 39, "update": 38.772, "loss": "0.109", "nll_loss": "0.109", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1761.5", "ups": "0.12", "wpb": "14232.1", "bsz": "32", "num_updates": "5260", "lr": "4.98947e-05", "gnorm": "1.387", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "88148"}
2022-12-16 19:59:36 | INFO | train_inner | {"epoch": 39, "update": 38.846, "loss": "0.109", "nll_loss": "0.109", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1762.6", "ups": "0.13", "wpb": "14034.5", "bsz": "32", "num_updates": "5270", "lr": "4.97895e-05", "gnorm": "0.904", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "88227"}
2022-12-16 20:00:57 | INFO | train_inner | {"epoch": 39, "update": 38.919, "loss": "0.112", "nll_loss": "0.112", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1752.1", "ups": "0.12", "wpb": "14164.5", "bsz": "32", "num_updates": "5280", "lr": "4.96842e-05", "gnorm": "1.077", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "88308"}
2022-12-16 20:02:21 | INFO | train_inner | {"epoch": 39, "update": 38.993, "loss": "0.112", "nll_loss": "0.112", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.08", "wps": "1658.2", "ups": "0.12", "wpb": "13864.9", "bsz": "32", "num_updates": "5290", "lr": "4.95789e-05", "gnorm": "0.931", "clip": "100", "loss_scale": "0.25", "train_wall": "83", "gb_free": "62.2", "wall": "88392"}
2022-12-16 20:02:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 20:03:14 | INFO | absl | Using default tokenizer.
2022-12-16 20:03:44 | INFO | absl | Using default tokenizer.
2022-12-16 20:04:16 | INFO | absl | Using default tokenizer.
2022-12-16 20:04:42 | INFO | absl | Using default tokenizer.
2022-12-16 20:05:08 | INFO | absl | Using default tokenizer.
2022-12-16 20:05:41 | INFO | absl | Using default tokenizer.
2022-12-16 20:06:10 | INFO | absl | Using default tokenizer.
2022-12-16 20:06:40 | INFO | absl | Using default tokenizer.
2022-12-16 20:07:16 | INFO | absl | Using default tokenizer.
2022-12-16 20:07:46 | INFO | absl | Using default tokenizer.
2022-12-16 20:08:19 | INFO | absl | Using default tokenizer.
2022-12-16 20:08:56 | INFO | absl | Using default tokenizer.
2022-12-16 20:09:22 | INFO | absl | Using default tokenizer.
2022-12-16 20:09:51 | INFO | absl | Using default tokenizer.
2022-12-16 20:10:23 | INFO | absl | Using default tokenizer.
2022-12-16 20:10:53 | INFO | absl | Using default tokenizer.
2022-12-16 20:11:23 | INFO | absl | Using default tokenizer.
2022-12-16 20:11:53 | INFO | absl | Using default tokenizer.
2022-12-16 20:12:32 | INFO | absl | Using default tokenizer.
2022-12-16 20:13:05 | INFO | absl | Using default tokenizer.
2022-12-16 20:13:36 | INFO | absl | Using default tokenizer.
2022-12-16 20:14:10 | INFO | absl | Using default tokenizer.
2022-12-16 20:14:45 | INFO | absl | Using default tokenizer.
2022-12-16 20:15:19 | INFO | absl | Using default tokenizer.
2022-12-16 20:15:51 | INFO | absl | Using default tokenizer.
2022-12-16 20:16:23 | INFO | absl | Using default tokenizer.
2022-12-16 20:17:02 | INFO | absl | Using default tokenizer.
2022-12-16 20:17:33 | INFO | absl | Using default tokenizer.
2022-12-16 20:18:12 | INFO | absl | Using default tokenizer.
2022-12-16 20:18:40 | INFO | absl | Using default tokenizer.
2022-12-16 20:19:19 | INFO | absl | Using default tokenizer.
2022-12-16 20:19:45 | INFO | valid | {"epoch": 39, "valid_loss": "6.171", "valid_nll_loss": "6.171", "valid_rouge1": 0.5104637612919363, "valid_rouge2": 0.1469848172871486, "valid_rougel": 0.2249421501533178, "valid_rouge_avg": 0.32872428928954256, "valid_ppl": "72.05", "valid_wps": "107.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5291", "valid_best_rouge_avg": 0.32872428928954256}
2022-12-16 20:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 5291 updates
2022-12-16 20:19:45 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 20:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 20:28:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 39 @ 5291 updates, score 0.32872428928954256) (writing took 507.79940392798744 seconds)
2022-12-16 20:28:13 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-12-16 20:28:13 | INFO | train | {"epoch": 39, "train_loss": "0.108", "train_nll_loss": "0.108", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.08", "train_wps": "706.3", "train_ups": "0.05", "train_wpb": "13878.6", "train_bsz": "31.8", "train_num_updates": "5291", "train_lr": "4.95684e-05", "train_gnorm": "1.385", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "89944"}
2022-12-16 20:28:13 | INFO | fairseq.trainer | begin training epoch 40
2022-12-16 20:28:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 20:29:56 | INFO | train_inner | {"epoch": 40, "update": 39.066, "loss": "0.095", "nll_loss": "0.095", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "79", "ups": "0.01", "wpb": "13084.5", "bsz": "29.6", "num_updates": "5300", "lr": "4.94737e-05", "gnorm": "1.214", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90047"}
2022-12-16 20:31:16 | INFO | train_inner | {"epoch": 40, "update": 39.14, "loss": "0.095", "nll_loss": "0.095", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1758.5", "ups": "0.12", "wpb": "14112.4", "bsz": "32", "num_updates": "5310", "lr": "4.93684e-05", "gnorm": "1.299", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.9", "wall": "90127"}
2022-12-16 20:32:37 | INFO | train_inner | {"epoch": 40, "update": 39.213, "loss": "0.094", "nll_loss": "0.094", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1706.2", "ups": "0.12", "wpb": "13796.5", "bsz": "32", "num_updates": "5320", "lr": "4.92632e-05", "gnorm": "1.207", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "90208"}
2022-12-16 20:33:57 | INFO | train_inner | {"epoch": 40, "update": 39.287, "loss": "0.092", "nll_loss": "0.092", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1740.2", "ups": "0.12", "wpb": "13927.6", "bsz": "32", "num_updates": "5330", "lr": "4.91579e-05", "gnorm": "1.5", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90288"}
2022-12-16 20:35:18 | INFO | train_inner | {"epoch": 40, "update": 39.36, "loss": "0.096", "nll_loss": "0.096", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1733.5", "ups": "0.12", "wpb": "13932.6", "bsz": "32", "num_updates": "5340", "lr": "4.90526e-05", "gnorm": "1.185", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90369"}
2022-12-16 20:36:36 | INFO | train_inner | {"epoch": 40, "update": 39.434, "loss": "0.092", "nll_loss": "0.092", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1770.4", "ups": "0.13", "wpb": "13918.7", "bsz": "32", "num_updates": "5350", "lr": "4.89474e-05", "gnorm": "1.265", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "90447"}
2022-12-16 20:37:56 | INFO | train_inner | {"epoch": 40, "update": 39.507, "loss": "0.093", "nll_loss": "0.093", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1723.7", "ups": "0.12", "wpb": "13792.8", "bsz": "32", "num_updates": "5360", "lr": "4.88421e-05", "gnorm": "1.285", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90527"}
2022-12-16 20:39:17 | INFO | train_inner | {"epoch": 40, "update": 39.581, "loss": "0.092", "nll_loss": "0.092", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1738", "ups": "0.12", "wpb": "14087.7", "bsz": "32", "num_updates": "5370", "lr": "4.87368e-05", "gnorm": "1.131", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.9", "wall": "90608"}
2022-12-16 20:40:37 | INFO | train_inner | {"epoch": 40, "update": 39.654, "loss": "0.098", "nll_loss": "0.098", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1741.6", "ups": "0.12", "wpb": "13941.9", "bsz": "32", "num_updates": "5380", "lr": "4.86316e-05", "gnorm": "0.838", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90688"}
2022-12-16 20:41:57 | INFO | train_inner | {"epoch": 40, "update": 39.728, "loss": "0.101", "nll_loss": "0.101", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1782.8", "ups": "0.13", "wpb": "14258.1", "bsz": "32", "num_updates": "5390", "lr": "4.85263e-05", "gnorm": "1.199", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "63", "wall": "90769"}
2022-12-16 20:43:17 | INFO | train_inner | {"epoch": 40, "update": 39.801, "loss": "0.102", "nll_loss": "0.102", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1755.9", "ups": "0.13", "wpb": "13906.2", "bsz": "32", "num_updates": "5400", "lr": "4.84211e-05", "gnorm": "0.861", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "90848"}
2022-12-16 20:44:37 | INFO | train_inner | {"epoch": 40, "update": 39.875, "loss": "0.103", "nll_loss": "0.103", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1735.8", "ups": "0.12", "wpb": "13889.4", "bsz": "32", "num_updates": "5410", "lr": "4.83158e-05", "gnorm": "1.115", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "90928"}
2022-12-16 20:45:57 | INFO | train_inner | {"epoch": 40, "update": 39.949, "loss": "0.1", "nll_loss": "0.1", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1710.2", "ups": "0.12", "wpb": "13690.6", "bsz": "32", "num_updates": "5420", "lr": "4.82105e-05", "gnorm": "0.923", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "91008"}
2022-12-16 20:46:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 20:47:38 | INFO | absl | Using default tokenizer.
2022-12-16 20:48:04 | INFO | absl | Using default tokenizer.
2022-12-16 20:48:29 | INFO | absl | Using default tokenizer.
2022-12-16 20:49:00 | INFO | absl | Using default tokenizer.
2022-12-16 20:49:27 | INFO | absl | Using default tokenizer.
2022-12-16 20:49:55 | INFO | absl | Using default tokenizer.
2022-12-16 20:50:27 | INFO | absl | Using default tokenizer.
2022-12-16 20:50:58 | INFO | absl | Using default tokenizer.
2022-12-16 20:51:30 | INFO | absl | Using default tokenizer.
2022-12-16 20:51:56 | INFO | absl | Using default tokenizer.
2022-12-16 20:52:28 | INFO | absl | Using default tokenizer.
2022-12-16 20:53:00 | INFO | absl | Using default tokenizer.
2022-12-16 20:53:29 | INFO | absl | Using default tokenizer.
2022-12-16 20:53:56 | INFO | absl | Using default tokenizer.
2022-12-16 20:54:24 | INFO | absl | Using default tokenizer.
2022-12-16 20:54:56 | INFO | absl | Using default tokenizer.
2022-12-16 20:55:31 | INFO | absl | Using default tokenizer.
2022-12-16 20:56:04 | INFO | absl | Using default tokenizer.
2022-12-16 20:56:35 | INFO | absl | Using default tokenizer.
2022-12-16 20:57:06 | INFO | absl | Using default tokenizer.
2022-12-16 20:57:35 | INFO | absl | Using default tokenizer.
2022-12-16 20:58:13 | INFO | absl | Using default tokenizer.
2022-12-16 20:58:44 | INFO | absl | Using default tokenizer.
2022-12-16 20:59:16 | INFO | absl | Using default tokenizer.
2022-12-16 20:59:48 | INFO | absl | Using default tokenizer.
2022-12-16 21:00:17 | INFO | absl | Using default tokenizer.
2022-12-16 21:00:48 | INFO | absl | Using default tokenizer.
2022-12-16 21:01:18 | INFO | absl | Using default tokenizer.
2022-12-16 21:01:49 | INFO | absl | Using default tokenizer.
2022-12-16 21:02:21 | INFO | absl | Using default tokenizer.
2022-12-16 21:02:52 | INFO | absl | Using default tokenizer.
2022-12-16 21:03:18 | INFO | valid | {"epoch": 40, "valid_loss": "6.192", "valid_nll_loss": "6.192", "valid_rouge1": 0.5092871118419335, "valid_rouge2": 0.14612315515255977, "valid_rougel": 0.22557067595629957, "valid_rouge_avg": 0.3277051334972467, "valid_ppl": "73.12", "valid_wps": "113.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5427", "valid_best_rouge_avg": 0.32872428928954256}
2022-12-16 21:03:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 5427 updates
2022-12-16 21:03:18 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 21:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 21:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 40 @ 5427 updates, score 0.3277051334972467) (writing took 20.84096198482439 seconds)
2022-12-16 21:03:39 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-12-16 21:03:39 | INFO | train | {"epoch": 40, "train_loss": "0.096", "train_nll_loss": "0.096", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.07", "train_wps": "887.5", "train_ups": "0.06", "train_wpb": "13872.4", "train_bsz": "31.8", "train_num_updates": "5427", "train_lr": "4.81368e-05", "train_gnorm": "1.149", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "92070"}
2022-12-16 21:03:39 | INFO | fairseq.trainer | begin training epoch 41
2022-12-16 21:03:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 21:04:40 | INFO | train_inner | {"epoch": 41, "update": 40.022, "loss": "0.096", "nll_loss": "0.096", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "113.6", "ups": "0.01", "wpb": "12759.6", "bsz": "29.6", "num_updates": "5430", "lr": "4.81053e-05", "gnorm": "1.17", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "92131"}
2022-12-16 21:06:02 | INFO | train_inner | {"epoch": 41, "update": 40.096, "loss": "0.089", "nll_loss": "0.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1673.2", "ups": "0.12", "wpb": "13743.8", "bsz": "32", "num_updates": "5440", "lr": "4.8e-05", "gnorm": "3.983", "clip": "100", "loss_scale": "0.25", "train_wall": "82", "gb_free": "62.2", "wall": "92213"}
2022-12-16 21:07:21 | INFO | train_inner | {"epoch": 41, "update": 40.169, "loss": "0.087", "nll_loss": "0.087", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1716.3", "ups": "0.13", "wpb": "13611.9", "bsz": "32", "num_updates": "5450", "lr": "4.78947e-05", "gnorm": "0.863", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "92292"}
2022-12-16 21:08:41 | INFO | train_inner | {"epoch": 41, "update": 40.243, "loss": "0.089", "nll_loss": "0.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1750.8", "ups": "0.13", "wpb": "13967.3", "bsz": "32", "num_updates": "5460", "lr": "4.77895e-05", "gnorm": "0.862", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "92372"}
2022-12-16 21:10:01 | INFO | train_inner | {"epoch": 41, "update": 40.316, "loss": "0.089", "nll_loss": "0.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1745", "ups": "0.13", "wpb": "13923.9", "bsz": "32", "num_updates": "5470", "lr": "4.76842e-05", "gnorm": "0.855", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "92452"}
2022-12-16 21:11:22 | INFO | train_inner | {"epoch": 41, "update": 40.39, "loss": "0.088", "nll_loss": "0.088", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1737.2", "ups": "0.12", "wpb": "14016", "bsz": "32", "num_updates": "5480", "lr": "4.75789e-05", "gnorm": "0.809", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "92533"}
2022-12-16 21:12:40 | INFO | train_inner | {"epoch": 41, "update": 40.463, "loss": "0.09", "nll_loss": "0.09", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1769.2", "ups": "0.13", "wpb": "13925.1", "bsz": "32", "num_updates": "5490", "lr": "4.74737e-05", "gnorm": "0.842", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "92611"}
2022-12-16 21:14:00 | INFO | train_inner | {"epoch": 41, "update": 40.537, "loss": "0.089", "nll_loss": "0.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1752.4", "ups": "0.13", "wpb": "13974.1", "bsz": "32", "num_updates": "5500", "lr": "4.73684e-05", "gnorm": "0.823", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "92691"}
2022-12-16 21:15:21 | INFO | train_inner | {"epoch": 41, "update": 40.61, "loss": "0.088", "nll_loss": "0.088", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1786.8", "ups": "0.12", "wpb": "14395.4", "bsz": "32", "num_updates": "5510", "lr": "4.72632e-05", "gnorm": "0.774", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "92772"}
2022-12-16 21:16:41 | INFO | train_inner | {"epoch": 41, "update": 40.684, "loss": "0.09", "nll_loss": "0.09", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1735.5", "ups": "0.12", "wpb": "13965.8", "bsz": "32", "num_updates": "5520", "lr": "4.71579e-05", "gnorm": "0.78", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "92852"}
2022-12-16 21:18:01 | INFO | train_inner | {"epoch": 41, "update": 40.757, "loss": "0.091", "nll_loss": "0.091", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1720", "ups": "0.12", "wpb": "13778.7", "bsz": "32", "num_updates": "5530", "lr": "4.70526e-05", "gnorm": "0.859", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "92932"}
2022-12-16 21:19:22 | INFO | train_inner | {"epoch": 41, "update": 40.831, "loss": "0.092", "nll_loss": "0.092", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1738.6", "ups": "0.12", "wpb": "14011.4", "bsz": "32", "num_updates": "5540", "lr": "4.69474e-05", "gnorm": "0.873", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "93013"}
2022-12-16 21:20:43 | INFO | train_inner | {"epoch": 41, "update": 40.904, "loss": "0.091", "nll_loss": "0.091", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.07", "wps": "1761", "ups": "0.12", "wpb": "14267.7", "bsz": "32", "num_updates": "5550", "lr": "4.68421e-05", "gnorm": "1.025", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "93094"}
2022-12-16 21:22:04 | INFO | train_inner | {"epoch": 41, "update": 40.978, "loss": "0.089", "nll_loss": "0.089", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1719.4", "ups": "0.12", "wpb": "13907.5", "bsz": "32", "num_updates": "5560", "lr": "4.67368e-05", "gnorm": "0.8", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "93175"}
2022-12-16 21:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 21:23:15 | INFO | absl | Using default tokenizer.
2022-12-16 21:23:42 | INFO | absl | Using default tokenizer.
2022-12-16 21:24:14 | INFO | absl | Using default tokenizer.
2022-12-16 21:24:42 | INFO | absl | Using default tokenizer.
2022-12-16 21:25:12 | INFO | absl | Using default tokenizer.
2022-12-16 21:25:46 | INFO | absl | Using default tokenizer.
2022-12-16 21:26:15 | INFO | absl | Using default tokenizer.
2022-12-16 21:26:48 | INFO | absl | Using default tokenizer.
2022-12-16 21:27:25 | INFO | absl | Using default tokenizer.
2022-12-16 21:27:56 | INFO | absl | Using default tokenizer.
2022-12-16 21:28:31 | INFO | absl | Using default tokenizer.
2022-12-16 21:29:05 | INFO | absl | Using default tokenizer.
2022-12-16 21:29:38 | INFO | absl | Using default tokenizer.
2022-12-16 21:30:09 | INFO | absl | Using default tokenizer.
2022-12-16 21:30:43 | INFO | absl | Using default tokenizer.
2022-12-16 21:31:23 | INFO | absl | Using default tokenizer.
2022-12-16 21:31:57 | INFO | absl | Using default tokenizer.
2022-12-16 21:32:31 | INFO | absl | Using default tokenizer.
2022-12-16 21:33:03 | INFO | absl | Using default tokenizer.
2022-12-16 21:33:39 | INFO | absl | Using default tokenizer.
2022-12-16 21:34:13 | INFO | absl | Using default tokenizer.
2022-12-16 21:34:49 | INFO | absl | Using default tokenizer.
2022-12-16 21:35:18 | INFO | absl | Using default tokenizer.
2022-12-16 21:35:52 | INFO | absl | Using default tokenizer.
2022-12-16 21:36:26 | INFO | absl | Using default tokenizer.
2022-12-16 21:36:58 | INFO | absl | Using default tokenizer.
2022-12-16 21:37:32 | INFO | absl | Using default tokenizer.
2022-12-16 21:38:01 | INFO | absl | Using default tokenizer.
2022-12-16 21:38:38 | INFO | absl | Using default tokenizer.
2022-12-16 21:39:15 | INFO | absl | Using default tokenizer.
2022-12-16 21:39:53 | INFO | absl | Using default tokenizer.
2022-12-16 21:40:20 | INFO | valid | {"epoch": 41, "valid_loss": "6.239", "valid_nll_loss": "6.239", "valid_rouge1": 0.5123032160555205, "valid_rouge2": 0.14766627460702633, "valid_rougel": 0.224937840903799, "valid_rouge_avg": 0.32998474533127353, "valid_ppl": "75.52", "valid_wps": "104", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5563", "valid_best_rouge_avg": 0.32998474533127353}
2022-12-16 21:40:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 5563 updates
2022-12-16 21:40:20 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 21:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 21:49:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 41 @ 5563 updates, score 0.32998474533127353) (writing took 531.6690831470769 seconds)
2022-12-16 21:49:11 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-12-16 21:49:11 | INFO | train | {"epoch": 41, "train_loss": "0.089", "train_nll_loss": "0.089", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.06", "train_wps": "690.7", "train_ups": "0.05", "train_wpb": "13879.8", "train_bsz": "31.8", "train_num_updates": "5563", "train_lr": "4.67053e-05", "train_gnorm": "1.092", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1087", "train_gb_free": "62.2", "train_wall": "94802"}
2022-12-16 21:49:11 | INFO | fairseq.trainer | begin training epoch 42
2022-12-16 21:49:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 21:50:41 | INFO | train_inner | {"epoch": 42, "update": 41.051, "loss": "0.08", "nll_loss": "0.08", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "75.3", "ups": "0.01", "wpb": "12920.3", "bsz": "29.6", "num_updates": "5570", "lr": "4.66316e-05", "gnorm": "1.081", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "94892"}
2022-12-16 21:52:02 | INFO | train_inner | {"epoch": 42, "update": 41.125, "loss": "0.076", "nll_loss": "0.076", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1693.2", "ups": "0.12", "wpb": "13809.3", "bsz": "32", "num_updates": "5580", "lr": "4.65263e-05", "gnorm": "1.047", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "94973"}
2022-12-16 21:53:22 | INFO | train_inner | {"epoch": 42, "update": 41.199, "loss": "0.08", "nll_loss": "0.08", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1750.9", "ups": "0.13", "wpb": "13980.6", "bsz": "32", "num_updates": "5590", "lr": "4.64211e-05", "gnorm": "1.07", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "95053"}
2022-12-16 21:54:43 | INFO | train_inner | {"epoch": 42, "update": 41.272, "loss": "0.081", "nll_loss": "0.081", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1726.3", "ups": "0.12", "wpb": "13994.6", "bsz": "32", "num_updates": "5600", "lr": "4.63158e-05", "gnorm": "0.77", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "95134"}
2022-12-16 21:56:03 | INFO | train_inner | {"epoch": 42, "update": 41.346, "loss": "0.08", "nll_loss": "0.08", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1753.1", "ups": "0.12", "wpb": "14055.1", "bsz": "32", "num_updates": "5610", "lr": "4.62105e-05", "gnorm": "0.897", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "95214"}
2022-12-16 21:57:24 | INFO | train_inner | {"epoch": 42, "update": 41.419, "loss": "0.08", "nll_loss": "0.08", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1731.3", "ups": "0.12", "wpb": "13951.5", "bsz": "32", "num_updates": "5620", "lr": "4.61053e-05", "gnorm": "0.807", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "95295"}
2022-12-16 21:58:44 | INFO | train_inner | {"epoch": 42, "update": 41.493, "loss": "0.081", "nll_loss": "0.081", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1729.1", "ups": "0.13", "wpb": "13773.8", "bsz": "32", "num_updates": "5630", "lr": "4.6e-05", "gnorm": "0.777", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "95375"}
2022-12-16 22:00:03 | INFO | train_inner | {"epoch": 42, "update": 41.566, "loss": "0.084", "nll_loss": "0.084", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1748", "ups": "0.13", "wpb": "13886.3", "bsz": "32", "num_updates": "5640", "lr": "4.58947e-05", "gnorm": "0.864", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "63", "wall": "95454"}
2022-12-16 22:01:24 | INFO | train_inner | {"epoch": 42, "update": 41.64, "loss": "0.082", "nll_loss": "0.082", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1738.4", "ups": "0.12", "wpb": "14016.4", "bsz": "32", "num_updates": "5650", "lr": "4.57895e-05", "gnorm": "0.87", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "95535"}
2022-12-16 22:02:42 | INFO | train_inner | {"epoch": 42, "update": 41.713, "loss": "0.083", "nll_loss": "0.083", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1777.8", "ups": "0.13", "wpb": "13818.3", "bsz": "32", "num_updates": "5660", "lr": "4.56842e-05", "gnorm": "1.107", "clip": "100", "loss_scale": "0.25", "train_wall": "77", "gb_free": "62.2", "wall": "95613"}
2022-12-16 22:04:02 | INFO | train_inner | {"epoch": 42, "update": 41.787, "loss": "0.082", "nll_loss": "0.082", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1718.1", "ups": "0.12", "wpb": "13828.3", "bsz": "32", "num_updates": "5670", "lr": "4.55789e-05", "gnorm": "0.798", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "95693"}
2022-12-16 22:05:22 | INFO | train_inner | {"epoch": 42, "update": 41.86, "loss": "0.086", "nll_loss": "0.086", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1781.3", "ups": "0.13", "wpb": "14228.3", "bsz": "32", "num_updates": "5680", "lr": "4.54737e-05", "gnorm": "0.774", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "95773"}
2022-12-16 22:06:44 | INFO | train_inner | {"epoch": 42, "update": 41.934, "loss": "0.086", "nll_loss": "0.086", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1720.9", "ups": "0.12", "wpb": "14078.9", "bsz": "32", "num_updates": "5690", "lr": "4.53684e-05", "gnorm": "0.814", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "95855"}
2022-12-16 22:07:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 22:08:41 | INFO | absl | Using default tokenizer.
2022-12-16 22:09:11 | INFO | absl | Using default tokenizer.
2022-12-16 22:09:37 | INFO | absl | Using default tokenizer.
2022-12-16 22:10:08 | INFO | absl | Using default tokenizer.
2022-12-16 22:10:35 | INFO | absl | Using default tokenizer.
2022-12-16 22:11:05 | INFO | absl | Using default tokenizer.
2022-12-16 22:11:35 | INFO | absl | Using default tokenizer.
2022-12-16 22:12:01 | INFO | absl | Using default tokenizer.
2022-12-16 22:12:34 | INFO | absl | Using default tokenizer.
2022-12-16 22:13:03 | INFO | absl | Using default tokenizer.
2022-12-16 22:13:32 | INFO | absl | Using default tokenizer.
2022-12-16 22:14:09 | INFO | absl | Using default tokenizer.
2022-12-16 22:14:40 | INFO | absl | Using default tokenizer.
2022-12-16 22:15:13 | INFO | absl | Using default tokenizer.
2022-12-16 22:15:42 | INFO | absl | Using default tokenizer.
2022-12-16 22:16:14 | INFO | absl | Using default tokenizer.
2022-12-16 22:16:49 | INFO | absl | Using default tokenizer.
2022-12-16 22:17:21 | INFO | absl | Using default tokenizer.
2022-12-16 22:17:56 | INFO | absl | Using default tokenizer.
2022-12-16 22:18:27 | INFO | absl | Using default tokenizer.
2022-12-16 22:19:01 | INFO | absl | Using default tokenizer.
2022-12-16 22:19:36 | INFO | absl | Using default tokenizer.
2022-12-16 22:20:10 | INFO | absl | Using default tokenizer.
2022-12-16 22:20:40 | INFO | absl | Using default tokenizer.
2022-12-16 22:21:10 | INFO | absl | Using default tokenizer.
2022-12-16 22:21:40 | INFO | absl | Using default tokenizer.
2022-12-16 22:22:14 | INFO | absl | Using default tokenizer.
2022-12-16 22:22:50 | INFO | absl | Using default tokenizer.
2022-12-16 22:23:25 | INFO | absl | Using default tokenizer.
2022-12-16 22:23:57 | INFO | absl | Using default tokenizer.
2022-12-16 22:24:29 | INFO | absl | Using default tokenizer.
2022-12-16 22:24:57 | INFO | valid | {"epoch": 42, "valid_loss": "6.285", "valid_nll_loss": "6.285", "valid_rouge1": 0.5112164934470137, "valid_rouge2": 0.14604205652517754, "valid_rougel": 0.22570891448828967, "valid_rouge_avg": 0.3286292749860957, "valid_ppl": "77.99", "valid_wps": "109.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5699", "valid_best_rouge_avg": 0.32998474533127353}
2022-12-16 22:24:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 5699 updates
2022-12-16 22:24:57 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 22:25:22 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 22:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 42 @ 5699 updates, score 0.3286292749860957) (writing took 25.589510423131287 seconds)
2022-12-16 22:25:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-12-16 22:25:22 | INFO | train | {"epoch": 42, "train_loss": "0.082", "train_nll_loss": "0.082", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.06", "train_wps": "869.4", "train_ups": "0.06", "train_wpb": "13878.6", "train_bsz": "31.8", "train_num_updates": "5699", "train_lr": "4.52737e-05", "train_gnorm": "0.903", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1085", "train_gb_free": "62.2", "train_wall": "96973"}
2022-12-16 22:25:22 | INFO | fairseq.trainer | begin training epoch 43
2022-12-16 22:25:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 22:26:09 | INFO | train_inner | {"epoch": 43, "update": 42.007, "loss": "0.086", "nll_loss": "0.086", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "111.3", "ups": "0.01", "wpb": "12966.4", "bsz": "29.6", "num_updates": "5700", "lr": "4.52632e-05", "gnorm": "1.19", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97020"}
2022-12-16 22:27:27 | INFO | train_inner | {"epoch": 43, "update": 42.081, "loss": "0.076", "nll_loss": "0.076", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1793.9", "ups": "0.13", "wpb": "13958.9", "bsz": "32", "num_updates": "5710", "lr": "4.51579e-05", "gnorm": "0.99", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "97098"}
2022-12-16 22:28:49 | INFO | train_inner | {"epoch": 43, "update": 42.154, "loss": "0.072", "nll_loss": "0.072", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1709.3", "ups": "0.12", "wpb": "13972.5", "bsz": "32", "num_updates": "5720", "lr": "4.50526e-05", "gnorm": "0.895", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "97180"}
2022-12-16 22:30:08 | INFO | train_inner | {"epoch": 43, "update": 42.228, "loss": "0.075", "nll_loss": "0.075", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1782", "ups": "0.13", "wpb": "14085.7", "bsz": "32", "num_updates": "5730", "lr": "4.49474e-05", "gnorm": "0.793", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.9", "wall": "97259"}
2022-12-16 22:31:27 | INFO | train_inner | {"epoch": 43, "update": 42.301, "loss": "0.074", "nll_loss": "0.074", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1763.6", "ups": "0.13", "wpb": "14047.7", "bsz": "32", "num_updates": "5740", "lr": "4.48421e-05", "gnorm": "0.888", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "97338"}
2022-12-16 22:32:47 | INFO | train_inner | {"epoch": 43, "update": 42.375, "loss": "0.074", "nll_loss": "0.074", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1767.3", "ups": "0.12", "wpb": "14152.7", "bsz": "32", "num_updates": "5750", "lr": "4.47368e-05", "gnorm": "0.939", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97418"}
2022-12-16 22:34:07 | INFO | train_inner | {"epoch": 43, "update": 42.449, "loss": "0.073", "nll_loss": "0.073", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1761.3", "ups": "0.13", "wpb": "14074.7", "bsz": "32", "num_updates": "5760", "lr": "4.46316e-05", "gnorm": "0.891", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97498"}
2022-12-16 22:35:28 | INFO | train_inner | {"epoch": 43, "update": 42.522, "loss": "0.076", "nll_loss": "0.076", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1713.3", "ups": "0.12", "wpb": "13749.3", "bsz": "32", "num_updates": "5770", "lr": "4.45263e-05", "gnorm": "0.846", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97579"}
2022-12-16 22:36:49 | INFO | train_inner | {"epoch": 43, "update": 42.596, "loss": "0.076", "nll_loss": "0.076", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1758.8", "ups": "0.12", "wpb": "14244.4", "bsz": "32", "num_updates": "5780", "lr": "4.44211e-05", "gnorm": "0.85", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "97660"}
2022-12-16 22:38:09 | INFO | train_inner | {"epoch": 43, "update": 42.669, "loss": "0.077", "nll_loss": "0.077", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1696.2", "ups": "0.12", "wpb": "13643.9", "bsz": "32", "num_updates": "5790", "lr": "4.43158e-05", "gnorm": "0.77", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97740"}
2022-12-16 22:39:30 | INFO | train_inner | {"epoch": 43, "update": 42.743, "loss": "0.079", "nll_loss": "0.079", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1736.5", "ups": "0.12", "wpb": "14131.7", "bsz": "32", "num_updates": "5800", "lr": "4.42105e-05", "gnorm": "0.938", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "63.6", "wall": "97821"}
2022-12-16 22:40:52 | INFO | train_inner | {"epoch": 43, "update": 42.816, "loss": "0.079", "nll_loss": "0.079", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1701.7", "ups": "0.12", "wpb": "13803.9", "bsz": "32", "num_updates": "5810", "lr": "4.41053e-05", "gnorm": "1.347", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "97903"}
2022-12-16 22:42:12 | INFO | train_inner | {"epoch": 43, "update": 42.89, "loss": "0.08", "nll_loss": "0.08", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1713.4", "ups": "0.12", "wpb": "13827.3", "bsz": "32", "num_updates": "5820", "lr": "4.4e-05", "gnorm": "0.902", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "97983"}
2022-12-16 22:43:32 | INFO | train_inner | {"epoch": 43, "update": 42.963, "loss": "0.081", "nll_loss": "0.081", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.06", "wps": "1721.4", "ups": "0.13", "wpb": "13751.4", "bsz": "32", "num_updates": "5830", "lr": "4.38947e-05", "gnorm": "0.884", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "63", "wall": "98063"}
2022-12-16 22:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 22:45:00 | INFO | absl | Using default tokenizer.
2022-12-16 22:45:33 | INFO | absl | Using default tokenizer.
2022-12-16 22:46:07 | INFO | absl | Using default tokenizer.
2022-12-16 22:46:35 | INFO | absl | Using default tokenizer.
2022-12-16 22:47:05 | INFO | absl | Using default tokenizer.
2022-12-16 22:47:30 | INFO | absl | Using default tokenizer.
2022-12-16 22:47:55 | INFO | absl | Using default tokenizer.
2022-12-16 22:48:23 | INFO | absl | Using default tokenizer.
2022-12-16 22:49:03 | INFO | absl | Using default tokenizer.
2022-12-16 22:49:35 | INFO | absl | Using default tokenizer.
2022-12-16 22:50:07 | INFO | absl | Using default tokenizer.
2022-12-16 22:50:44 | INFO | absl | Using default tokenizer.
2022-12-16 22:51:14 | INFO | absl | Using default tokenizer.
2022-12-16 22:51:51 | INFO | absl | Using default tokenizer.
2022-12-16 22:52:21 | INFO | absl | Using default tokenizer.
2022-12-16 22:52:51 | INFO | absl | Using default tokenizer.
2022-12-16 22:53:25 | INFO | absl | Using default tokenizer.
2022-12-16 22:53:59 | INFO | absl | Using default tokenizer.
2022-12-16 22:54:29 | INFO | absl | Using default tokenizer.
2022-12-16 22:54:59 | INFO | absl | Using default tokenizer.
2022-12-16 22:55:32 | INFO | absl | Using default tokenizer.
2022-12-16 22:56:10 | INFO | absl | Using default tokenizer.
2022-12-16 22:56:41 | INFO | absl | Using default tokenizer.
2022-12-16 22:57:12 | INFO | absl | Using default tokenizer.
2022-12-16 22:57:42 | INFO | absl | Using default tokenizer.
2022-12-16 22:58:13 | INFO | absl | Using default tokenizer.
2022-12-16 22:58:42 | INFO | absl | Using default tokenizer.
2022-12-16 22:59:12 | INFO | absl | Using default tokenizer.
2022-12-16 22:59:51 | INFO | absl | Using default tokenizer.
2022-12-16 23:00:26 | INFO | absl | Using default tokenizer.
2022-12-16 23:01:05 | INFO | absl | Using default tokenizer.
2022-12-16 23:01:29 | INFO | valid | {"epoch": 43, "valid_loss": "6.317", "valid_nll_loss": "6.317", "valid_rouge1": 0.5138147305798149, "valid_rouge2": 0.14732194615437108, "valid_rougel": 0.22601135526281343, "valid_rouge_avg": 0.33056833836709293, "valid_ppl": "79.72", "valid_wps": "107.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5835", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-16 23:01:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 5835 updates
2022-12-16 23:01:29 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 23:01:52 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_best.pt
2022-12-16 23:08:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 43 @ 5835 updates, score 0.33056833836709293) (writing took 437.4884514051955 seconds)
2022-12-16 23:08:46 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-12-16 23:08:46 | INFO | train | {"epoch": 43, "train_loss": "0.076", "train_nll_loss": "0.076", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.05", "train_wps": "724.9", "train_ups": "0.05", "train_wpb": "13879", "train_bsz": "31.8", "train_num_updates": "5835", "train_lr": "4.38421e-05", "train_gnorm": "0.929", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1090", "train_gb_free": "62.2", "train_wall": "99577"}
2022-12-16 23:08:46 | INFO | fairseq.trainer | begin training epoch 44
2022-12-16 23:08:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 23:09:58 | INFO | train_inner | {"epoch": 44, "update": 43.037, "loss": "0.074", "nll_loss": "0.074", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "82", "ups": "0.01", "wpb": "12998.8", "bsz": "29.6", "num_updates": "5840", "lr": "4.37895e-05", "gnorm": "0.947", "clip": "100", "loss_scale": "0.25", "train_wall": "83", "gb_free": "62.2", "wall": "99649"}
2022-12-16 23:11:17 | INFO | train_inner | {"epoch": 44, "update": 43.11, "loss": "0.071", "nll_loss": "0.071", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1752.8", "ups": "0.13", "wpb": "13781", "bsz": "32", "num_updates": "5850", "lr": "4.36842e-05", "gnorm": "1.283", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "99728"}
2022-12-16 23:12:36 | INFO | train_inner | {"epoch": 44, "update": 43.184, "loss": "0.071", "nll_loss": "0.071", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1724.2", "ups": "0.13", "wpb": "13688.1", "bsz": "32", "num_updates": "5860", "lr": "4.35789e-05", "gnorm": "1.048", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "99807"}
2022-12-16 23:13:57 | INFO | train_inner | {"epoch": 44, "update": 43.257, "loss": "0.07", "nll_loss": "0.07", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1722.8", "ups": "0.12", "wpb": "13915.1", "bsz": "32", "num_updates": "5870", "lr": "4.34737e-05", "gnorm": "0.819", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "99888"}
2022-12-16 23:15:17 | INFO | train_inner | {"epoch": 44, "update": 43.331, "loss": "0.072", "nll_loss": "0.072", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1757.9", "ups": "0.13", "wpb": "14039.9", "bsz": "32", "num_updates": "5880", "lr": "4.33684e-05", "gnorm": "0.776", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "99968"}
2022-12-16 23:16:37 | INFO | train_inner | {"epoch": 44, "update": 43.404, "loss": "0.068", "nll_loss": "0.068", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1766.3", "ups": "0.13", "wpb": "14112.3", "bsz": "32", "num_updates": "5890", "lr": "4.32632e-05", "gnorm": "0.805", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "100048"}
2022-12-16 23:17:57 | INFO | train_inner | {"epoch": 44, "update": 43.478, "loss": "0.073", "nll_loss": "0.073", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1725.6", "ups": "0.12", "wpb": "13932.3", "bsz": "32", "num_updates": "5900", "lr": "4.31579e-05", "gnorm": "1.125", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "100128"}
2022-12-16 23:19:18 | INFO | train_inner | {"epoch": 44, "update": 43.551, "loss": "0.071", "nll_loss": "0.071", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1711.2", "ups": "0.12", "wpb": "13757.5", "bsz": "32", "num_updates": "5910", "lr": "4.30526e-05", "gnorm": "0.754", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.9", "wall": "100209"}
2022-12-16 23:20:38 | INFO | train_inner | {"epoch": 44, "update": 43.625, "loss": "0.069", "nll_loss": "0.069", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1736.9", "ups": "0.12", "wpb": "13985.4", "bsz": "32", "num_updates": "5920", "lr": "4.29474e-05", "gnorm": "0.915", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "100289"}
2022-12-16 23:21:57 | INFO | train_inner | {"epoch": 44, "update": 43.699, "loss": "0.071", "nll_loss": "0.071", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1767.2", "ups": "0.13", "wpb": "13972.2", "bsz": "32", "num_updates": "5930", "lr": "4.28421e-05", "gnorm": "0.778", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "100368"}
2022-12-16 23:23:19 | INFO | train_inner | {"epoch": 44, "update": 43.772, "loss": "0.072", "nll_loss": "0.072", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1697.5", "ups": "0.12", "wpb": "13808.3", "bsz": "32", "num_updates": "5940", "lr": "4.27368e-05", "gnorm": "0.954", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.9", "wall": "100450"}
2022-12-16 23:24:40 | INFO | train_inner | {"epoch": 44, "update": 43.846, "loss": "0.075", "nll_loss": "0.075", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1715.1", "ups": "0.12", "wpb": "13967.9", "bsz": "32", "num_updates": "5950", "lr": "4.26316e-05", "gnorm": "1.727", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "64.3", "wall": "100531"}
2022-12-16 23:26:01 | INFO | train_inner | {"epoch": 44, "update": 43.919, "loss": "0.074", "nll_loss": "0.074", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1718.8", "ups": "0.12", "wpb": "13846", "bsz": "32", "num_updates": "5960", "lr": "4.25263e-05", "gnorm": "1.011", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "100612"}
2022-12-16 23:27:22 | INFO | train_inner | {"epoch": 44, "update": 43.993, "loss": "0.07", "nll_loss": "0.07", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1780.7", "ups": "0.12", "wpb": "14467.7", "bsz": "32", "num_updates": "5970", "lr": "4.24211e-05", "gnorm": "0.769", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "100693"}
2022-12-16 23:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-16 23:28:16 | INFO | absl | Using default tokenizer.
2022-12-16 23:28:46 | INFO | absl | Using default tokenizer.
2022-12-16 23:29:13 | INFO | absl | Using default tokenizer.
2022-12-16 23:29:42 | INFO | absl | Using default tokenizer.
2022-12-16 23:30:17 | INFO | absl | Using default tokenizer.
2022-12-16 23:30:46 | INFO | absl | Using default tokenizer.
2022-12-16 23:31:22 | INFO | absl | Using default tokenizer.
2022-12-16 23:31:54 | INFO | absl | Using default tokenizer.
2022-12-16 23:32:29 | INFO | absl | Using default tokenizer.
2022-12-16 23:32:58 | INFO | absl | Using default tokenizer.
2022-12-16 23:33:31 | INFO | absl | Using default tokenizer.
2022-12-16 23:34:05 | INFO | absl | Using default tokenizer.
2022-12-16 23:34:32 | INFO | absl | Using default tokenizer.
2022-12-16 23:35:11 | INFO | absl | Using default tokenizer.
2022-12-16 23:35:39 | INFO | absl | Using default tokenizer.
2022-12-16 23:36:06 | INFO | absl | Using default tokenizer.
2022-12-16 23:36:35 | INFO | absl | Using default tokenizer.
2022-12-16 23:37:08 | INFO | absl | Using default tokenizer.
2022-12-16 23:37:36 | INFO | absl | Using default tokenizer.
2022-12-16 23:38:08 | INFO | absl | Using default tokenizer.
2022-12-16 23:38:38 | INFO | absl | Using default tokenizer.
2022-12-16 23:39:14 | INFO | absl | Using default tokenizer.
2022-12-16 23:39:46 | INFO | absl | Using default tokenizer.
2022-12-16 23:40:21 | INFO | absl | Using default tokenizer.
2022-12-16 23:40:54 | INFO | absl | Using default tokenizer.
2022-12-16 23:41:29 | INFO | absl | Using default tokenizer.
2022-12-16 23:42:03 | INFO | absl | Using default tokenizer.
2022-12-16 23:42:36 | INFO | absl | Using default tokenizer.
2022-12-16 23:43:15 | INFO | absl | Using default tokenizer.
2022-12-16 23:43:47 | INFO | absl | Using default tokenizer.
2022-12-16 23:44:22 | INFO | absl | Using default tokenizer.
2022-12-16 23:44:50 | INFO | valid | {"epoch": 44, "valid_loss": "6.346", "valid_nll_loss": "6.346", "valid_rouge1": 0.50761518779931, "valid_rouge2": 0.14377163089076994, "valid_rougel": 0.2255197804569178, "valid_rouge_avg": 0.32569340934504004, "valid_ppl": "81.34", "valid_wps": "107.4", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "5971", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-16 23:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 5971 updates
2022-12-16 23:44:50 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 23:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-16 23:45:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 44 @ 5971 updates, score 0.32569340934504004) (writing took 24.233612627955154 seconds)
2022-12-16 23:45:14 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-12-16 23:45:14 | INFO | train | {"epoch": 44, "train_loss": "0.071", "train_nll_loss": "0.071", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.05", "train_wps": "862.7", "train_ups": "0.06", "train_wpb": "13878.5", "train_bsz": "31.8", "train_num_updates": "5971", "train_lr": "4.24105e-05", "train_gnorm": "0.983", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1087", "train_gb_free": "62.2", "train_wall": "101765"}
2022-12-16 23:45:14 | INFO | fairseq.trainer | begin training epoch 45
2022-12-16 23:45:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-16 23:47:05 | INFO | train_inner | {"epoch": 45, "update": 44.066, "loss": "0.064", "nll_loss": "0.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "110.9", "ups": "0.01", "wpb": "13119.4", "bsz": "29.6", "num_updates": "5980", "lr": "4.23158e-05", "gnorm": "1.051", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "101876"}
2022-12-16 23:48:24 | INFO | train_inner | {"epoch": 45, "update": 44.14, "loss": "0.064", "nll_loss": "0.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1696.8", "ups": "0.13", "wpb": "13502.3", "bsz": "32", "num_updates": "5990", "lr": "4.22105e-05", "gnorm": "0.722", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "101955"}
2022-12-16 23:49:45 | INFO | train_inner | {"epoch": 45, "update": 44.213, "loss": "0.064", "nll_loss": "0.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1758.9", "ups": "0.12", "wpb": "14127.5", "bsz": "32", "num_updates": "6000", "lr": "4.21053e-05", "gnorm": "0.934", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102036"}
2022-12-16 23:51:05 | INFO | train_inner | {"epoch": 45, "update": 44.287, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1767.2", "ups": "0.13", "wpb": "14127.2", "bsz": "32", "num_updates": "6010", "lr": "4.2e-05", "gnorm": "1.265", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102116"}
2022-12-16 23:52:25 | INFO | train_inner | {"epoch": 45, "update": 44.36, "loss": "0.066", "nll_loss": "0.066", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1710.8", "ups": "0.12", "wpb": "13782", "bsz": "32", "num_updates": "6020", "lr": "4.18947e-05", "gnorm": "0.741", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102196"}
2022-12-16 23:53:45 | INFO | train_inner | {"epoch": 45, "update": 44.434, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1784.1", "ups": "0.13", "wpb": "14238.2", "bsz": "32", "num_updates": "6030", "lr": "4.17895e-05", "gnorm": "0.933", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.9", "wall": "102276"}
2022-12-16 23:55:04 | INFO | train_inner | {"epoch": 45, "update": 44.507, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1756.9", "ups": "0.13", "wpb": "13856.2", "bsz": "32", "num_updates": "6040", "lr": "4.16842e-05", "gnorm": "1.564", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "62.2", "wall": "102355"}
2022-12-16 23:56:24 | INFO | train_inner | {"epoch": 45, "update": 44.581, "loss": "0.07", "nll_loss": "0.07", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1769.2", "ups": "0.12", "wpb": "14218.3", "bsz": "32", "num_updates": "6050", "lr": "4.15789e-05", "gnorm": "0.909", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102435"}
2022-12-16 23:57:44 | INFO | train_inner | {"epoch": 45, "update": 44.654, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1734.1", "ups": "0.13", "wpb": "13730.4", "bsz": "32", "num_updates": "6060", "lr": "4.14737e-05", "gnorm": "0.868", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "102515"}
2022-12-16 23:59:04 | INFO | train_inner | {"epoch": 45, "update": 44.728, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1712.9", "ups": "0.12", "wpb": "13844.4", "bsz": "32", "num_updates": "6070", "lr": "4.13684e-05", "gnorm": "0.699", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102596"}
2022-12-17 00:00:26 | INFO | train_inner | {"epoch": 45, "update": 44.801, "loss": "0.068", "nll_loss": "0.068", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1756.4", "ups": "0.12", "wpb": "14265.9", "bsz": "32", "num_updates": "6080", "lr": "4.12632e-05", "gnorm": "0.717", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "102677"}
2022-12-17 00:01:46 | INFO | train_inner | {"epoch": 45, "update": 44.875, "loss": "0.068", "nll_loss": "0.068", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1715.5", "ups": "0.12", "wpb": "13739.9", "bsz": "32", "num_updates": "6090", "lr": "4.11579e-05", "gnorm": "0.705", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102757"}
2022-12-17 00:03:06 | INFO | train_inner | {"epoch": 45, "update": 44.949, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1744.5", "ups": "0.12", "wpb": "14066.5", "bsz": "32", "num_updates": "6100", "lr": "4.10526e-05", "gnorm": "0.699", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "102838"}
2022-12-17 00:04:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 00:04:50 | INFO | absl | Using default tokenizer.
2022-12-17 00:05:21 | INFO | absl | Using default tokenizer.
2022-12-17 00:05:53 | INFO | absl | Using default tokenizer.
2022-12-17 00:06:26 | INFO | absl | Using default tokenizer.
2022-12-17 00:06:59 | INFO | absl | Using default tokenizer.
2022-12-17 00:07:27 | INFO | absl | Using default tokenizer.
2022-12-17 00:07:56 | INFO | absl | Using default tokenizer.
2022-12-17 00:08:30 | INFO | absl | Using default tokenizer.
2022-12-17 00:09:06 | INFO | absl | Using default tokenizer.
2022-12-17 00:09:36 | INFO | absl | Using default tokenizer.
2022-12-17 00:10:10 | INFO | absl | Using default tokenizer.
2022-12-17 00:10:47 | INFO | absl | Using default tokenizer.
2022-12-17 00:11:20 | INFO | absl | Using default tokenizer.
2022-12-17 00:11:50 | INFO | absl | Using default tokenizer.
2022-12-17 00:12:20 | INFO | absl | Using default tokenizer.
2022-12-17 00:12:52 | INFO | absl | Using default tokenizer.
2022-12-17 00:13:32 | INFO | absl | Using default tokenizer.
2022-12-17 00:14:06 | INFO | absl | Using default tokenizer.
2022-12-17 00:14:36 | INFO | absl | Using default tokenizer.
2022-12-17 00:15:08 | INFO | absl | Using default tokenizer.
2022-12-17 00:15:37 | INFO | absl | Using default tokenizer.
2022-12-17 00:16:13 | INFO | absl | Using default tokenizer.
2022-12-17 00:16:44 | INFO | absl | Using default tokenizer.
2022-12-17 00:17:21 | INFO | absl | Using default tokenizer.
2022-12-17 00:17:59 | INFO | absl | Using default tokenizer.
2022-12-17 00:18:43 | INFO | absl | Using default tokenizer.
2022-12-17 00:19:15 | INFO | absl | Using default tokenizer.
2022-12-17 00:19:48 | INFO | absl | Using default tokenizer.
2022-12-17 00:20:30 | INFO | absl | Using default tokenizer.
2022-12-17 00:21:09 | INFO | absl | Using default tokenizer.
2022-12-17 00:21:50 | INFO | absl | Using default tokenizer.
2022-12-17 00:22:20 | INFO | valid | {"epoch": 45, "valid_loss": "6.405", "valid_nll_loss": "6.405", "valid_rouge1": 0.5130228305721586, "valid_rouge2": 0.1471790668605822, "valid_rougel": 0.22545106040870644, "valid_rouge_avg": 0.3301009487163703, "valid_ppl": "84.72", "valid_wps": "101.7", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6107", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 00:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 6107 updates
2022-12-17 00:22:20 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 00:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 00:22:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 45 @ 6107 updates, score 0.3301009487163703) (writing took 22.506085827946663 seconds)
2022-12-17 00:22:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-12-17 00:22:43 | INFO | train | {"epoch": 45, "train_loss": "0.067", "train_nll_loss": "0.067", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.05", "train_wps": "839.6", "train_ups": "0.06", "train_wpb": "13880.8", "train_bsz": "31.8", "train_num_updates": "6107", "train_lr": "4.09789e-05", "train_gnorm": "0.899", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1087", "train_gb_free": "62.2", "train_wall": "104014"}
2022-12-17 00:22:43 | INFO | fairseq.trainer | begin training epoch 46
2022-12-17 00:22:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-17 00:23:44 | INFO | train_inner | {"epoch": 46, "update": 45.022, "loss": "0.066", "nll_loss": "0.066", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "101.9", "ups": "0.01", "wpb": "12615.5", "bsz": "29.6", "num_updates": "6110", "lr": "4.09474e-05", "gnorm": "0.861", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "104075"}
2022-12-17 00:25:05 | INFO | train_inner | {"epoch": 46, "update": 45.096, "loss": "0.06", "nll_loss": "0.06", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1735.1", "ups": "0.12", "wpb": "13970.8", "bsz": "32", "num_updates": "6120", "lr": "4.08421e-05", "gnorm": "0.719", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "104156"}
2022-12-17 00:26:26 | INFO | train_inner | {"epoch": 46, "update": 45.169, "loss": "0.061", "nll_loss": "0.061", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1704.9", "ups": "0.12", "wpb": "13761", "bsz": "32", "num_updates": "6130", "lr": "4.07368e-05", "gnorm": "0.711", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.9", "wall": "104237"}
2022-12-17 00:27:47 | INFO | train_inner | {"epoch": 46, "update": 45.243, "loss": "0.062", "nll_loss": "0.062", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1688.5", "ups": "0.12", "wpb": "13719.5", "bsz": "32", "num_updates": "6140", "lr": "4.06316e-05", "gnorm": "1.147", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "104318"}
2022-12-17 00:29:07 | INFO | train_inner | {"epoch": 46, "update": 45.316, "loss": "0.062", "nll_loss": "0.062", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1749.5", "ups": "0.12", "wpb": "14092.2", "bsz": "32", "num_updates": "6150", "lr": "4.05263e-05", "gnorm": "0.982", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "104399"}
2022-12-17 00:30:28 | INFO | train_inner | {"epoch": 46, "update": 45.39, "loss": "0.062", "nll_loss": "0.062", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1746.2", "ups": "0.12", "wpb": "14064.8", "bsz": "32", "num_updates": "6160", "lr": "4.04211e-05", "gnorm": "0.759", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "104479"}
2022-12-17 00:31:49 | INFO | train_inner | {"epoch": 46, "update": 45.463, "loss": "0.063", "nll_loss": "0.063", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1685.1", "ups": "0.12", "wpb": "13696.9", "bsz": "32", "num_updates": "6170", "lr": "4.03158e-05", "gnorm": "0.816", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "104560"}
2022-12-17 00:33:10 | INFO | train_inner | {"epoch": 46, "update": 45.537, "loss": "0.061", "nll_loss": "0.061", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1736.1", "ups": "0.12", "wpb": "14014.6", "bsz": "32", "num_updates": "6180", "lr": "4.02105e-05", "gnorm": "0.67", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "104641"}
2022-12-17 00:34:31 | INFO | train_inner | {"epoch": 46, "update": 45.61, "loss": "0.06", "nll_loss": "0.06", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1713.7", "ups": "0.12", "wpb": "13893.9", "bsz": "32", "num_updates": "6190", "lr": "4.01053e-05", "gnorm": "1.384", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "104722"}
2022-12-17 00:35:50 | INFO | train_inner | {"epoch": 46, "update": 45.684, "loss": "0.063", "nll_loss": "0.063", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1819.3", "ups": "0.13", "wpb": "14274.5", "bsz": "32", "num_updates": "6200", "lr": "4e-05", "gnorm": "0.677", "clip": "100", "loss_scale": "0.25", "train_wall": "78", "gb_free": "63.7", "wall": "104801"}
2022-12-17 00:37:11 | INFO | train_inner | {"epoch": 46, "update": 45.757, "loss": "0.061", "nll_loss": "0.061", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1724", "ups": "0.12", "wpb": "14073.6", "bsz": "32", "num_updates": "6210", "lr": "3.98947e-05", "gnorm": "0.678", "clip": "100", "loss_scale": "0.25", "train_wall": "81", "gb_free": "62.2", "wall": "104882"}
2022-12-17 00:38:31 | INFO | train_inner | {"epoch": 46, "update": 45.831, "loss": "0.064", "nll_loss": "0.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1772.4", "ups": "0.13", "wpb": "14043.9", "bsz": "32", "num_updates": "6220", "lr": "3.97895e-05", "gnorm": "0.691", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "104962"}
2022-12-17 00:39:51 | INFO | train_inner | {"epoch": 46, "update": 45.904, "loss": "0.064", "nll_loss": "0.064", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1775.7", "ups": "0.12", "wpb": "14222.9", "bsz": "32", "num_updates": "6230", "lr": "3.96842e-05", "gnorm": "0.674", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "105042"}
2022-12-17 00:41:10 | INFO | train_inner | {"epoch": 46, "update": 45.978, "loss": "0.067", "nll_loss": "0.067", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.05", "wps": "1754.4", "ups": "0.13", "wpb": "13869.6", "bsz": "32", "num_updates": "6240", "lr": "3.95789e-05", "gnorm": "0.874", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "105121"}
2022-12-17 00:41:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 00:42:20 | INFO | absl | Using default tokenizer.
2022-12-17 00:42:47 | INFO | absl | Using default tokenizer.
2022-12-17 00:43:28 | INFO | absl | Using default tokenizer.
2022-12-17 00:43:56 | INFO | absl | Using default tokenizer.
2022-12-17 00:44:26 | INFO | absl | Using default tokenizer.
2022-12-17 00:44:55 | INFO | absl | Using default tokenizer.
2022-12-17 00:45:31 | INFO | absl | Using default tokenizer.
2022-12-17 00:46:00 | INFO | absl | Using default tokenizer.
2022-12-17 00:46:28 | INFO | absl | Using default tokenizer.
2022-12-17 00:46:55 | INFO | absl | Using default tokenizer.
2022-12-17 00:47:26 | INFO | absl | Using default tokenizer.
2022-12-17 00:48:02 | INFO | absl | Using default tokenizer.
2022-12-17 00:48:32 | INFO | absl | Using default tokenizer.
2022-12-17 00:49:04 | INFO | absl | Using default tokenizer.
2022-12-17 00:49:36 | INFO | absl | Using default tokenizer.
2022-12-17 00:50:10 | INFO | absl | Using default tokenizer.
2022-12-17 00:50:40 | INFO | absl | Using default tokenizer.
2022-12-17 00:51:09 | INFO | absl | Using default tokenizer.
2022-12-17 00:51:46 | INFO | absl | Using default tokenizer.
2022-12-17 00:52:17 | INFO | absl | Using default tokenizer.
2022-12-17 00:52:48 | INFO | absl | Using default tokenizer.
2022-12-17 00:53:25 | INFO | absl | Using default tokenizer.
2022-12-17 00:53:59 | INFO | absl | Using default tokenizer.
2022-12-17 00:54:36 | INFO | absl | Using default tokenizer.
2022-12-17 00:55:10 | INFO | absl | Using default tokenizer.
2022-12-17 00:55:43 | INFO | absl | Using default tokenizer.
2022-12-17 00:56:16 | INFO | absl | Using default tokenizer.
2022-12-17 00:56:49 | INFO | absl | Using default tokenizer.
2022-12-17 00:57:25 | INFO | absl | Using default tokenizer.
2022-12-17 00:57:57 | INFO | absl | Using default tokenizer.
2022-12-17 00:58:34 | INFO | absl | Using default tokenizer.
2022-12-17 00:59:00 | INFO | valid | {"epoch": 46, "valid_loss": "6.427", "valid_nll_loss": "6.427", "valid_rouge1": 0.5119985744634948, "valid_rouge2": 0.14662332626589072, "valid_rougel": 0.22536340123994963, "valid_rouge_avg": 0.32931095036469277, "valid_ppl": "86.03", "valid_wps": "106.5", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6243", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 00:59:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 6243 updates
2022-12-17 00:59:00 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 00:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 00:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 46 @ 6243 updates, score 0.32931095036469277) (writing took 24.205309238983318 seconds)
2022-12-17 00:59:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-12-17 00:59:25 | INFO | train | {"epoch": 46, "train_loss": "0.062", "train_nll_loss": "0.062", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.04", "train_wps": "857.1", "train_ups": "0.06", "train_wpb": "13878.9", "train_bsz": "31.8", "train_num_updates": "6243", "train_lr": "3.95474e-05", "train_gnorm": "0.838", "train_clip": "100", "train_loss_scale": "0.25", "train_train_wall": "1088", "train_gb_free": "63", "train_wall": "106216"}
2022-12-17 00:59:25 | INFO | fairseq.trainer | begin training epoch 47
2022-12-17 00:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-17 01:00:57 | INFO | train_inner | {"epoch": 47, "update": 46.051, "loss": "0.058", "nll_loss": "0.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "107", "ups": "0.01", "wpb": "12709.8", "bsz": "29.6", "num_updates": "6250", "lr": "3.94737e-05", "gnorm": "0.857", "clip": "100", "loss_scale": "0.25", "train_wall": "79", "gb_free": "62.2", "wall": "106309"}
2022-12-17 01:02:18 | INFO | train_inner | {"epoch": 47, "update": 46.125, "loss": "0.055", "nll_loss": "0.055", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1753.9", "ups": "0.12", "wpb": "14081.6", "bsz": "32", "num_updates": "6260", "lr": "3.93684e-05", "gnorm": "0.631", "clip": "100", "loss_scale": "0.25", "train_wall": "80", "gb_free": "62.2", "wall": "106389"}
2022-12-17 01:03:38 | INFO | train_inner | {"epoch": 47, "update": 46.199, "loss": "0.056", "nll_loss": "0.056", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1774.3", "ups": "0.12", "wpb": "14269.5", "bsz": "32", "num_updates": "6270", "lr": "3.92632e-05", "gnorm": "0.628", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "106469"}
2022-12-17 01:05:00 | INFO | train_inner | {"epoch": 47, "update": 46.272, "loss": "0.058", "nll_loss": "0.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1708.5", "ups": "0.12", "wpb": "13938.3", "bsz": "32", "num_updates": "6280", "lr": "3.91579e-05", "gnorm": "0.673", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "106551"}
2022-12-17 01:06:19 | INFO | train_inner | {"epoch": 47, "update": 46.346, "loss": "0.057", "nll_loss": "0.057", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1745.7", "ups": "0.13", "wpb": "13885.5", "bsz": "32", "num_updates": "6290", "lr": "3.90526e-05", "gnorm": "0.865", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "106630"}
2022-12-17 01:07:42 | INFO | train_inner | {"epoch": 47, "update": 46.419, "loss": "0.057", "nll_loss": "0.057", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1685.2", "ups": "0.12", "wpb": "13879.6", "bsz": "32", "num_updates": "6300", "lr": "3.89474e-05", "gnorm": "0.642", "clip": "100", "loss_scale": "0.5", "train_wall": "82", "gb_free": "62.2", "wall": "106713"}
2022-12-17 01:09:01 | INFO | train_inner | {"epoch": 47, "update": 46.493, "loss": "0.058", "nll_loss": "0.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1739.9", "ups": "0.13", "wpb": "13824.5", "bsz": "32", "num_updates": "6310", "lr": "3.88421e-05", "gnorm": "0.681", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "106792"}
2022-12-17 01:10:22 | INFO | train_inner | {"epoch": 47, "update": 46.566, "loss": "0.055", "nll_loss": "0.055", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1717.5", "ups": "0.12", "wpb": "13938.6", "bsz": "32", "num_updates": "6320", "lr": "3.87368e-05", "gnorm": "0.614", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "106873"}
2022-12-17 01:11:44 | INFO | train_inner | {"epoch": 47, "update": 46.64, "loss": "0.056", "nll_loss": "0.056", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1708.9", "ups": "0.12", "wpb": "13898.3", "bsz": "32", "num_updates": "6330", "lr": "3.86316e-05", "gnorm": "0.651", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "106955"}
2022-12-17 01:13:05 | INFO | train_inner | {"epoch": 47, "update": 46.713, "loss": "0.056", "nll_loss": "0.056", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1679.3", "ups": "0.12", "wpb": "13671.8", "bsz": "32", "num_updates": "6340", "lr": "3.85263e-05", "gnorm": "0.789", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "107036"}
2022-12-17 01:14:25 | INFO | train_inner | {"epoch": 47, "update": 46.787, "loss": "0.055", "nll_loss": "0.055", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1782.4", "ups": "0.13", "wpb": "14202.5", "bsz": "32", "num_updates": "6350", "lr": "3.84211e-05", "gnorm": "0.649", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "107116"}
2022-12-17 01:15:45 | INFO | train_inner | {"epoch": 47, "update": 46.86, "loss": "0.057", "nll_loss": "0.057", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1748.4", "ups": "0.12", "wpb": "14060", "bsz": "32", "num_updates": "6360", "lr": "3.83158e-05", "gnorm": "0.701", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "107196"}
2022-12-17 01:17:06 | INFO | train_inner | {"epoch": 47, "update": 46.934, "loss": "0.058", "nll_loss": "0.058", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1694.3", "ups": "0.12", "wpb": "13744.8", "bsz": "32", "num_updates": "6370", "lr": "3.82105e-05", "gnorm": "0.643", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "107277"}
2022-12-17 01:18:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 01:19:05 | INFO | absl | Using default tokenizer.
2022-12-17 01:19:37 | INFO | absl | Using default tokenizer.
2022-12-17 01:20:02 | INFO | absl | Using default tokenizer.
2022-12-17 01:20:33 | INFO | absl | Using default tokenizer.
2022-12-17 01:20:59 | INFO | absl | Using default tokenizer.
2022-12-17 01:21:32 | INFO | absl | Using default tokenizer.
2022-12-17 01:22:00 | INFO | absl | Using default tokenizer.
2022-12-17 01:22:28 | INFO | absl | Using default tokenizer.
2022-12-17 01:22:57 | INFO | absl | Using default tokenizer.
2022-12-17 01:23:30 | INFO | absl | Using default tokenizer.
2022-12-17 01:24:02 | INFO | absl | Using default tokenizer.
2022-12-17 01:24:36 | INFO | absl | Using default tokenizer.
2022-12-17 01:25:08 | INFO | absl | Using default tokenizer.
2022-12-17 01:25:40 | INFO | absl | Using default tokenizer.
2022-12-17 01:26:15 | INFO | absl | Using default tokenizer.
2022-12-17 01:26:42 | INFO | absl | Using default tokenizer.
2022-12-17 01:27:16 | INFO | absl | Using default tokenizer.
2022-12-17 01:27:47 | INFO | absl | Using default tokenizer.
2022-12-17 01:28:19 | INFO | absl | Using default tokenizer.
2022-12-17 01:28:50 | INFO | absl | Using default tokenizer.
2022-12-17 01:29:22 | INFO | absl | Using default tokenizer.
2022-12-17 01:30:01 | INFO | absl | Using default tokenizer.
2022-12-17 01:30:36 | INFO | absl | Using default tokenizer.
2022-12-17 01:31:07 | INFO | absl | Using default tokenizer.
2022-12-17 01:31:42 | INFO | absl | Using default tokenizer.
2022-12-17 01:32:16 | INFO | absl | Using default tokenizer.
2022-12-17 01:32:47 | INFO | absl | Using default tokenizer.
2022-12-17 01:33:26 | INFO | absl | Using default tokenizer.
2022-12-17 01:34:05 | INFO | absl | Using default tokenizer.
2022-12-17 01:34:39 | INFO | absl | Using default tokenizer.
2022-12-17 01:35:19 | INFO | absl | Using default tokenizer.
2022-12-17 01:35:46 | INFO | valid | {"epoch": 47, "valid_loss": "6.439", "valid_nll_loss": "6.439", "valid_rouge1": 0.5142496879127485, "valid_rouge2": 0.14646266893101198, "valid_rougel": 0.22736703042863438, "valid_rouge_avg": 0.3303561784218801, "valid_ppl": "86.77", "valid_wps": "106.3", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6379", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 01:35:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 6379 updates
2022-12-17 01:35:46 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 01:36:08 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 01:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 47 @ 6379 updates, score 0.3303561784218801) (writing took 22.424883405910805 seconds)
2022-12-17 01:36:09 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-12-17 01:36:09 | INFO | train | {"epoch": 47, "train_loss": "0.057", "train_nll_loss": "0.057", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.04", "train_wps": "856.1", "train_ups": "0.06", "train_wpb": "13873.8", "train_bsz": "31.8", "train_num_updates": "6379", "train_lr": "3.81158e-05", "train_gnorm": "0.697", "train_clip": "100", "train_loss_scale": "0.5", "train_train_wall": "1093", "train_gb_free": "62.2", "train_wall": "108420"}
2022-12-17 01:36:09 | INFO | fairseq.trainer | begin training epoch 48
2022-12-17 01:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-17 01:36:55 | INFO | train_inner | {"epoch": 48, "update": 47.007, "loss": "0.057", "nll_loss": "0.057", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "109.2", "ups": "0.01", "wpb": "12975.4", "bsz": "29.6", "num_updates": "6380", "lr": "3.81053e-05", "gnorm": "1.215", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "108466"}
2022-12-17 01:38:16 | INFO | train_inner | {"epoch": 48, "update": 47.081, "loss": "0.052", "nll_loss": "0.052", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1735", "ups": "0.12", "wpb": "14068.8", "bsz": "32", "num_updates": "6390", "lr": "3.8e-05", "gnorm": "0.678", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "108547"}
2022-12-17 01:39:36 | INFO | train_inner | {"epoch": 48, "update": 47.154, "loss": "0.052", "nll_loss": "0.052", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1710.3", "ups": "0.13", "wpb": "13593.2", "bsz": "32", "num_updates": "6400", "lr": "3.78947e-05", "gnorm": "0.614", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "108627"}
2022-12-17 01:40:56 | INFO | train_inner | {"epoch": 48, "update": 47.228, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1766.5", "ups": "0.13", "wpb": "14122.8", "bsz": "32", "num_updates": "6410", "lr": "3.77895e-05", "gnorm": "0.621", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "63.6", "wall": "108707"}
2022-12-17 01:42:16 | INFO | train_inner | {"epoch": 48, "update": 47.301, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1751.2", "ups": "0.12", "wpb": "14093.9", "bsz": "32", "num_updates": "6420", "lr": "3.76842e-05", "gnorm": "0.613", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "108787"}
2022-12-17 01:43:38 | INFO | train_inner | {"epoch": 48, "update": 47.375, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1718.1", "ups": "0.12", "wpb": "14030.5", "bsz": "32", "num_updates": "6430", "lr": "3.75789e-05", "gnorm": "0.739", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "108869"}
2022-12-17 01:44:57 | INFO | train_inner | {"epoch": 48, "update": 47.449, "loss": "0.054", "nll_loss": "0.054", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1720.4", "ups": "0.13", "wpb": "13676.9", "bsz": "32", "num_updates": "6440", "lr": "3.74737e-05", "gnorm": "0.629", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "108948"}
2022-12-17 01:46:18 | INFO | train_inner | {"epoch": 48, "update": 47.522, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1737.8", "ups": "0.12", "wpb": "14109.2", "bsz": "32", "num_updates": "6450", "lr": "3.73684e-05", "gnorm": "0.653", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "109030"}
2022-12-17 01:47:39 | INFO | train_inner | {"epoch": 48, "update": 47.596, "loss": "0.052", "nll_loss": "0.052", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1765.2", "ups": "0.12", "wpb": "14143.6", "bsz": "32", "num_updates": "6460", "lr": "3.72632e-05", "gnorm": "0.691", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "109110"}
2022-12-17 01:49:00 | INFO | train_inner | {"epoch": 48, "update": 47.669, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1705.8", "ups": "0.12", "wpb": "13947.4", "bsz": "32", "num_updates": "6470", "lr": "3.71579e-05", "gnorm": "0.828", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "109191"}
2022-12-17 01:50:21 | INFO | train_inner | {"epoch": 48, "update": 47.743, "loss": "0.052", "nll_loss": "0.052", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1724.2", "ups": "0.12", "wpb": "13903.8", "bsz": "32", "num_updates": "6480", "lr": "3.70526e-05", "gnorm": "0.636", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "109272"}
2022-12-17 01:51:40 | INFO | train_inner | {"epoch": 48, "update": 47.816, "loss": "0.054", "nll_loss": "0.054", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1744.1", "ups": "0.13", "wpb": "13808.9", "bsz": "32", "num_updates": "6490", "lr": "3.69474e-05", "gnorm": "0.623", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "109351"}
2022-12-17 01:53:02 | INFO | train_inner | {"epoch": 48, "update": 47.89, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1684.3", "ups": "0.12", "wpb": "13706.1", "bsz": "32", "num_updates": "6500", "lr": "3.68421e-05", "gnorm": "0.615", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "109433"}
2022-12-17 01:54:22 | INFO | train_inner | {"epoch": 48, "update": 47.963, "loss": "0.054", "nll_loss": "0.054", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1757.9", "ups": "0.12", "wpb": "14206", "bsz": "32", "num_updates": "6510", "lr": "3.67368e-05", "gnorm": "0.708", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "109514"}
2022-12-17 01:55:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 01:55:50 | INFO | absl | Using default tokenizer.
2022-12-17 01:56:20 | INFO | absl | Using default tokenizer.
2022-12-17 01:56:50 | INFO | absl | Using default tokenizer.
2022-12-17 01:57:19 | INFO | absl | Using default tokenizer.
2022-12-17 01:57:48 | INFO | absl | Using default tokenizer.
2022-12-17 01:58:13 | INFO | absl | Using default tokenizer.
2022-12-17 01:58:41 | INFO | absl | Using default tokenizer.
2022-12-17 01:59:08 | INFO | absl | Using default tokenizer.
2022-12-17 01:59:38 | INFO | absl | Using default tokenizer.
2022-12-17 02:00:07 | INFO | absl | Using default tokenizer.
2022-12-17 02:00:43 | INFO | absl | Using default tokenizer.
2022-12-17 02:01:20 | INFO | absl | Using default tokenizer.
2022-12-17 02:01:53 | INFO | absl | Using default tokenizer.
2022-12-17 02:02:21 | INFO | absl | Using default tokenizer.
2022-12-17 02:02:54 | INFO | absl | Using default tokenizer.
2022-12-17 02:03:24 | INFO | absl | Using default tokenizer.
2022-12-17 02:03:56 | INFO | absl | Using default tokenizer.
2022-12-17 02:04:27 | INFO | absl | Using default tokenizer.
2022-12-17 02:04:59 | INFO | absl | Using default tokenizer.
2022-12-17 02:05:28 | INFO | absl | Using default tokenizer.
2022-12-17 02:05:58 | INFO | absl | Using default tokenizer.
2022-12-17 02:06:35 | INFO | absl | Using default tokenizer.
2022-12-17 02:07:05 | INFO | absl | Using default tokenizer.
2022-12-17 02:07:35 | INFO | absl | Using default tokenizer.
2022-12-17 02:08:08 | INFO | absl | Using default tokenizer.
2022-12-17 02:08:37 | INFO | absl | Using default tokenizer.
2022-12-17 02:09:14 | INFO | absl | Using default tokenizer.
2022-12-17 02:09:48 | INFO | absl | Using default tokenizer.
2022-12-17 02:10:26 | INFO | absl | Using default tokenizer.
2022-12-17 02:10:53 | INFO | absl | Using default tokenizer.
2022-12-17 02:11:25 | INFO | absl | Using default tokenizer.
2022-12-17 02:11:48 | INFO | valid | {"epoch": 48, "valid_loss": "6.5", "valid_nll_loss": "6.5", "valid_rouge1": 0.5061698871280718, "valid_rouge2": 0.14275376016369132, "valid_rougel": 0.22376939543827665, "valid_rouge_avg": 0.3244618236458815, "valid_ppl": "90.5", "valid_wps": "111", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6515", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 02:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 6515 updates
2022-12-17 02:11:48 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 02:12:10 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 02:12:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 48 @ 6515 updates, score 0.3244618236458815) (writing took 22.20621981099248 seconds)
2022-12-17 02:12:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-12-17 02:12:10 | INFO | train | {"epoch": 48, "train_loss": "0.053", "train_nll_loss": "0.053", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.04", "train_wps": "873", "train_ups": "0.06", "train_wpb": "13874.6", "train_bsz": "31.8", "train_num_updates": "6515", "train_lr": "3.66842e-05", "train_gnorm": "0.695", "train_clip": "100", "train_loss_scale": "0.5", "train_train_wall": "1092", "train_gb_free": "62.2", "train_wall": "110581"}
2022-12-17 02:12:10 | INFO | fairseq.trainer | begin training epoch 49
2022-12-17 02:12:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-17 02:13:28 | INFO | train_inner | {"epoch": 49, "update": 48.037, "loss": "0.051", "nll_loss": "0.051", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "110.4", "ups": "0.01", "wpb": "12646.8", "bsz": "29.6", "num_updates": "6520", "lr": "3.66316e-05", "gnorm": "0.765", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "110659"}
2022-12-17 02:14:50 | INFO | train_inner | {"epoch": 49, "update": 48.11, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1687", "ups": "0.12", "wpb": "13868.1", "bsz": "32", "num_updates": "6530", "lr": "3.65263e-05", "gnorm": "0.605", "clip": "100", "loss_scale": "0.5", "train_wall": "82", "gb_free": "62.2", "wall": "110741"}
2022-12-17 02:16:09 | INFO | train_inner | {"epoch": 49, "update": 48.184, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1804.6", "ups": "0.13", "wpb": "14221.6", "bsz": "32", "num_updates": "6540", "lr": "3.64211e-05", "gnorm": "0.683", "clip": "100", "loss_scale": "0.5", "train_wall": "78", "gb_free": "62.2", "wall": "110820"}
2022-12-17 02:17:30 | INFO | train_inner | {"epoch": 49, "update": 48.257, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1688.8", "ups": "0.12", "wpb": "13729.4", "bsz": "32", "num_updates": "6550", "lr": "3.63158e-05", "gnorm": "0.584", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "110901"}
2022-12-17 02:18:50 | INFO | train_inner | {"epoch": 49, "update": 48.331, "loss": "0.048", "nll_loss": "0.048", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1792.2", "ups": "0.13", "wpb": "14276.9", "bsz": "32", "num_updates": "6560", "lr": "3.62105e-05", "gnorm": "0.624", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.9", "wall": "110981"}
2022-12-17 02:20:10 | INFO | train_inner | {"epoch": 49, "update": 48.404, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1744.8", "ups": "0.13", "wpb": "13902.5", "bsz": "32", "num_updates": "6570", "lr": "3.61053e-05", "gnorm": "0.599", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "111061"}
2022-12-17 02:21:29 | INFO | train_inner | {"epoch": 49, "update": 48.478, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1776.6", "ups": "0.13", "wpb": "14138.6", "bsz": "32", "num_updates": "6580", "lr": "3.6e-05", "gnorm": "0.583", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "111140"}
2022-12-17 02:22:51 | INFO | train_inner | {"epoch": 49, "update": 48.551, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1716.1", "ups": "0.12", "wpb": "13954.8", "bsz": "32", "num_updates": "6590", "lr": "3.58947e-05", "gnorm": "0.623", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "111222"}
2022-12-17 02:24:11 | INFO | train_inner | {"epoch": 49, "update": 48.625, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1725.8", "ups": "0.13", "wpb": "13781.6", "bsz": "32", "num_updates": "6600", "lr": "3.57895e-05", "gnorm": "0.634", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "111302"}
2022-12-17 02:25:32 | INFO | train_inner | {"epoch": 49, "update": 48.699, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1726.2", "ups": "0.12", "wpb": "14033.7", "bsz": "32", "num_updates": "6610", "lr": "3.56842e-05", "gnorm": "0.616", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "111383"}
2022-12-17 02:26:50 | INFO | train_inner | {"epoch": 49, "update": 48.772, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1770.9", "ups": "0.13", "wpb": "13880.2", "bsz": "32", "num_updates": "6620", "lr": "3.55789e-05", "gnorm": "0.789", "clip": "100", "loss_scale": "0.5", "train_wall": "78", "gb_free": "62.2", "wall": "111461"}
2022-12-17 02:28:12 | INFO | train_inner | {"epoch": 49, "update": 48.846, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1701", "ups": "0.12", "wpb": "13838.6", "bsz": "32", "num_updates": "6630", "lr": "3.54737e-05", "gnorm": "0.644", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "111543"}
2022-12-17 02:29:31 | INFO | train_inner | {"epoch": 49, "update": 48.919, "loss": "0.051", "nll_loss": "0.051", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1756.9", "ups": "0.13", "wpb": "13977", "bsz": "32", "num_updates": "6640", "lr": "3.53684e-05", "gnorm": "1.296", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "111622"}
2022-12-17 02:30:54 | INFO | train_inner | {"epoch": 49, "update": 48.993, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1696.9", "ups": "0.12", "wpb": "13967.4", "bsz": "32", "num_updates": "6650", "lr": "3.52632e-05", "gnorm": "0.623", "clip": "100", "loss_scale": "0.5", "train_wall": "82", "gb_free": "62.2", "wall": "111705"}
2022-12-17 02:31:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 02:31:46 | INFO | absl | Using default tokenizer.
2022-12-17 02:32:16 | INFO | absl | Using default tokenizer.
2022-12-17 02:32:43 | INFO | absl | Using default tokenizer.
2022-12-17 02:33:13 | INFO | absl | Using default tokenizer.
2022-12-17 02:33:40 | INFO | absl | Using default tokenizer.
2022-12-17 02:34:07 | INFO | absl | Using default tokenizer.
2022-12-17 02:34:37 | INFO | absl | Using default tokenizer.
2022-12-17 02:35:09 | INFO | absl | Using default tokenizer.
2022-12-17 02:35:45 | INFO | absl | Using default tokenizer.
2022-12-17 02:36:21 | INFO | absl | Using default tokenizer.
2022-12-17 02:36:50 | INFO | absl | Using default tokenizer.
2022-12-17 02:37:23 | INFO | absl | Using default tokenizer.
2022-12-17 02:37:55 | INFO | absl | Using default tokenizer.
2022-12-17 02:38:26 | INFO | absl | Using default tokenizer.
2022-12-17 02:38:58 | INFO | absl | Using default tokenizer.
2022-12-17 02:39:28 | INFO | absl | Using default tokenizer.
2022-12-17 02:39:59 | INFO | absl | Using default tokenizer.
2022-12-17 02:40:35 | INFO | absl | Using default tokenizer.
2022-12-17 02:41:10 | INFO | absl | Using default tokenizer.
2022-12-17 02:41:45 | INFO | absl | Using default tokenizer.
2022-12-17 02:42:16 | INFO | absl | Using default tokenizer.
2022-12-17 02:42:55 | INFO | absl | Using default tokenizer.
2022-12-17 02:43:27 | INFO | absl | Using default tokenizer.
2022-12-17 02:44:07 | INFO | absl | Using default tokenizer.
2022-12-17 02:44:38 | INFO | absl | Using default tokenizer.
2022-12-17 02:45:16 | INFO | absl | Using default tokenizer.
2022-12-17 02:45:47 | INFO | absl | Using default tokenizer.
2022-12-17 02:46:16 | INFO | absl | Using default tokenizer.
2022-12-17 02:46:55 | INFO | absl | Using default tokenizer.
2022-12-17 02:47:24 | INFO | absl | Using default tokenizer.
2022-12-17 02:48:06 | INFO | absl | Using default tokenizer.
2022-12-17 02:48:33 | INFO | valid | {"epoch": 49, "valid_loss": "6.516", "valid_nll_loss": "6.516", "valid_rouge1": 0.5103358231808318, "valid_rouge2": 0.14367290404494448, "valid_rougel": 0.22340984767505295, "valid_rouge_avg": 0.32700436361288815, "valid_ppl": "91.51", "valid_wps": "105.9", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6651", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 02:48:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 6651 updates
2022-12-17 02:48:33 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 02:48:56 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 02:48:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 49 @ 6651 updates, score 0.32700436361288815) (writing took 22.933839389821514 seconds)
2022-12-17 02:48:56 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-12-17 02:48:56 | INFO | train | {"epoch": 49, "train_loss": "0.049", "train_nll_loss": "0.049", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.03", "train_wps": "855.2", "train_ups": "0.06", "train_wpb": "13873.8", "train_bsz": "31.8", "train_num_updates": "6651", "train_lr": "3.52526e-05", "train_gnorm": "0.693", "train_clip": "100", "train_loss_scale": "0.5", "train_train_wall": "1087", "train_gb_free": "62.2", "train_wall": "112787"}
2022-12-17 02:48:56 | INFO | fairseq.trainer | begin training epoch 50
2022-12-17 02:48:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-12-17 02:50:45 | INFO | train_inner | {"epoch": 50, "update": 49.066, "loss": "0.044", "nll_loss": "0.044", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "109.5", "ups": "0.01", "wpb": "13046.8", "bsz": "29.6", "num_updates": "6660", "lr": "3.51579e-05", "gnorm": "0.767", "clip": "100", "loss_scale": "0.5", "train_wall": "77", "gb_free": "62.2", "wall": "112896"}
2022-12-17 02:52:05 | INFO | train_inner | {"epoch": 50, "update": 49.14, "loss": "0.046", "nll_loss": "0.046", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1724.3", "ups": "0.13", "wpb": "13768.3", "bsz": "32", "num_updates": "6670", "lr": "3.50526e-05", "gnorm": "0.574", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "112976"}
2022-12-17 02:53:25 | INFO | train_inner | {"epoch": 50, "update": 49.213, "loss": "0.046", "nll_loss": "0.046", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1743.3", "ups": "0.12", "wpb": "13957.9", "bsz": "32", "num_updates": "6680", "lr": "3.49474e-05", "gnorm": "0.601", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113056"}
2022-12-17 02:54:46 | INFO | train_inner | {"epoch": 50, "update": 49.287, "loss": "0.045", "nll_loss": "0.045", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1737", "ups": "0.12", "wpb": "13964.8", "bsz": "32", "num_updates": "6690", "lr": "3.48421e-05", "gnorm": "0.589", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113137"}
2022-12-17 02:56:06 | INFO | train_inner | {"epoch": 50, "update": 49.36, "loss": "0.046", "nll_loss": "0.046", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1707.6", "ups": "0.12", "wpb": "13755", "bsz": "32", "num_updates": "6700", "lr": "3.47368e-05", "gnorm": "0.573", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113217"}
2022-12-17 02:57:28 | INFO | train_inner | {"epoch": 50, "update": 49.434, "loss": "0.046", "nll_loss": "0.046", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1705.2", "ups": "0.12", "wpb": "14011.9", "bsz": "32", "num_updates": "6710", "lr": "3.46316e-05", "gnorm": "0.634", "clip": "100", "loss_scale": "0.5", "train_wall": "82", "gb_free": "62.2", "wall": "113299"}
2022-12-17 02:58:50 | INFO | train_inner | {"epoch": 50, "update": 49.507, "loss": "0.046", "nll_loss": "0.046", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1708.3", "ups": "0.12", "wpb": "13873", "bsz": "32", "num_updates": "6720", "lr": "3.45263e-05", "gnorm": "0.749", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "113381"}
2022-12-17 03:00:10 | INFO | train_inner | {"epoch": 50, "update": 49.581, "loss": "0.047", "nll_loss": "0.047", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1768.4", "ups": "0.12", "wpb": "14270.4", "bsz": "32", "num_updates": "6730", "lr": "3.44211e-05", "gnorm": "0.636", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113461"}
2022-12-17 03:01:32 | INFO | train_inner | {"epoch": 50, "update": 49.654, "loss": "0.048", "nll_loss": "0.048", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1728.2", "ups": "0.12", "wpb": "14128.8", "bsz": "32", "num_updates": "6740", "lr": "3.43158e-05", "gnorm": "0.642", "clip": "100", "loss_scale": "0.5", "train_wall": "81", "gb_free": "62.2", "wall": "113543"}
2022-12-17 03:02:51 | INFO | train_inner | {"epoch": 50, "update": 49.728, "loss": "0.048", "nll_loss": "0.048", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1775.6", "ups": "0.13", "wpb": "14068.2", "bsz": "32", "num_updates": "6750", "lr": "3.42105e-05", "gnorm": "0.902", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "113622"}
2022-12-17 03:04:12 | INFO | train_inner | {"epoch": 50, "update": 49.801, "loss": "0.049", "nll_loss": "0.049", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.03", "wps": "1742.6", "ups": "0.12", "wpb": "13981.9", "bsz": "32", "num_updates": "6760", "lr": "3.41053e-05", "gnorm": "1.176", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113703"}
2022-12-17 03:05:31 | INFO | train_inner | {"epoch": 50, "update": 49.875, "loss": "0.05", "nll_loss": "0.05", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1760.8", "ups": "0.13", "wpb": "13962", "bsz": "32", "num_updates": "6770", "lr": "3.4e-05", "gnorm": "1.901", "clip": "100", "loss_scale": "0.5", "train_wall": "79", "gb_free": "62.2", "wall": "113782"}
2022-12-17 03:06:51 | INFO | train_inner | {"epoch": 50, "update": 49.949, "loss": "0.053", "nll_loss": "0.053", "rouge1": 0.0, "rouge2": 0.0, "rougel": 0.0, "rouge_avg": 0.0, "ppl": "1.04", "wps": "1717.5", "ups": "0.12", "wpb": "13766.1", "bsz": "32", "num_updates": "6780", "lr": "3.38947e-05", "gnorm": "3.962", "clip": "100", "loss_scale": "0.5", "train_wall": "80", "gb_free": "62.2", "wall": "113862"}
2022-12-17 03:07:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-12-17 03:08:31 | INFO | absl | Using default tokenizer.
2022-12-17 03:08:57 | INFO | absl | Using default tokenizer.
2022-12-17 03:09:25 | INFO | absl | Using default tokenizer.
2022-12-17 03:09:55 | INFO | absl | Using default tokenizer.
2022-12-17 03:10:23 | INFO | absl | Using default tokenizer.
2022-12-17 03:10:55 | INFO | absl | Using default tokenizer.
2022-12-17 03:11:23 | INFO | absl | Using default tokenizer.
2022-12-17 03:11:52 | INFO | absl | Using default tokenizer.
2022-12-17 03:12:22 | INFO | absl | Using default tokenizer.
2022-12-17 03:12:49 | INFO | absl | Using default tokenizer.
2022-12-17 03:13:25 | INFO | absl | Using default tokenizer.
2022-12-17 03:14:00 | INFO | absl | Using default tokenizer.
2022-12-17 03:14:31 | INFO | absl | Using default tokenizer.
2022-12-17 03:14:59 | INFO | absl | Using default tokenizer.
2022-12-17 03:15:28 | INFO | absl | Using default tokenizer.
2022-12-17 03:15:59 | INFO | absl | Using default tokenizer.
2022-12-17 03:16:30 | INFO | absl | Using default tokenizer.
2022-12-17 03:16:59 | INFO | absl | Using default tokenizer.
2022-12-17 03:17:26 | INFO | absl | Using default tokenizer.
2022-12-17 03:17:57 | INFO | absl | Using default tokenizer.
2022-12-17 03:18:27 | INFO | absl | Using default tokenizer.
2022-12-17 03:19:03 | INFO | absl | Using default tokenizer.
2022-12-17 03:19:32 | INFO | absl | Using default tokenizer.
2022-12-17 03:20:05 | INFO | absl | Using default tokenizer.
2022-12-17 03:20:38 | INFO | absl | Using default tokenizer.
2022-12-17 03:21:08 | INFO | absl | Using default tokenizer.
2022-12-17 03:21:42 | INFO | absl | Using default tokenizer.
2022-12-17 03:22:12 | INFO | absl | Using default tokenizer.
2022-12-17 03:22:49 | INFO | absl | Using default tokenizer.
2022-12-17 03:23:28 | INFO | absl | Using default tokenizer.
2022-12-17 03:24:08 | INFO | absl | Using default tokenizer.
2022-12-17 03:24:35 | INFO | valid | {"epoch": 50, "valid_loss": "6.579", "valid_nll_loss": "6.579", "valid_rouge1": 0.5113458276595697, "valid_rouge2": 0.14710737775709548, "valid_rougel": 0.22685020394748667, "valid_rouge_avg": 0.3292266027083326, "valid_ppl": "95.62", "valid_wps": "110.6", "valid_wpb": "3454.4", "valid_bsz": "7.8", "valid_num_updates": "6787", "valid_best_rouge_avg": 0.33056833836709293}
2022-12-17 03:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 6787 updates
2022-12-17 03:24:35 | INFO | fairseq.trainer | Saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 03:25:02 | INFO | fairseq.trainer | Finished saving checkpoint to checkpoints/checkpoint_last.pt
2022-12-17 03:25:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 50 @ 6787 updates, score 0.3292266027083326) (writing took 27.413684369064867 seconds)
2022-12-17 03:25:02 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-12-17 03:25:02 | INFO | train | {"epoch": 50, "train_loss": "0.048", "train_nll_loss": "0.048", "train_rouge1": 0.0, "train_rouge2": 0.0, "train_rougel": 0.0, "train_rouge_avg": 0.0, "train_ppl": "1.03", "train_wps": "871.6", "train_ups": "0.06", "train_wpb": "13880.4", "train_bsz": "31.8", "train_num_updates": "6787", "train_lr": "3.38211e-05", "train_gnorm": "1.074", "train_clip": "100", "train_loss_scale": "0.5", "train_train_wall": "1086", "train_gb_free": "62.2", "train_wall": "114953"}
2022-12-17 03:25:02 | INFO | fairseq_cli.train | done training in 114919.6 seconds
